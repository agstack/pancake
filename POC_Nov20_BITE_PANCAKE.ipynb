{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# POC-Nov20: BITE + PANCAKE Demo\n",
        "\n",
        "**AI-native spatio-temporal data organization and interaction - for the GenAI and Agentic-era**\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates:\n",
        "1. **BITE**: Bidirectional Interchange Transport Envelope - flexible JSON data structure\n",
        "2. **PANCAKE**: Persistent-Agentic-Node + Contextual Accretive Knowledge Ensemble - AI-native storage\n",
        "3. **TAP**: Third-party Agentic-Pipeline - manifold for geospatial data\n",
        "4. **SIRUP**: Spatio-temporal Intelligence for Reasoning and Unified Perception - enriched data flow\n",
        "5. **Multi-pronged RAG**: Semantic + Spatial + Temporal similarity\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import hashlib\n",
        "from ulid import ULID\n",
        "import psycopg2\n",
        "from psycopg2.extras import Json\n",
        "import s2sphere as s2\n",
        "from shapely.geometry import shape, Point\n",
        "from shapely.wkt import loads as load_wkt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "TERRAPIPE_SECRET = \"dkpnSTZVeWRhWG5NNmdpY2xPM2kzNnJ3cXJkbWpFaQ==\"\n",
        "TERRAPIPE_CLIENT = \"Dev\"\n",
        "TEST_GEOID = \"63f764609b85eb356d387c1630a0671d3a8a56ffb6c91d1e52b1d7f2fe3c4213\"\n",
        "OPENAI_API_KEY = \"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\"\n",
        "\n",
        "# Database connections\n",
        "PANCAKE_DB = \"postgresql://pancake_user:pancake_pass@localhost:5432/pancake_poc\"\n",
        "TRADITIONAL_DB = \"postgresql://pancake_user:pancake_pass@localhost:5432/traditional_poc\"\n",
        "\n",
        "# Initialize OpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "print(\"‚úì Environment configured\")\n",
        "print(f\"‚úì Test GeoID: {TEST_GEOID}\")\n",
        "print(f\"‚úì OpenAI client initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: BITE Specification\n",
        "\n",
        "### The Bidirectional Interchange Transport Envelope\n",
        "\n",
        "BITE is a universal format for spatio-temporal data with three components:\n",
        "- **Header**: Metadata (ID, GeoID, timestamp, type, source)\n",
        "- **Body**: Actual data payload (flexible JSON)\n",
        "- **Footer**: Integrity (hash, schema version, tags, references)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BITE:\n",
        "    \"\"\"\n",
        "    Bidirectional Interchange Transport Envelope\n",
        "    A universal format for spatio-temporal data interchange\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def create(\n",
        "        bite_type: str,\n",
        "        geoid: str,\n",
        "        body: Dict[str, Any],\n",
        "        source: Dict[str, Any] = None,\n",
        "        tags: List[str] = None,\n",
        "        references: List[str] = None,\n",
        "        timestamp: str = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Create a BITE with proper structure\"\"\"\n",
        "        \n",
        "        bite_id = str(ULID())\n",
        "        ts = timestamp or datetime.utcnow().isoformat() + \"Z\"\n",
        "        \n",
        "        header = {\n",
        "            \"id\": bite_id,\n",
        "            \"geoid\": geoid,\n",
        "            \"timestamp\": ts,\n",
        "            \"type\": bite_type,\n",
        "        }\n",
        "        \n",
        "        if source:\n",
        "            header[\"source\"] = source\n",
        "        \n",
        "        # Compute hash for integrity\n",
        "        header_str = json.dumps(header, sort_keys=True)\n",
        "        body_str = json.dumps(body, sort_keys=True)\n",
        "        hash_val = hashlib.sha256((header_str + body_str).encode()).hexdigest()\n",
        "        \n",
        "        footer = {\n",
        "            \"hash\": hash_val,\n",
        "            \"schema_version\": \"1.0\"\n",
        "        }\n",
        "        \n",
        "        if tags:\n",
        "            footer[\"tags\"] = tags\n",
        "        if references:\n",
        "            footer[\"references\"] = references\n",
        "        \n",
        "        return {\n",
        "            \"Header\": header,\n",
        "            \"Body\": body,\n",
        "            \"Footer\": footer\n",
        "        }\n",
        "    \n",
        "    @staticmethod\n",
        "    def validate(bite: Dict[str, Any]) -> bool:\n",
        "        \"\"\"Validate BITE structure and integrity\"\"\"\n",
        "        required_keys = {\"Header\", \"Body\", \"Footer\"}\n",
        "        if set(bite.keys()) != required_keys:\n",
        "            return False\n",
        "        \n",
        "        header = bite[\"Header\"]\n",
        "        required_header = {\"id\", \"geoid\", \"timestamp\", \"type\"}\n",
        "        if not required_header.issubset(set(header.keys())):\n",
        "            return False\n",
        "        \n",
        "        # Validate hash\n",
        "        header_str = json.dumps(header, sort_keys=True)\n",
        "        body_str = json.dumps(bite[\"Body\"], sort_keys=True)\n",
        "        computed_hash = hashlib.sha256((header_str + body_str).encode()).hexdigest()\n",
        "        \n",
        "        return bite[\"Footer\"][\"hash\"] == computed_hash\n",
        "\n",
        "print(\"‚úì BITE class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Create an Observation BITE (Point)\n",
        "observation_bite = BITE.create(\n",
        "    bite_type=\"observation\",\n",
        "    geoid=TEST_GEOID,\n",
        "    body={\n",
        "        \"observation_type\": \"disease\",\n",
        "        \"crop\": \"coffee\",\n",
        "        \"disease\": \"coffee_rust\",\n",
        "        \"severity\": \"moderate\",\n",
        "        \"affected_plants\": 45,\n",
        "        \"location_detail\": \"western_section\",\n",
        "        \"notes\": \"Orange pustules visible on leaf undersides\"\n",
        "    },\n",
        "    source={\n",
        "        \"agent\": \"field-agent-maria\",\n",
        "        \"device\": \"mobile-app-v2.1\"\n",
        "    },\n",
        "    tags=[\"disease\", \"coffee\", \"urgent\", \"point\"]\n",
        ")\n",
        "\n",
        "print(\"üìç Observation BITE (Point):\")\n",
        "print(json.dumps(observation_bite, indent=2))\n",
        "print(f\"\\n‚úì Valid: {BITE.validate(observation_bite)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: TAP & SIRUP - Real Geospatial Data Pipeline\n",
        "\n",
        "### TAP: Third-party Agentic-Pipeline\n",
        "A manifold that connects external data vendors (like terrapipe.io) to GeoIDs, automatically transforming raw data into BITEs.\n",
        "\n",
        "### SIRUP: Spatio-temporal Intelligence for Reasoning and Unified Perception\n",
        "The enriched data flowing through TAP - includes spatial context, temporal markers, and semantic metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TAPClient:\n",
        "    \"\"\"\n",
        "    TAP: Third-party Agentic-Pipeline\n",
        "    Manifold for connecting SIRUP vendors to GeoIDs\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.terrapipe_url = \"https://appserver.terrapipe.io\"\n",
        "        self.headers = {\n",
        "            \"secretkey\": TERRAPIPE_SECRET,\n",
        "            \"client\": TERRAPIPE_CLIENT\n",
        "        }\n",
        "    \n",
        "    def get_sirup_dates(self, geoid: str, start_date: str, end_date: str) -> List[str]:\n",
        "        \"\"\"Get available SIRUP dates for a GeoID\"\"\"\n",
        "        url = f\"{self.terrapipe_url}/getNDVIDatesForGeoid\"\n",
        "        params = {\n",
        "            \"geoid\": geoid,\n",
        "            \"start_date\": start_date,\n",
        "            \"end_date\": end_date\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                return response.json().get(\"dates\", [])\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching SIRUP dates: {e}\")\n",
        "        return []\n",
        "    \n",
        "    def get_sirup_ndvi(self, geoid: str, date: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Fetch SIRUP (Spatio-temporal Intelligence for Reasoning and Unified Perception)\n",
        "        from terrapipe.io for a specific GeoID and date\n",
        "        \"\"\"\n",
        "        url = f\"{self.terrapipe_url}/getNDVIImg\"\n",
        "        params = {\n",
        "            \"geoid\": geoid,\n",
        "            \"date\": date\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers, params=params)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching SIRUP data: {e}\")\n",
        "        return None\n",
        "    \n",
        "    def sirup_to_bite(self, geoid: str, date: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Transform SIRUP data into BITE format\n",
        "        This is the core TAP functionality: vendor data ‚Üí BITE\n",
        "        \"\"\"\n",
        "        sirup_data = self.get_sirup_ndvi(geoid, date)\n",
        "        \n",
        "        if not sirup_data:\n",
        "            return None\n",
        "        \n",
        "        # Extract key metrics\n",
        "        ndvi_features = sirup_data.get(\"ndvi_img\", {}).get(\"features\", [])\n",
        "        ndvi_values = [f[\"properties\"][\"NDVI\"] for f in ndvi_features if \"NDVI\" in f[\"properties\"]]\n",
        "        \n",
        "        # Create SIRUP body\n",
        "        body = {\n",
        "            \"sirup_type\": \"satellite_ndvi\",\n",
        "            \"vendor\": \"terrapipe.io\",\n",
        "            \"date\": date,\n",
        "            \"boundary\": sirup_data.get(\"boundary_geoDataFrameDict\"),\n",
        "            \"ndvi_stats\": {\n",
        "                \"mean\": float(np.mean(ndvi_values)) if ndvi_values else None,\n",
        "                \"min\": float(np.min(ndvi_values)) if ndvi_values else None,\n",
        "                \"max\": float(np.max(ndvi_values)) if ndvi_values else None,\n",
        "                \"std\": float(np.std(ndvi_values)) if ndvi_values else None,\n",
        "                \"count\": len(ndvi_values)\n",
        "            },\n",
        "            \"ndvi_image\": sirup_data.get(\"ndvi_img\"),\n",
        "            \"metadata\": sirup_data.get(\"metadata\")\n",
        "        }\n",
        "        \n",
        "        bite = BITE.create(\n",
        "            bite_type=\"imagery_sirup\",\n",
        "            geoid=geoid,\n",
        "            body=body,\n",
        "            source={\n",
        "                \"pipeline\": \"TAP-terrapipe-v1\",\n",
        "                \"vendor\": \"terrapipe.io\",\n",
        "                \"auto_generated\": True\n",
        "            },\n",
        "            tags=[\"satellite\", \"ndvi\", \"vegetation\", \"automated\", \"polygon\"]\n",
        "        )\n",
        "        \n",
        "        return bite\n",
        "\n",
        "# Initialize TAP\n",
        "tap = TAPClient()\n",
        "print(\"‚úì TAP Client initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test TAP with Real terrapipe.io Data\n",
        "print(\"üõ∞Ô∏è Fetching real SIRUP data from terrapipe.io...\")\n",
        "\n",
        "# Get available dates for the test GeoID\n",
        "dates = tap.get_sirup_dates(TEST_GEOID, \"2024-10-01\", \"2024-10-31\")\n",
        "print(f\"\\n‚úì Available SIRUP dates for test GeoID: {len(dates)}\")\n",
        "if dates:\n",
        "    print(f\"  Sample dates: {dates[:5]}\")\n",
        "    \n",
        "    # Create SIRUP BITE from real data\n",
        "    test_date = dates[0]\n",
        "    print(f\"\\nüì° Creating SIRUP BITE for {test_date}...\")\n",
        "    sirup_bite = tap.sirup_to_bite(TEST_GEOID, test_date)\n",
        "    \n",
        "    if sirup_bite:\n",
        "        print(f\"\\n‚úì SIRUP BITE created successfully!\")\n",
        "        print(f\"  BITE ID: {sirup_bite['Header']['id']}\")\n",
        "        print(f\"  Type: {sirup_bite['Header']['type']}\")\n",
        "        print(f\"  NDVI Stats: {sirup_bite['Body']['ndvi_stats']}\")\n",
        "        print(f\"  Valid: {BITE.validate(sirup_bite)}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Failed to create SIRUP BITE\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No SIRUP dates available for this period\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Generate Synthetic BITE Dataset\n",
        "\n",
        "We'll generate 100 BITEs representing 4 agricultural data types:\n",
        "- **40 Observations** (Point BITEs): Coffee rust, pests, growth anomalies\n",
        "- **30 Satellite Imagery** (Polygon BITEs): NDVI from SIRUP/TAP\n",
        "- **20 Soil Samples** (Point BITEs): Lab analysis results\n",
        "- **10 Pesticide Recommendations** (Polygon BITEs): Spray applications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_geoid_nearby(base_geoid: str, offset_km: float = 1.0) -> str:\n",
        "    \"\"\"\n",
        "    Generate a nearby geoid by offsetting lat/lon\n",
        "    For demo purposes - in production, use Asset Registry API\n",
        "    \"\"\"\n",
        "    # Simplified for demo - real implementation would:\n",
        "    # 1. GET /fetch-field/{geoid} from Asset Registry\n",
        "    # 2. Parse WKT polygon\n",
        "    # 3. Offset coordinates\n",
        "    # 4. POST new polygon to Asset Registry\n",
        "    # 5. Receive new geoid\n",
        "    seed = f\"{base_geoid}_{offset_km}_{np.random.random()}\"\n",
        "    return hashlib.sha256(seed.encode()).hexdigest()\n",
        "\n",
        "def generate_synthetic_bites(n: int = 100, base_geoid: str = TEST_GEOID) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Generate 100 synthetic BITEs for POC demo\"\"\"\n",
        "    bites = []\n",
        "    \n",
        "    # Distribution: 40 observations, 30 SIRUP, 20 soil, 10 pesticide\n",
        "    distributions = [\n",
        "        (\"observation\", 40),\n",
        "        (\"imagery_sirup\", 30),\n",
        "        (\"soil_sample\", 20),\n",
        "        (\"pesticide_recommendation\", 10)\n",
        "    ]\n",
        "    \n",
        "    for bite_type, count in distributions:\n",
        "        for i in range(count):\n",
        "            # Vary geoid for spatial diversity\n",
        "            if i % 3 == 0:\n",
        "                geoid = base_geoid\n",
        "            else:\n",
        "                geoid = generate_geoid_nearby(base_geoid, offset_km=i*0.5)\n",
        "            \n",
        "            # Vary timestamp for temporal diversity (0-90 days ago)\n",
        "            days_ago = np.random.randint(0, 90)\n",
        "            timestamp = (datetime.utcnow() - timedelta(days=days_ago)).isoformat() + \"Z\"\n",
        "            \n",
        "            if bite_type == \"observation\":\n",
        "                body = {\n",
        "                    \"observation_type\": np.random.choice([\"disease\", \"pest\", \"growth\", \"harvest\"]),\n",
        "                    \"crop\": \"coffee\",\n",
        "                    \"disease\": np.random.choice([\"coffee_rust\", \"coffee_borer\", \"leaf_miner\", None]),\n",
        "                    \"severity\": np.random.choice([\"low\", \"moderate\", \"high\", \"severe\"]),\n",
        "                    \"affected_area_pct\": float(np.random.randint(5, 60)),\n",
        "                    \"notes\": f\"Field observation #{i+1}\"\n",
        "                }\n",
        "                tags = [\"field-observation\", \"point\"]\n",
        "            \n",
        "            elif bite_type == \"imagery_sirup\":\n",
        "                body = {\n",
        "                    \"sirup_type\": \"satellite_ndvi\",\n",
        "                    \"vendor\": \"terrapipe.io\",\n",
        "                    \"date\": (datetime.utcnow() - timedelta(days=days_ago)).strftime(\"%Y-%m-%d\"),\n",
        "                    \"ndvi_stats\": {\n",
        "                        \"mean\": float(np.random.uniform(0.2, 0.8)),\n",
        "                        \"min\": float(np.random.uniform(0.0, 0.3)),\n",
        "                        \"max\": float(np.random.uniform(0.7, 1.0)),\n",
        "                        \"std\": float(np.random.uniform(0.05, 0.15)),\n",
        "                        \"count\": int(np.random.randint(100, 500))\n",
        "                    }\n",
        "                }\n",
        "                tags = [\"satellite\", \"ndvi\", \"automated\", \"polygon\"]\n",
        "            \n",
        "            elif bite_type == \"soil_sample\":\n",
        "                body = {\n",
        "                    \"sample_type\": \"lab_analysis\",\n",
        "                    \"ph\": float(np.random.uniform(5.5, 7.5)),\n",
        "                    \"nitrogen_ppm\": float(np.random.uniform(10, 50)),\n",
        "                    \"phosphorus_ppm\": float(np.random.uniform(5, 30)),\n",
        "                    \"potassium_ppm\": float(np.random.uniform(50, 200)),\n",
        "                    \"organic_matter_pct\": float(np.random.uniform(2, 8)),\n",
        "                    \"sample_depth_cm\": float(np.random.choice([15, 30, 45]))\n",
        "                }\n",
        "                tags = [\"soil\", \"lab-result\", \"point\"]\n",
        "            \n",
        "            else:  # pesticide_recommendation\n",
        "                body = {\n",
        "                    \"recommendation_type\": \"pesticide_spray\",\n",
        "                    \"target\": np.random.choice([\"coffee_rust\", \"coffee_borer\", \"leaf_miner\", \"nematodes\"]),\n",
        "                    \"product\": f\"Product-{np.random.choice(['CopperOxychloride', 'Propiconazole', 'Cyproconazole'])}\",\n",
        "                    \"dosage_per_hectare\": float(np.random.uniform(1.0, 5.0)),\n",
        "                    \"timing\": np.random.choice([\"morning\", \"evening\", \"night\"]),\n",
        "                    \"weather_conditions\": \"dry, no rain forecast 48h\",\n",
        "                    \"application_method\": np.random.choice([\"backpack_sprayer\", \"tractor_boom\", \"drone\"])\n",
        "                }\n",
        "                tags = [\"recommendation\", \"pesticide\", \"polygon\"]\n",
        "            \n",
        "            bite = BITE.create(\n",
        "                bite_type=bite_type,\n",
        "                geoid=geoid,\n",
        "                body=body,\n",
        "                timestamp=timestamp,\n",
        "                tags=tags\n",
        "            )\n",
        "            \n",
        "            bites.append(bite)\n",
        "    \n",
        "    return bites\n",
        "\n",
        "# Generate dataset\n",
        "print(\"üîÑ Generating 100 synthetic BITEs...\")\n",
        "synthetic_bites = generate_synthetic_bites(100)\n",
        "print(f\"‚úì Generated {len(synthetic_bites)} BITEs\")\n",
        "\n",
        "# Summary\n",
        "bite_types = {}\n",
        "for bite in synthetic_bites:\n",
        "    bt = bite[\"Header\"][\"type\"]\n",
        "    bite_types[bt] = bite_types.get(bt, 0) + 1\n",
        "\n",
        "print(\"\\nüìä BITE Distribution:\")\n",
        "for bt, count in sorted(bite_types.items()):\n",
        "    print(f\"  {bt}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show examples of each BITE type\n",
        "print(\"\\\\nüìã Sample BITEs:\\\\n\")\n",
        "for bt in [\"observation\", \"imagery_sirup\", \"soil_sample\", \"pesticide_recommendation\"]:\n",
        "    sample = next(b for b in synthetic_bites if b[\"Header\"][\"type\"] == bt)\n",
        "    print(f\"\\\\n{bt.upper()}:\")\n",
        "    print(f\"  ID: {sample['Header']['id']}\")\n",
        "    print(f\"  GeoID: {sample['Header']['geoid'][:16]}...\")\n",
        "    print(f\"  Timestamp: {sample['Header']['timestamp']}\")\n",
        "    print(f\"  Body Preview: {json.dumps(sample['Body'], indent=4)[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Setup Parallel Databases\n",
        "\n",
        "We'll create two databases for comparison:\n",
        "1. **PANCAKE**: AI-native, single table, JSONB body, pgvector embeddings\n",
        "2. **Traditional**: Relational, 4 normalized tables, fixed schema\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_pancake_db():\n",
        "    \"\"\"Setup PANCAKE database with AI-native structure\"\"\"\n",
        "    try:\n",
        "        conn = psycopg2.connect(PANCAKE_DB)\n",
        "        cur = conn.cursor()\n",
        "        \n",
        "        # Create pgvector extension\n",
        "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
        "        \n",
        "        # Drop existing table if it exists\n",
        "        cur.execute(\"DROP TABLE IF EXISTS bites CASCADE;\")\n",
        "        \n",
        "        # Single table for all BITEs - polyglot data in one place\n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE bites (\n",
        "                id TEXT PRIMARY KEY,\n",
        "                geoid TEXT NOT NULL,\n",
        "                timestamp TIMESTAMPTZ NOT NULL,\n",
        "                type TEXT NOT NULL,\n",
        "                header JSONB NOT NULL,\n",
        "                body JSONB NOT NULL,\n",
        "                footer JSONB NOT NULL,\n",
        "                embedding vector(1536),\n",
        "                created_at TIMESTAMPTZ DEFAULT NOW()\n",
        "            );\n",
        "        \"\"\")\n",
        "        \n",
        "        # Indexes for performance\n",
        "        cur.execute(\"CREATE INDEX idx_geoid ON bites(geoid);\")\n",
        "        cur.execute(\"CREATE INDEX idx_timestamp ON bites(timestamp);\")\n",
        "        cur.execute(\"CREATE INDEX idx_type ON bites(type);\")\n",
        "        cur.execute(\"CREATE INDEX idx_geoid_time ON bites(geoid, timestamp);\")\n",
        "        cur.execute(\"CREATE INDEX idx_body_gin ON bites USING GIN (body);\")\n",
        "        \n",
        "        conn.commit()\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "        \n",
        "        print(\"‚úì PANCAKE database setup complete\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è PANCAKE database setup failed: {e}\")\n",
        "        print(\"  (This is OK if PostgreSQL is not running - demo will continue)\")\n",
        "        return False\n",
        "\n",
        "# Run setup\n",
        "pancake_ready = setup_pancake_db()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_traditional_db():\n",
        "    \"\"\"Setup traditional relational database with normalized schema\"\"\"\n",
        "    try:\n",
        "        conn = psycopg2.connect(TRADITIONAL_DB)\n",
        "        cur = conn.cursor()\n",
        "        \n",
        "        # Drop existing tables\n",
        "        cur.execute(\"DROP TABLE IF EXISTS observations CASCADE;\")\n",
        "        cur.execute(\"DROP TABLE IF EXISTS satellite_imagery CASCADE;\")\n",
        "        cur.execute(\"DROP TABLE IF EXISTS soil_samples CASCADE;\")\n",
        "        cur.execute(\"DROP TABLE IF EXISTS pesticide_recommendations CASCADE;\")\n",
        "        \n",
        "        # Separate table for each data type - traditional relational approach\n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE observations (\n",
        "                id TEXT PRIMARY KEY,\n",
        "                geoid TEXT NOT NULL,\n",
        "                timestamp TIMESTAMPTZ NOT NULL,\n",
        "                observation_type TEXT,\n",
        "                crop TEXT,\n",
        "                disease TEXT,\n",
        "                severity TEXT,\n",
        "                affected_area_pct FLOAT,\n",
        "                notes TEXT\n",
        "            );\n",
        "        \"\"\")\n",
        "        \n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE satellite_imagery (\n",
        "                id TEXT PRIMARY KEY,\n",
        "                geoid TEXT NOT NULL,\n",
        "                timestamp TIMESTAMPTZ NOT NULL,\n",
        "                vendor TEXT,\n",
        "                date TEXT,\n",
        "                ndvi_mean FLOAT,\n",
        "                ndvi_min FLOAT,\n",
        "                ndvi_max FLOAT,\n",
        "                ndvi_std FLOAT,\n",
        "                ndvi_count INT\n",
        "            );\n",
        "        \"\"\")\n",
        "        \n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE soil_samples (\n",
        "                id TEXT PRIMARY KEY,\n",
        "                geoid TEXT NOT NULL,\n",
        "                timestamp TIMESTAMPTZ NOT NULL,\n",
        "                sample_type TEXT,\n",
        "                ph FLOAT,\n",
        "                nitrogen_ppm FLOAT,\n",
        "                phosphorus_ppm FLOAT,\n",
        "                potassium_ppm FLOAT,\n",
        "                organic_matter_pct FLOAT,\n",
        "                sample_depth_cm FLOAT\n",
        "            );\n",
        "        \"\"\")\n",
        "        \n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE pesticide_recommendations (\n",
        "                id TEXT PRIMARY KEY,\n",
        "                geoid TEXT NOT NULL,\n",
        "                timestamp TIMESTAMPTZ NOT NULL,\n",
        "                recommendation_type TEXT,\n",
        "                target TEXT,\n",
        "                product TEXT,\n",
        "                dosage_per_hectare FLOAT,\n",
        "                timing TEXT,\n",
        "                weather_conditions TEXT,\n",
        "                application_method TEXT\n",
        "            );\n",
        "        \"\"\")\n",
        "        \n",
        "        # Indexes\n",
        "        for table in [\"observations\", \"satellite_imagery\", \"soil_samples\", \"pesticide_recommendations\"]:\n",
        "            cur.execute(f\"CREATE INDEX idx_{table}_geoid ON {table}(geoid);\")\n",
        "            cur.execute(f\"CREATE INDEX idx_{table}_timestamp ON {table}(timestamp);\")\n",
        "        \n",
        "        conn.commit()\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "        \n",
        "        print(\"‚úì Traditional database setup complete\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Traditional database setup failed: {e}\")\n",
        "        print(\"  (This is OK if PostgreSQL is not running - demo will continue)\")\n",
        "        return False\n",
        "\n",
        "# Run setup\n",
        "traditional_ready = setup_traditional_db()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Multi-Pronged Similarity Index\n",
        "\n",
        "The \"GeoID Magic\" - combining three types of similarity:\n",
        "1. **Semantic**: OpenAI embeddings + cosine similarity\n",
        "2. **Spatial**: S2 geodesic distance between GeoIDs\n",
        "3. **Temporal**: Time delta decay function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Semantic Similarity\n",
        "def get_embedding(text: str, max_retries: int = 3) -> List[float]:\n",
        "    \"\"\"Get OpenAI embedding for text with retry logic\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.embeddings.create(\n",
        "                model=\"text-embedding-3-small\",\n",
        "                input=text[:8000]  # Truncate if too long\n",
        "            )\n",
        "            return response.data[0].embedding\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "            print(f\"Embedding error: {e}\")\n",
        "            # Return zero vector as fallback\n",
        "            return [0.0] * 1536\n",
        "\n",
        "def semantic_similarity(emb1: List[float], emb2: List[float]) -> float:\n",
        "    \"\"\"Cosine similarity between embeddings\"\"\"\n",
        "    dot_product = np.dot(emb1, emb2)\n",
        "    norm1 = np.linalg.norm(emb1)\n",
        "    norm2 = np.linalg.norm(emb2)\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return float(dot_product / (norm1 * norm2))\n",
        "\n",
        "print(\"‚úì Semantic similarity functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Spatial Similarity (using S2 geometry behind the scenes via GeoID)\n",
        "def geoid_to_centroid(geoid: str) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Convert GeoID to centroid lat/lon\n",
        "    In production: call Asset Registry API to get WKT, then compute centroid\n",
        "    For demo: use approximate location\n",
        "    \"\"\"\n",
        "    # In production:\n",
        "    # 1. GET https://api-ar.agstack.org/fetch-field/{geoid}\n",
        "    # 2. Parse WKT polygon\n",
        "    # 3. Compute centroid using shapely\n",
        "    # 4. Return (lat, lon)\n",
        "    \n",
        "    # For demo: return approximate UAE location for test geoid\n",
        "    if geoid == TEST_GEOID:\n",
        "        return (24.536, 54.427)\n",
        "    else:\n",
        "        # Vary slightly for synthetic geoids\n",
        "        hash_val = int(geoid[:8], 16) if len(geoid) >= 8 else 0\n",
        "        lat_offset = (hash_val % 100) / 1000.0  # 0-0.1 degree variation\n",
        "        lon_offset = ((hash_val >> 8) % 100) / 1000.0\n",
        "        return (24.536 + lat_offset, 54.427 + lon_offset)\n",
        "\n",
        "def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
        "    \"\"\"Calculate geodesic distance in km using Haversine formula\"\"\"\n",
        "    R = 6371  # Earth radius in km\n",
        "    dlat = np.radians(lat2 - lat1)\n",
        "    dlon = np.radians(lon2 - lon1)\n",
        "    a = (np.sin(dlat/2)**2 + \n",
        "         np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon/2)**2)\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "    return R * c\n",
        "\n",
        "def spatial_similarity(geoid1: str, geoid2: str) -> float:\n",
        "    \"\"\"\n",
        "    Spatial similarity based on geodesic distance\n",
        "    Returns value between 0 (far) and 1 (same location)\n",
        "    Uses S2 geometry indirectly through GeoID centroid\n",
        "    \"\"\"\n",
        "    if geoid1 == geoid2:\n",
        "        return 1.0\n",
        "    \n",
        "    lat1, lon1 = geoid_to_centroid(geoid1)\n",
        "    lat2, lon2 = geoid_to_centroid(geoid2)\n",
        "    \n",
        "    distance_km = haversine_distance(lat1, lon1, lat2, lon2)\n",
        "    \n",
        "    # Exponential decay: same location = 1.0, 10km = ~0.37, 50km = ~0.007\n",
        "    # This is the \"GeoID magic\" - automatic spatial relationships\n",
        "    similarity = float(np.exp(-distance_km / 10.0))\n",
        "    return similarity\n",
        "\n",
        "print(\"‚úì Spatial similarity functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Temporal Similarity\n",
        "def temporal_similarity(ts1: str, ts2: str) -> float:\n",
        "    \"\"\"\n",
        "    Temporal similarity based on time delta\n",
        "    Returns value between 0 (far apart) and 1 (same time)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dt1 = datetime.fromisoformat(ts1.replace('Z', '+00:00'))\n",
        "        dt2 = datetime.fromisoformat(ts2.replace('Z', '+00:00'))\n",
        "        \n",
        "        delta_days = abs((dt2 - dt1).days)\n",
        "        \n",
        "        # Exponential decay: same day = 1.0, 7 days = ~0.37, 30 days = ~0.02\n",
        "        similarity = float(np.exp(-delta_days / 7.0))\n",
        "        return similarity\n",
        "    except Exception as e:\n",
        "        return 0.0\n",
        "\n",
        "print(\"‚úì Temporal similarity function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Combined Multi-Pronged Similarity\n",
        "def multi_pronged_similarity(\n",
        "    bite1: Dict[str, Any],\n",
        "    bite2: Dict[str, Any],\n",
        "    weights: Dict[str, float] = None,\n",
        "    embeddings: Dict[str, List[float]] = None\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Compute multi-pronged similarity: semantic + spatial + temporal\n",
        "    \n",
        "    This is the core innovation - combining three types of distance\n",
        "    to find truly relevant data across polyglot sources\n",
        "    \n",
        "    Returns: (total_similarity, component_scores)\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        # Default equal weighting\n",
        "        weights = {\"semantic\": 0.33, \"spatial\": 0.33, \"temporal\": 0.34}\n",
        "    \n",
        "    bite1_id = bite1[\"Header\"][\"id\"]\n",
        "    bite2_id = bite2[\"Header\"][\"id\"]\n",
        "    \n",
        "    # Semantic similarity\n",
        "    if embeddings and bite1_id in embeddings and bite2_id in embeddings:\n",
        "        sem_sim = semantic_similarity(embeddings[bite1_id], embeddings[bite2_id])\n",
        "    else:\n",
        "        # Fallback: compute on the fly\n",
        "        text1 = f\"{bite1['Header']['type']}: {json.dumps(bite1['Body'])}\"\n",
        "        text2 = f\"{bite2['Header']['type']}: {json.dumps(bite2['Body'])}\"\n",
        "        emb1 = get_embedding(text1)\n",
        "        emb2 = get_embedding(text2)\n",
        "        sem_sim = semantic_similarity(emb1, emb2)\n",
        "    \n",
        "    # Spatial similarity (via GeoID)\n",
        "    geoid1 = bite1[\"Header\"][\"geoid\"]\n",
        "    geoid2 = bite2[\"Header\"][\"geoid\"]\n",
        "    spat_sim = spatial_similarity(geoid1, geoid2)\n",
        "    \n",
        "    # Temporal similarity\n",
        "    ts1 = bite1[\"Header\"][\"timestamp\"]\n",
        "    ts2 = bite1[\"Header\"][\"timestamp\"]\n",
        "    temp_sim = temporal_similarity(ts1, ts2)\n",
        "    \n",
        "    # Weighted combination\n",
        "    total_sim = (\n",
        "        weights[\"semantic\"] * sem_sim +\n",
        "        weights[\"spatial\"] * spat_sim +\n",
        "        weights[\"temporal\"] * temp_sim\n",
        "    )\n",
        "    \n",
        "    components = {\n",
        "        \"semantic\": sem_sim,\n",
        "        \"spatial\": spat_sim,\n",
        "        \"temporal\": temp_sim\n",
        "    }\n",
        "    \n",
        "    return total_sim, components\n",
        "\n",
        "print(\"‚úì Multi-pronged similarity function defined\")\n",
        "print(\"\\\\nüéØ This is the 'GeoID Magic' - automatic spatio-temporal relationships!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Test multi-pronged similarity\n",
        "print(\"\\\\nüß™ Testing Multi-Pronged Similarity:\\\\n\")\n",
        "\n",
        "# Pick two BITEs - one observation, one soil sample at same location\n",
        "obs_bite = next(b for b in synthetic_bites if b[\"Header\"][\"type\"] == \"observation\" and b[\"Header\"][\"geoid\"] == TEST_GEOID)\n",
        "soil_bite = next(b for b in synthetic_bites if b[\"Header\"][\"type\"] == \"soil_sample\" and b[\"Header\"][\"geoid\"] == TEST_GEOID)\n",
        "\n",
        "total_sim, components = multi_pronged_similarity(obs_bite, soil_bite)\n",
        "\n",
        "print(f\"Comparing:\")\n",
        "print(f\"  BITE 1: {obs_bite['Header']['type']} at {obs_bite['Header']['timestamp'][:10]}\")\n",
        "print(f\"  BITE 2: {soil_bite['Header']['type']} at {soil_bite['Header']['timestamp'][:10]}\")\n",
        "print(f\"\\\\nSimilarity Components:\")\n",
        "print(f\"  Semantic:  {components['semantic']:.3f}\")\n",
        "print(f\"  Spatial:   {components['spatial']:.3f} (same GeoID)\")\n",
        "print(f\"  Temporal:  {components['temporal']:.3f}\")\n",
        "print(f\"  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
        "print(f\"  Total:     {total_sim:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Load Data into Databases\n",
        "\n",
        "Now we'll load our 100 synthetic BITEs into both databases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_into_pancake(bites: List[Dict[str, Any]], batch_size: int = 20):\n",
        "    \"\"\"Load BITEs into PANCAKE database with embeddings\"\"\"\n",
        "    if not pancake_ready:\n",
        "        print(\"‚ö†Ô∏è Skipping PANCAKE load - database not available\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        conn = psycopg2.connect(PANCAKE_DB)\n",
        "        cur = conn.cursor()\n",
        "        \n",
        "        print(f\"üîÑ Loading {len(bites)} BITEs into PANCAKE...\")\n",
        "        \n",
        "        for i, bite in enumerate(bites):\n",
        "            if i % 20 == 0:\n",
        "                print(f\"  Progress: {i}/{len(bites)}\")\n",
        "            \n",
        "            # Create text for embedding\n",
        "            text = f\"{bite['Header']['type']}: {json.dumps(bite['Body'])}\"\n",
        "            embedding = get_embedding(text)\n",
        "            \n",
        "            # Insert\n",
        "            cur.execute(\"\"\"\n",
        "                INSERT INTO bites (id, geoid, timestamp, type, header, body, footer, embedding)\n",
        "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
        "                ON CONFLICT (id) DO NOTHING\n",
        "            \"\"\", (\n",
        "                bite[\"Header\"][\"id\"],\n",
        "                bite[\"Header\"][\"geoid\"],\n",
        "                bite[\"Header\"][\"timestamp\"],\n",
        "                bite[\"Header\"][\"type\"],\n",
        "                Json(bite[\"Header\"]),\n",
        "                Json(bite[\"Body\"]),\n",
        "                Json(bite[\"Footer\"]),\n",
        "                embedding\n",
        "            ))\n",
        "            \n",
        "            # Rate limit OpenAI API\n",
        "            if i % 10 == 0 and i > 0:\n",
        "                time.sleep(0.5)\n",
        "        \n",
        "        conn.commit()\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "        \n",
        "        print(f\"‚úì Loaded {len(bites)} BITEs into PANCAKE\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error loading into PANCAKE: {e}\")\n",
        "        return False\n",
        "\n",
        "# Load data\n",
        "pancake_loaded = load_into_pancake(synthetic_bites)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_into_traditional(bites: List[Dict[str, Any]]):\n",
        "    \"\"\"Load BITEs into traditional relational database\"\"\"\n",
        "    if not traditional_ready:\n",
        "        print(\"‚ö†Ô∏è Skipping Traditional DB load - database not available\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        conn = psycopg2.connect(TRADITIONAL_DB)\n",
        "        cur = conn.cursor()\n",
        "        \n",
        "        print(f\"üîÑ Loading {len(bites)} records into Traditional DB...\")\n",
        "        \n",
        "        for bite in bites:\n",
        "            bite_id = bite[\"Header\"][\"id\"]\n",
        "            geoid = bite[\"Header\"][\"geoid\"]\n",
        "            timestamp = bite[\"Header\"][\"timestamp\"]\n",
        "            bite_type = bite[\"Header\"][\"type\"]\n",
        "            body = bite[\"Body\"]\n",
        "            \n",
        "            if bite_type == \"observation\":\n",
        "                cur.execute(\"\"\"\n",
        "                    INSERT INTO observations \n",
        "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
        "                    ON CONFLICT (id) DO NOTHING\n",
        "                \"\"\", (\n",
        "                    bite_id, geoid, timestamp,\n",
        "                    body.get(\"observation_type\"),\n",
        "                    body.get(\"crop\"),\n",
        "                    body.get(\"disease\"),\n",
        "                    body.get(\"severity\"),\n",
        "                    body.get(\"affected_area_pct\"),\n",
        "                    body.get(\"notes\")\n",
        "                ))\n",
        "            \n",
        "            elif bite_type == \"imagery_sirup\":\n",
        "                stats = body.get(\"ndvi_stats\", {})\n",
        "                cur.execute(\"\"\"\n",
        "                    INSERT INTO satellite_imagery\n",
        "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
        "                    ON CONFLICT (id) DO NOTHING\n",
        "                \"\"\", (\n",
        "                    bite_id, geoid, timestamp,\n",
        "                    body.get(\"vendor\"),\n",
        "                    body.get(\"date\"),\n",
        "                    stats.get(\"mean\"),\n",
        "                    stats.get(\"min\"),\n",
        "                    stats.get(\"max\"),\n",
        "                    stats.get(\"std\"),\n",
        "                    stats.get(\"count\")\n",
        "                ))\n",
        "            \n",
        "            elif bite_type == \"soil_sample\":\n",
        "                cur.execute(\"\"\"\n",
        "                    INSERT INTO soil_samples\n",
        "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
        "                    ON CONFLICT (id) DO NOTHING\n",
        "                \"\"\", (\n",
        "                    bite_id, geoid, timestamp,\n",
        "                    body.get(\"sample_type\"),\n",
        "                    body.get(\"ph\"),\n",
        "                    body.get(\"nitrogen_ppm\"),\n",
        "                    body.get(\"phosphorus_ppm\"),\n",
        "                    body.get(\"potassium_ppm\"),\n",
        "                    body.get(\"organic_matter_pct\"),\n",
        "                    body.get(\"sample_depth_cm\")\n",
        "                ))\n",
        "            \n",
        "            elif bite_type == \"pesticide_recommendation\":\n",
        "                cur.execute(\"\"\"\n",
        "                    INSERT INTO pesticide_recommendations\n",
        "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
        "                    ON CONFLICT (id) DO NOTHING\n",
        "                \"\"\", (\n",
        "                    bite_id, geoid, timestamp,\n",
        "                    body.get(\"recommendation_type\"),\n",
        "                    body.get(\"target\"),\n",
        "                    body.get(\"product\"),\n",
        "                    body.get(\"dosage_per_hectare\"),\n",
        "                    body.get(\"timing\"),\n",
        "                    body.get(\"weather_conditions\"),\n",
        "                    body.get(\"application_method\")\n",
        "                ))\n",
        "        \n",
        "        conn.commit()\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "        \n",
        "        print(f\"‚úì Loaded {len(bites)} records into Traditional DB\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error loading into Traditional DB: {e}\")\n",
        "        return False\n",
        "\n",
        "# Load data\n",
        "traditional_loaded = load_into_traditional(synthetic_bites)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Performance Benchmarks - PANCAKE vs Traditional\n",
        "\n",
        "We'll test 5 levels of query complexity to demonstrate the advantages of the AI-native approach\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define benchmark queries\n",
        "test_date_30d = (datetime.utcnow() - timedelta(days=30)).isoformat()\n",
        "test_date_7d = (datetime.utcnow() - timedelta(days=7)).isoformat()\n",
        "\n",
        "benchmark_results = {\n",
        "    \"level\": [],\n",
        "    \"description\": [],\n",
        "    \"pancake_time_ms\": [],\n",
        "    \"traditional_time_ms\": [],\n",
        "    \"speedup\": [],\n",
        "    \"query_type\": []\n",
        "}\n",
        "\n",
        "def run_benchmark(level: int, description: str, query_type: str, pancake_fn, traditional_fn):\n",
        "    \"\"\"Run a benchmark query on both databases\"\"\"\n",
        "    print(f\"\\\\nüèÉ Level {level}: {description}\")\n",
        "    \n",
        "    # Skip if databases not ready\n",
        "    if not (pancake_ready and traditional_ready):\n",
        "        print(\"  ‚ö†Ô∏è Skipping - databases not available\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        # Run PANCAKE query\n",
        "        start = time.time()\n",
        "        p_results = pancake_fn()\n",
        "        pancake_time = (time.time() - start) * 1000\n",
        "        \n",
        "        # Run Traditional query\n",
        "        start = time.time()\n",
        "        t_results = traditional_fn()\n",
        "        traditional_time = (time.time() - start) * 1000\n",
        "        \n",
        "        speedup = traditional_time / pancake_time if pancake_time > 0 else 0\n",
        "        \n",
        "        print(f\"  PANCAKE:     {len(p_results)} results in {pancake_time:.2f}ms\")\n",
        "        print(f\"  Traditional: {len(t_results)} results in {traditional_time:.2f}ms\")\n",
        "        print(f\"  Speedup:     {speedup:.2f}x\")\n",
        "        \n",
        "        benchmark_results[\"level\"].append(level)\n",
        "        benchmark_results[\"description\"].append(description)\n",
        "        benchmark_results[\"pancake_time_ms\"].append(pancake_time)\n",
        "        benchmark_results[\"traditional_time_ms\"].append(traditional_time)\n",
        "        benchmark_results[\"speedup\"].append(speedup)\n",
        "        benchmark_results[\"query_type\"].append(query_type)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Benchmark error: {e}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"PERFORMANCE BENCHMARKS: PANCAKE vs TRADITIONAL\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Level 1: Simple temporal query\n",
        "def level1_pancake():\n",
        "    conn = psycopg2.connect(PANCAKE_DB)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, type, geoid, timestamp\n",
        "        FROM bites\n",
        "        WHERE timestamp >= %s AND type = 'observation'\n",
        "        ORDER BY timestamp DESC\n",
        "    \"\"\", (test_date_30d,))\n",
        "    results = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "def level1_traditional():\n",
        "    conn = psycopg2.connect(TRADITIONAL_DB)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, geoid, timestamp\n",
        "        FROM observations\n",
        "        WHERE timestamp >= %s\n",
        "        ORDER BY timestamp DESC\n",
        "    \"\"\", (test_date_30d,))\n",
        "    results = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "run_benchmark(1, \"Temporal Query (observations from last 30 days)\", \"temporal\", level1_pancake, level1_traditional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Level 2: Spatial query\n",
        "def level2_pancake():\n",
        "    conn = psycopg2.connect(PANCAKE_DB)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, geoid, body\n",
        "        FROM bites\n",
        "        WHERE geoid = %s AND type = 'soil_sample'\n",
        "        ORDER BY timestamp DESC\n",
        "        LIMIT 10\n",
        "    \"\"\", (TEST_GEOID,))\n",
        "    results = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "def level2_traditional():\n",
        "    conn = psycopg2.connect(TRADITIONAL_DB)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, geoid, ph, nitrogen_ppm, organic_matter_pct\n",
        "        FROM soil_samples\n",
        "        WHERE geoid = %s\n",
        "        ORDER BY timestamp DESC\n",
        "        LIMIT 10\n",
        "    \"\"\", (TEST_GEOID,))\n",
        "    results = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "run_benchmark(2, \"Spatial Query (soil samples at specific GeoID)\", \"spatial\", level2_pancake, level2_traditional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Level 3: Multi-type polyglot query\n",
        "def level3_pancake():\n",
        "    conn = psycopg2.connect(PANCAKE_DB)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, type, geoid, timestamp, body\n",
        "        FROM bites\n",
        "        WHERE geoid = %s\n",
        "        AND timestamp >= %s\n",
        "        AND type IN ('observation', 'imagery_sirup', 'soil_sample')\n",
        "        ORDER BY timestamp DESC\n",
        "    \"\"\", (TEST_GEOID, test_date_30d))\n",
        "    results = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "def level3_traditional():\n",
        "    conn = psycopg2.connect(TRADITIONAL_DB)\n",
        "    cur = conn.cursor()\n",
        "    # Requires UNION across 3 tables\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, 'observation' as type, geoid, timestamp\n",
        "        FROM observations\n",
        "        WHERE geoid = %s AND timestamp >= %s\n",
        "        UNION ALL\n",
        "        SELECT id, 'imagery' as type, geoid, timestamp\n",
        "        FROM satellite_imagery\n",
        "        WHERE geoid = %s AND timestamp >= %s\n",
        "        UNION ALL\n",
        "        SELECT id, 'soil' as type, geoid, timestamp\n",
        "        FROM soil_samples\n",
        "        WHERE geoid = %s AND timestamp >= %s\n",
        "        ORDER BY timestamp DESC\n",
        "    \"\"\", (TEST_GEOID, test_date_30d, TEST_GEOID, test_date_30d, TEST_GEOID, test_date_30d))\n",
        "    results = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "run_benchmark(3, \"Multi-Type Polyglot Query (3 data types, 1 location)\", \"polyglot\", level3_pancake, level3_traditional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Level 4: JSONB query (schema-less advantage)\n",
        "def level4_pancake():\n",
        "    conn = psycopg2.connect(PANCAKE_DB)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, type, body\n",
        "        FROM bites\n",
        "        WHERE body @> '{\"severity\": \"high\"}'\n",
        "        OR body @> '{\"severity\": \"severe\"}'\n",
        "        ORDER BY timestamp DESC\n",
        "    \"\"\")\n",
        "    results = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "def level4_traditional():\n",
        "    conn = psycopg2.connect(TRADITIONAL_DB)\n",
        "    cur = conn.cursor()\n",
        "    # Can only query observations table - schema limitation\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, 'observation' as type, severity\n",
        "        FROM observations\n",
        "        WHERE severity IN ('high', 'severe')\n",
        "        ORDER BY timestamp DESC\n",
        "    \"\"\")\n",
        "    results = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "run_benchmark(4, \"Schema-less Query (severity across all types)\", \"jsonb\", level4_pancake, level4_traditional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Level 5: Complex spatio-temporal aggregate\n",
        "def level5_pancake():\n",
        "    conn = psycopg2.connect(PANCAKE_DB)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT \n",
        "            type,\n",
        "            COUNT(*) as count,\n",
        "            MIN(timestamp) as earliest,\n",
        "            MAX(timestamp) as latest\n",
        "        FROM bites\n",
        "        WHERE timestamp >= %s\n",
        "        GROUP BY type\n",
        "        ORDER BY count DESC\n",
        "    \"\"\", (test_date_30d,))\n",
        "    results = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "def level5_traditional():\n",
        "    conn = psycopg2.connect(TRADITIONAL_DB)\n",
        "    cur = conn.cursor()\n",
        "    # Requires UNION across all 4 tables\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT 'observation' as type, COUNT(*) as count, MIN(timestamp) as earliest, MAX(timestamp) as latest\n",
        "        FROM observations WHERE timestamp >= %s\n",
        "        UNION ALL\n",
        "        SELECT 'imagery' as type, COUNT(*), MIN(timestamp), MAX(timestamp)\n",
        "        FROM satellite_imagery WHERE timestamp >= %s\n",
        "        UNION ALL\n",
        "        SELECT 'soil' as type, COUNT(*), MIN(timestamp), MAX(timestamp)\n",
        "        FROM soil_samples WHERE timestamp >= %s\n",
        "        UNION ALL\n",
        "        SELECT 'pesticide' as type, COUNT(*), MIN(timestamp), MAX(timestamp)\n",
        "        FROM pesticide_recommendations WHERE timestamp >= %s\n",
        "        ORDER BY count DESC\n",
        "    \"\"\", (test_date_30d, test_date_30d, test_date_30d, test_date_30d))\n",
        "    results = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return results\n",
        "\n",
        "run_benchmark(5, \"Complex Aggregate (stats across all types)\", \"aggregate\", level5_pancake, level5_traditional)\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize benchmark results\n",
        "if benchmark_results[\"level\"]:\n",
        "    df_bench = pd.DataFrame(benchmark_results)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Chart 1: Query times\n",
        "    ax1 = axes[0]\n",
        "    x = np.arange(len(df_bench))\n",
        "    width = 0.35\n",
        "    ax1.bar(x - width/2, df_bench['pancake_time_ms'], width, label='PANCAKE', color='#2ecc71')\n",
        "    ax1.bar(x + width/2, df_bench['traditional_time_ms'], width, label='Traditional', color='#e74c3c')\n",
        "    ax1.set_xlabel('Query Level')\n",
        "    ax1.set_ylabel('Time (ms)')\n",
        "    ax1.set_title('Query Performance Comparison')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels([f\"L{i}\" for i in df_bench['level']])\n",
        "    ax1.legend()\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Chart 2: Speedup\n",
        "    ax2 = axes[1]\n",
        "    colors = ['#3498db' if s >= 1 else '#e67e22' for s in df_bench['speedup']]\n",
        "    ax2.bar(x, df_bench['speedup'], color=colors)\n",
        "    ax2.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Break-even')\n",
        "    ax2.set_xlabel('Query Level')\n",
        "    ax2.set_ylabel('Speedup (x)')\n",
        "    ax2.set_title('PANCAKE Speedup vs Traditional')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels([f\"L{i}\" for i in df_bench['level']])\n",
        "    ax2.legend()\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('benchmark_results.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\\\n‚úì Benchmark chart saved: benchmark_results.png\")\n",
        "else:\n",
        "    print(\"\\\\n‚ö†Ô∏è No benchmark results to visualize\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: RAG with Multi-Pronged Similarity\n",
        "\n",
        "Now for the magic - natural language queries powered by semantic + spatial + temporal similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rag_query(\n",
        "    query_text: str,\n",
        "    top_k: int = 5,\n",
        "    geoid_filter: str = None,\n",
        "    time_filter: str = None\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    RAG query using multi-pronged similarity\n",
        "    This is the future - SQL ‚Üí NLP\n",
        "    \"\"\"\n",
        "    if not pancake_loaded:\n",
        "        print(\"‚ö†Ô∏è PANCAKE database not available for RAG queries\")\n",
        "        return []\n",
        "    \n",
        "    try:\n",
        "        conn = psycopg2.connect(PANCAKE_DB)\n",
        "        cur = conn.cursor()\n",
        "        \n",
        "        # Get query embedding\n",
        "        query_embedding = get_embedding(query_text)\n",
        "        \n",
        "        # Build SQL with filters\n",
        "        sql = \"\"\"\n",
        "            SELECT id, geoid, timestamp, type, header, body, footer,\n",
        "                   embedding <=> %s::vector as distance\n",
        "            FROM bites\n",
        "            WHERE 1=1\n",
        "        \"\"\"\n",
        "        params = [query_embedding]\n",
        "        \n",
        "        if geoid_filter:\n",
        "            sql += \" AND geoid = %s\"\n",
        "            params.append(geoid_filter)\n",
        "        \n",
        "        if time_filter:\n",
        "            sql += \" AND timestamp >= %s\"\n",
        "            params.append(time_filter)\n",
        "        \n",
        "        sql += \" ORDER BY distance LIMIT %s\"\n",
        "        params.append(top_k)\n",
        "        \n",
        "        cur.execute(sql, params)\n",
        "        results = cur.fetchall()\n",
        "        \n",
        "        cur.close()\n",
        "        conn.close()\n",
        "        \n",
        "        # Format results\n",
        "        bites = []\n",
        "        for row in results:\n",
        "            bite = {\n",
        "                \"Header\": row[4],\n",
        "                \"Body\": row[5],\n",
        "                \"Footer\": row[6],\n",
        "                \"semantic_distance\": float(row[7])\n",
        "            }\n",
        "            bites.append(bite)\n",
        "        \n",
        "        return bites\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è RAG query error: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"‚úì RAG query function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test RAG Queries\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"RAG QUERIES WITH MULTI-PRONGED SIMILARITY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Query 1: Simple semantic\n",
        "print(\"\\\\nüîç Query 1: 'Show me recent coffee disease reports'\")\n",
        "results1 = rag_query(\"coffee disease reports severe rust\", top_k=3)\n",
        "for i, bite in enumerate(results1, 1):\n",
        "    print(f\"\\\\n  Result {i}:\")\n",
        "    print(f\"    Type: {bite['Header']['type']}\")\n",
        "    print(f\"    GeoID: {bite['Header']['geoid'][:16]}...\")\n",
        "    print(f\"    Time: {bite['Header']['timestamp'][:10]}\")\n",
        "    print(f\"    Semantic Distance: {bite['semantic_distance']:.3f}\")\n",
        "    body_preview = json.dumps(bite['Body'], indent=6)[:150]\n",
        "    print(f\"    Body: {body_preview}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query 2: With spatial filter\n",
        "print(\"\\\\nüîç Query 2: 'What's the vegetation health at this specific field?'\")\n",
        "results2 = rag_query(\n",
        "    \"vegetation health NDVI satellite imagery\", \n",
        "    top_k=3,\n",
        "    geoid_filter=TEST_GEOID\n",
        ")\n",
        "for i, bite in enumerate(results2, 1):\n",
        "    print(f\"\\\\n  Result {i}:\")\n",
        "    print(f\"    Type: {bite['Header']['type']}\")\n",
        "    print(f\"    GeoID: {bite['Header']['geoid'][:16]}... (filtered)\")\n",
        "    print(f\"    Semantic Distance: {bite['semantic_distance']:.3f}\")\n",
        "    if 'ndvi_stats' in bite['Body']:\n",
        "        print(f\"    NDVI Mean: {bite['Body']['ndvi_stats'].get('mean', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query 3: With temporal filter\n",
        "recent_date = (datetime.utcnow() - timedelta(days=14)).isoformat()\n",
        "print(\"\\\\nüîç Query 3: 'Recent soil analysis results with nutrients'\")\n",
        "results3 = rag_query(\n",
        "    \"soil analysis nutrients nitrogen phosphorus pH laboratory\", \n",
        "    top_k=3,\n",
        "    time_filter=recent_date\n",
        ")\n",
        "for i, bite in enumerate(results3, 1):\n",
        "    print(f\"\\\\n  Result {i}:\")\n",
        "    print(f\"    Type: {bite['Header']['type']}\")\n",
        "    print(f\"    Timestamp: {bite['Header']['timestamp'][:10]}\")\n",
        "    print(f\"    Semantic Distance: {bite['semantic_distance']:.3f}\")\n",
        "    if 'ph' in bite['Body']:\n",
        "        print(f\"    pH: {bite['Body'].get('ph', 'N/A')}\")\n",
        "        print(f\"    N: {bite['Body'].get('nitrogen_ppm', 'N/A')} ppm\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: Conversational AI with LLM Integration\n",
        "\n",
        "The ultimate user experience - ask questions in plain English, get intelligent answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_pancake(question: str, geoid: str = None, days_back: int = 30) -> str:\n",
        "    \"\"\"\n",
        "    Ask a natural language question and get AI-synthesized answer\n",
        "    This is the GenAI-era interface - no SQL required!\n",
        "    \"\"\"\n",
        "    # Get relevant BITEs\n",
        "    time_filter = None\n",
        "    if days_back:\n",
        "        time_filter = (datetime.utcnow() - timedelta(days=days_back)).isoformat()\n",
        "    \n",
        "    relevant_bites = rag_query(question, top_k=10, geoid_filter=geoid, time_filter=time_filter)\n",
        "    \n",
        "    if not relevant_bites:\n",
        "        return \"No relevant data found in PANCAKE.\"\n",
        "    \n",
        "    # Build context\n",
        "    context = \"Relevant agricultural data from PANCAKE:\\\\n\\\\n\"\n",
        "    for i, bite in enumerate(relevant_bites, 1):\n",
        "        context += f\"{i}. {bite['Header']['type']} recorded at {bite['Header']['timestamp'][:10]}:\\\\n\"\n",
        "        context += f\"   {json.dumps(bite['Body'], indent=3)[:300]}\\\\n\\\\n\"\n",
        "    \n",
        "    try:\n",
        "        # Ask LLM\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\", \n",
        "                    \"content\": \"You are an agricultural data analyst. Answer questions based on the provided spatio-temporal data from PANCAKE. Be specific, cite data points, and provide actionable insights.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\", \n",
        "                    \"content\": f\"Question: {question}\\\\n\\\\n{context}\"\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=500\n",
        "        )\n",
        "        \n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"LLM error: {e}. Retrieved {len(relevant_bites)} relevant BITEs but couldn't generate answer.\"\n",
        "\n",
        "print(\"‚úì Conversational AI function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Conversational Queries\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"CONVERSATIONAL AI QUERIES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Question 1\n",
        "print(\"\\\\n‚ùì Q1: What diseases or problems are affecting coffee crops this month?\")\n",
        "answer1 = ask_pancake(\"What diseases or problems are affecting coffee crops this month?\", days_back=30)\n",
        "print(f\"\\\\nüí° A1:\\\\n{answer1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question 2\n",
        "print(\"\\\\n‚ùì Q2: What's the vegetation health status based on satellite data?\")\n",
        "answer2 = ask_pancake(\n",
        "    \"What's the NDVI trend and overall vegetation health status for the farm?\",\n",
        "    geoid=TEST_GEOID,\n",
        "    days_back=60\n",
        ")\n",
        "print(f\"\\\\nüí° A2:\\\\n{answer2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question 3\n",
        "print(\"\\\\n‚ùì Q3: Should I apply pesticides based on recent observations and recommendations?\")\n",
        "answer3 = ask_pancake(\n",
        "    \"Based on recent disease observations and existing pesticide recommendations, what action should I take?\",\n",
        "    days_back=14\n",
        ")\n",
        "print(f\"\\\\nüí° A3:\\\\n{answer3}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Summary Statistics\n",
        "print(\"\\\\n\" + \"=\"*70)\n",
        "print(\"üìä POC-Nov20 FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\\\n‚úì BITEs Generated: {len(synthetic_bites)}\")\n",
        "print(f\"  - Observations (Point): {sum(1 for b in synthetic_bites if b['Header']['type'] == 'observation')}\")\n",
        "print(f\"  - SIRUP Imagery (Polygon): {sum(1 for b in synthetic_bites if b['Header']['type'] == 'imagery_sirup')}\")\n",
        "print(f\"  - Soil Samples (Point): {sum(1 for b in synthetic_bites if b['Header']['type'] == 'soil_sample')}\")\n",
        "print(f\"  - Pesticide Recs (Polygon): {sum(1 for b in synthetic_bites if b['Header']['type'] == 'pesticide_recommendation')}\")\n",
        "\n",
        "if pancake_loaded:\n",
        "    print(f\"\\\\n‚úì PANCAKE Database: Loaded successfully\")\n",
        "    print(f\"  - Single table, JSONB body, pgvector embeddings\")\n",
        "    print(f\"  - Multi-pronged similarity index active\")\n",
        "\n",
        "if traditional_loaded:\n",
        "    print(f\"\\\\n‚úì Traditional Database: Loaded successfully\")\n",
        "    print(f\"  - 4 normalized tables, fixed schema\")\n",
        "\n",
        "if benchmark_results[\"level\"]:\n",
        "    avg_speedup = np.mean(benchmark_results[\"speedup\"])\n",
        "    print(f\"\\\\n‚úì Performance Benchmarks: {len(benchmark_results['level'])} tests\")\n",
        "    print(f\"  - Average PANCAKE Speedup: {avg_speedup:.2f}x\")\n",
        "    print(f\"  - Best for: Polyglot queries, JSONB flexibility\")\n",
        "\n",
        "print(f\"\\\\n‚úì RAG Queries: Enabled\")\n",
        "print(f\"  - Semantic similarity via OpenAI embeddings\")\n",
        "print(f\"  - Spatial similarity via GeoID + S2\")\n",
        "print(f\"  - Temporal similarity via time decay\")\n",
        "\n",
        "print(f\"\\\\n‚úì Conversational AI: Enabled\")\n",
        "print(f\"  - Natural language ‚Üí SQL ‚Üí LLM synthesis\")\n",
        "print(f\"  - No coding required for end users\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformative Potential for Agriculture\n",
        "\n",
        "### üå± Why This Matters\n",
        "\n",
        "**1. Interoperability Crisis Solved**\n",
        "- Current: 100+ ag-tech vendors, 100+ data formats\n",
        "- BITE: One universal format for all\n",
        "- Impact: True data portability and ecosystem collaboration\n",
        "\n",
        "**2. AI-Native from Day One**\n",
        "- Current: ETL hell, schema migrations, data silos\n",
        "- PANCAKE: Direct JSON storage, automatic embeddings\n",
        "- Impact: 10x faster to deploy AI/ML on agricultural data\n",
        "\n",
        "**3. Spatial Intelligence Built-In**\n",
        "- Current: PostGIS complexity, manual spatial joins\n",
        "- GeoID: Automatic spatial relationships via S2\n",
        "- Impact: Field agents, satellites, IoT - all spatially linked\n",
        "\n",
        "**4. Vendor-Agnostic Data Pipelines**\n",
        "- Current: Locked into proprietary APIs and formats\n",
        "- TAP/SIRUP: Universal manifold for any data source\n",
        "- Impact: Farmers choose best vendors, data stays portable\n",
        "\n",
        "**5. Natural Language Interface**\n",
        "- Current: SQL experts required, dashboards rigid\n",
        "- RAG + LLM: \"What diseases are spreading?\" ‚Üí Answer\n",
        "- Impact: Every farmer can query their data\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "1. **Open-source BITE specification** (v1.0)\n",
        "2. **TAP vendor SDK** for easy integration\n",
        "3. **PANCAKE reference implementation** (this POC++)\n",
        "4. **Agriculture consortium** for standards adoption\n",
        "5. **White paper** (10 pages) for broader dissemination\n",
        "\n",
        "---\n",
        "\n",
        "### üéâ POC-Nov20 Complete!\n",
        "\n",
        "**Core Message:**  \n",
        "*AI-native spatio-temporal data organization and interaction - for the GenAI and Agentic-era*\n",
        "\n",
        "**Built with:**  \n",
        "BITE + PANCAKE + TAP + SIRUP + GeoID Magic\n",
        "\n",
        "**Demonstrated:**  \n",
        "Polyglot data ‚Üí Multi-pronged RAG ‚Üí Conversational AI\n",
        "\n",
        "**Vision:**  \n",
        "The future of agricultural data is open, interoperable, and AI-ready.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
