STATE OF AI REPORT .
October 9, 2025

Nathan Benaich
AIR STREET CAPITAL .
stateof.ai

airstreet.com

|1

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Follow our writing on

(press.airstreet.com)

If you enjoy reading the State of AI Report, we invite you to read and subscribe to Air Street Press, the home of
our analytical writing, news, and opinions.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

|2

About the authors

Nathan Benaich
Nathan is the General Partner of Air Street Capital, a
venture capital ï¬rm investing in AI-ï¬rst companies. He
runs the Research and Applied AI Summit (RAAIS), the
RAAIS Foundation (funding open-source AI projects), AI
communities in the US and Europe, and Spinout.fyi
(improving university spinout creation). He studied
biology at Williams College and earned a PhD from
Cambridge in cancer research as a Gates Scholar.

stateof.ai 2025

|3

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

State of AI Report 2025 team

Zeke Gillman

Nell Norman

Ryan Tovcimak

Zeke is a Tech Policy Fellow at Stanford,
and co-author of Regulating under
Uncertainty. He previously worked at
Harvard Business School and the DOJ
Antitrust Division, and holds a BA in
Political Science and Philosophy from
the University of Chicago.

Nell is a grad student in Computing at
Imperial College London focusing on
how LLMs could enable scalable vishing
fraud. She previously helped AI teams
build reliable products at AI agent
platform V7 Labs, and has a ï¬rst class
BA from Oxford University.

Ryan is a founder of the AI Stack
Tracker. His work spans red-teaming
frontier models, benchmarking the
global AI competition, and tracking
trends in AI compute and power
demands. He holds a BS in Econ from
Vanderbilt University.

stateof.ai 2025

|4

Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Artiï¬cial intelligence (AI) is a multidisciplinary ï¬eld of science and engineering devoted to creating intelligent machines.
AI acts as a force multiplier for technological progress in our increasingly digital, data-driven world. This is because
everything around us, from culture to consumer products, is ultimately a product of intelligence.

Now in its eighth consecutive year, the State of AI Report is the most widely read and trusted open-access publication
tracking progress in artiï¬cial intelligence. Consider it a curated compilation of the most signiï¬cant and thought-provoking
work from the past 12 months. Our goal is to inform and shape an ongoing conversation about the state of AI, where the
ï¬eld is heading, and what its developments mean for the future.
This yearâ€™s report examines six key dimensions of the AI ecosystem:
- Research: Technological breakthroughs and their capabilities.
- Industry: Areas of commercial application for AI and their business impact.
- Politics: Regulation, economic implications, and the evolving geopolitics of AI.
- Safety: Efforts to identify and mitigate catastrophic risks that highly capable future AI systems could pose.
- Survey: Findings from the largest open-access survey of 1,200 AI practitioners and their AI usage patterns.
- Predictions: Our outlook for the next 12 months, alongside a review of our 2024 forecasts to keep us accountable.
Produced by Nathan Benaich and Air Street Capital team.

stateof.ai 2025

|5

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Deï¬nitions
Artiï¬cial intelligence (AI): a broad discipline with the goal of creating intelligent machines, as opposed to the natural intelligence of humans and
animals. While artiï¬cial general and super intelligence (AGI and ASI) are terms that donâ€™t have agreed upon deï¬nitions, we use them to describe
machines that could match (AGI) and then exceed (ASI) the full range of human cognitive ability across all economically valuable tasks.
AI Agent: an AI-powered system that can take actions in an environment. For example, an LLM that has access to a suite of tools and has to decide
which one to use in order to accomplish a task that it has been prompted to do.
AI Safety: a ï¬eld that studies and attempts to mitigate the risks (minor to catastrophic) which future AI could pose to humanity.
Context window: The number of input tokens that an LLM model can attend to while answer a userâ€™s prompt.
Diffusion: An algorithm that iteratively denoises an artiï¬cially corrupted signal in order to generate new, high-quality outputs. In recent years it has
been at the forefront of image generation and protein design.
Environment: The world an AI agent acts in. It receives the agentâ€™s actions and returns the next observation and often a reward (i.e. a signal of the
action being good or bad). In this context, trajectories are the time-ordered record of an agentâ€™s experience in an environment, typically tuples like
(observation/state, action, reward, next observation) from start to ï¬nish. These trajectories are used for RL.
Function calling / tool use: Structured calls that let models invoke APIs, search, code, or calculators with typed arguments and schemas.
Generative AI: A family of AI systems that are capable of generating new content (e.g. text, images, audio, or 3D assets) based on 'prompts'.
Graphics Processing Unit (GPU): the workhorse AI semiconductor that enables a large number calculations to be computed in parallel.

stateof.ai 2025

|6

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Deï¬nitions
(Large) Language model (LM, LLM): a model trained on vast amounts of (often) textual data to predict the next word in a self-supervised manner.
Mixture-of-Experts (MoE): A model type where only few expert blocks activate per token, giving high capacity at lower compute per step.
Prompt: a user input often written in natural language that is used to instruct an LLM to generate something or take action.
Reasoning model: A model that plans and veriï¬es its thinking as it generates output tokens, often via test-time compute and post-hoc checking. The
modelâ€™s explicit step-by-step reasoning trace (intermediate tokens that lay out calculations, sub-goals, and logical steps en route to an answer) is
called a Chain of Thought (CoT).
Reinforcement learning (RL): an area of ML in which software agents learn goal-oriented behavior by trial and error in an environment that
provides rewards or penalties in response to their actions (called a â€œpolicyâ€) towards achieving that goal.
Test-time compute (or inference-time compute): Spending more inference budget (longer chains, multiple samples, self-consistency) to raise
accuracy without changing weights.
Transformer: a model architecture at the core of most state of the art (SOTA) ML research. It is composed of multiple â€œattentionâ€ layers which learn
which parts of the input data are the most important for a given task. Transformers started in NLP (speciï¬cally machine translation) and
subsequently were expanded into computer vision, audio, and other modalities.
Vision-Language-Action Model (VLAM): a model that jointly learn from visual inputs, natural language, and embodied interactions to not only
interpret and describe the world but also to plan and execute actions within it. Without the actions piece, this model becomes a VLM.
World model: a model that predicts next states conditioned on actions, enabling real-time, interactive control.

stateof.ai 2025

|7

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Deï¬nitions
Model type legend
In the rest of the slides, icons in the top right corner indicate input and output modalities for the model.

Input/Output types:
ğŸ“: Text
ğŸ–¼: Image
</> : Code
ğŸ›  : Software tool use (text, code generation & execution)
ğŸ¥ : Video
ğŸµ: Music
ğŸ“¦: 3D
ğŸ¤–: Robot state
ğŸ§¬: Biological modality
ğŸ§ª: Chemical modality

Model types:
ğŸ“ â†’ ğŸ“ : LLMs
ğŸ“+ ğŸ–¼ â†’ ğŸ“ : Multimodal LLMs
ğŸ“+ ğŸ–¼+ ğŸ¤– â†’ ğŸ“ : Multimodal LLMs for Robotics
ğŸ“ â†’ </> : Text to Code
ğŸ“ â†’ ğŸ›  : Text to Software tool use
ğŸ“ â†’ ğŸ–¼ : Text to Image
ğŸ“ â†’ ğŸ¥ : Text to Video
ğŸ“ â†’ ğŸµ : Text to Music
ğŸ–¼ â†’ ğŸ“¦ : Image to 3D
ğŸ“ â†’ ğŸ“¦ : Text to 3D
ğŸ“ â†’ ğŸ§¬/ğŸ§ª : Biological/chemical models
ğŸ“ â†’ ğŸŒ : World model

stateof.ai 2025

|8

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Executive Summary
Research
- Reasoning deï¬ned the year, with OpenAI, Google, Anthropic, and DeepSeek trading leads and pushing visible â€œthink-then-answerâ€ methods into real products.
- Open models improved fast and Chinaâ€™s open-weight ecosystem surged, yet the top models remain closed and keep widening their capability-per-dollar edge.
- Benchmarks buckled under contamination and variance, while agents, world models, and domain tools (code, science, medicine) became actually useful.

Industry
- Real revenue arrived at scale as AI-ï¬rst companies crossed tens of billions, and ï¬‚agship labs stretched their lead with better capability-to-cost curves.
- NVIDIA ripped past $4T and 90% ownership of AI research papers while custom chips and neoclouds rose. Circular mega-deals funded huge build-outs.
- Power became the new bottleneck as multi-GW clusters moved from slideware to site plans and grid constraints started to shape roadmaps and margins.

Politics
- The AI race heats up as the U.S. leans into â€œAmerica-ï¬rst AIâ€ with export gyrations while China accelerates self-reliance ambitions and domestic silicon.
- Regulation takes a back seat in the face of turbo-investments: international diplomacy stalls and the AI Act runs into implementation hurdles.
- â€œAI goes globalâ€ became concrete, with petrodollars and national programs funding gigantic data centers and model access as job loss data trickles in.

Safety
- AI labs activated unprecedented protections for bio and scheming risks, others missed self-imposed deadlines, or quietly abandoned testing protocols.
- External safety organizations operate on annual budgets smaller than what leading labs collectively spend in a single day.
- Cyber capabilities doubled every 5 months outpacing defensive measures. Criminals orchestrated ransomware using AI agents inï¬ltrate F500 companies.

stateof.ai 2025

|9

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Scorecard: Reviewing our predictions from 2024

stateof.ai 2025

| 10

Introduction | Research | Industry | Politics | Safety | Survey | Predictions
Our 2024 Prediction

Evidence

A $10B+ investment from a sovereign state into a US large AI lab invokes national security review.

~

Sovereign-backed initiatives (HUMAIN $10B VC fund, UAEâ€™s Stargate AI infra cluster) are
infrastructure partnerships rather than direct majority investments into a US AI lab.

An app or website created solely by someone with no coding ability will go viral (e.g. App Store
Top-100).

YES

Formula Bot, built entirely using Bubble, exploded to 100,000 visitors overnight from a
Reddit post and generated $30,000 in its ï¬rst three months.

Frontier labs implement meaningful changes to data collection practices after cases begin reaching
trial.

YES

Anthropic landmark $1.5B settlement with authors, deleting works and shifting to legally
acquired books. OpenAIâ€™s paid content partnerships with Future (owner of Marie Claire).

Early EU AI Act implementation ends up softer than anticipated after lawmakers worry theyâ€™ve
overreached.

~

The Commission is phasing obligations and leaning on a voluntary GPAI Code of Practice
ï¬rst, so early implementation has been softer, even as binding rules arrive later.

An open source alternative to OpenAI o1 surpasses it across a range of reasoning benchmarks.

YES

DeepSeek-R1 outperforms OpenAI's o1 on key reasoning benchmarks including AIME,
MATH-500, and SWE-bench Veriï¬ed.

Challengers fail to make any meaningful dent in NVIDIAâ€™s market position.

YES

NVIDIA remains dominant, competitors fail to make signiï¬cant market share dents.

Levels of investment in humanoids will trail off, as companies struggle to achieve product-market ï¬t.

NO

$3B has been invested into humanoids in 2025, up from $1.4B last year.

Strong results from Appleâ€™s on-device research accelerates momentum around personal on-device AI.

NO

Apple Intelligence rolled out with many models running on-device and helped push a
broader industry push to on-device AI. Shipments of AI-capable smartphones climbed.

A research paper generated by an AI Scientist is accepted at a major ML conference or workshop.

YES

An AI-generated scientiï¬c paper The AI Scientist-v2 was accepted at an ICLR workshop.

A video game based around interacting with GenAI-based elements will achieve break-out status.

NO

Not yet.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 11

Section 1: Research

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 12

Think before you speak: o1 â€œthinkingâ€ ignites the reasoning race
As 2024 drew a close, OpenAI released o1-preview, the ï¬rst reasoning model that demonstrated inference-time
scaling with RL using its CoT as a scratch pad. This led to more robust problem solving in reasoning-heavy domains
like code and science. For example, the general release o1 exhibited accuracy improvements on the American
Invitational Mathematics Examination (AIME) with greater training and test time compute. As a result, OpenAI
leaned even more strongly into the scaling their reasoning efforts in 2025.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 13

Deeply sought: could frontier reasoning ever be found in the open?
Barely 2 months after o1-preview, DeepSeek (the Chinese upstart AI lab spun out of a high-frequency quant ï¬rm)
released their ï¬rst reasoning model, R1-lite-preview, thatâ€™s built atop the earlier strong V2.5 base model. Like
OpenAI, they showed predictable accuracy improvements on AIME with greater test-time compute budget.
Impressively, R1-lite-preview actually beat o1-preview on AIME 2024 pass@1 by scoring 52.5 vs. 44.6. BUT, very
few seemed to take noticeâ€¦ Wall Street certainly didnâ€™t.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 14

Are you not entertained? DeepSeek V3 brings you to R1
A few days after Christmas 2024, DeepSeek unveiled V3, a strong 671B MoE V3 that lowered training and
inference cost with FP8 mixed precision, multi-token prediction, and auxiliary-free routing. Using V3 as the base,
they trained R1-Zero only with RL using veriï¬able rewards and Group Relative Policy Optimization, a critic-free
algorithm that removes reward and value models.
â— R1-Zero follows a â€œthink â†’ answerâ€ format and uses a simple rule-based reward
for getting the ï¬nal answer right, which is cheaper and harder to game than a
learned neural reward model.
â— GRPO compares multiple sampled answers within a group to form a relative
baseline, so it does not need a value head or a separate reward model.
â— During training the model lengthens its thoughts, explores, and reallocates
compute to hard problems. Its AIME score rises from 15.6% to about 71% in
roughly 8.5k steps, with majority-vote runs reaching o1-0912 levels.
â— R1 then repairs readability with a small CoT warm start, a language-consistency
reward, large supervised ï¬netuning, and a ï¬nal RL pass. AIME increases to 79.8,
MATH-500 to 97.3, GPQA to 71.5, the approach distills well into smaller models.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 15

More thinking, more tool use, less cost: DeepSeek V3.1 and V3.2-Exp
DeepSeek V3.1 marks a substantial leap over V3, introducing a hybrid thinking mode that toggles between
reasoning and lightweight inference. It demonstrates faster â€œthinkâ€ efï¬ciency than R1 and V3, while greatly
improving tool use and multi-step agent workï¬‚ows. Now V3.2-Exp keeps that behavior and swaps dense
attention for DeepSeek Sparse Attention (DSA), where a tiny â€œlightning indexerâ€ picks the top-k past tokens to
attend to each step. You get roughly the same capability as V3.1 on coding/search/agent tasks, but markedly
lower cost and latency for 32â€“128K contexts.
â— DeepSeek V3.1 builds on the V3 base with a hybrid thinking
mode, enabling both lightweight inference and deep reasoning.
â— V3.2-Exp introduces a lightweight selector that picks the few
tokens that matter, instead of attending to every token in a long
prompt.
â— The model posts similar scores to V3.1 on coding/search/agent
tasks, with small dips on a few reasoning sets, and clearly lower
preï¬ll and decode cost for 32-128K contexts.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 16

Parallel reasoning: beyond depth to branchâ€‘andâ€‘merge inference
MoE routing scales capacity but preserves single-ï¬‚ow inference and doesnâ€™t change how the model thinks. A new
route is branching multiple inference paths and aggregating them versus just going deeper or using wider
models enables exploration, reduces hallucination, and better leverages parallel hardware.
â— Adaptive Parallel Reasoning (pictured) enables models to
dynamically orchestrate branching inference through spawn() and
join() operations, training both parent and child threads end-to-end
using RL to optimize coordinated behavior. This boosted
performance on the Countdown task at 4K context: 83.4%
(APR + RL) vs. 60.0% (baseline).
â— Sample Set Aggregator (right) trains a compact model to fuse
multiple reasoning samples into one coherent answer,
outperforming naive re-ranking methods.
â— Models like Gemini Deep Think, which shows its step-by-step
reasoning transparently, exemplify this branch-and-evaluate
paradigm in deployed systems.

stateof.ai 2025

| 17

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The reasoning timeline: from o1 â€œthinkingâ€ to R1, GPT-5 and parallel compute routing
o1 preview
Sept-24

Dec-24
o1 GA

DeepSeek R1
Jan-25

Dec-24
Gemini 2.0 Flash
Thinking preview

Feb-25
Claude 3.7
extended
thinking

o3/o4-mini
Apr-25

Gemini 2.5 Pro
Thinking
Jun-25

Aug-25
GPT-5

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 18

12 months pass, and OpenAI models remain at the frontier of intelligence
Across independent leaderboards, OpenAIâ€™s GPT-5 variants still set the pace, but the gap has narrowed. A
fast-moving open-weights pack from China (DeepSeek, Qwen, Kimi) and closed-source group in the US (Gemini,
Claude, Grok) sits within a few points on reasoning/coding. Thus, while US lab leadership persists, China is the
clear #2, and open models now provide a credible fast-follower ï¬‚oor.

stateof.ai 2025

| 19

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The illusion of reasoning gains
The improvements we observe from recent reasoning methods fall entirely within baseline model variance ranges
(i.e. margin of error), which suggests that perceived reasoning progress may be illusoryâ€¦
â— Current benchmarks are highly sensitive to the
implementation (decoding parameters, seeds, prompts,
hardware) and small dataset sizes. For example, AIME'24
only has 30 examples where one question shifts Pass@1
by 3+ percentage points, causing double-digit
performance swings.
â— Whatâ€™s more, RL approaches show minimal real gains and overï¬t easily. Under standardised evaluation,
many RL methods drop 6-17% from reported results with no statistically signiï¬cant improvements over
baselines.
â— Recent methods' improvements fall entirely within baseline model variance ranges, suggesting limited
progress. This highlights the critical need for rigorous multi-seed evaluation protocols and transparent
reporting standards.

stateof.ai 2025

| 20

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

How far have we come?
One widely discussed paper suggests that large reasoning models (LRMs) paradoxically give up on complex
problems and only outperform standard models in a narrow complexity window. However, critics argue these
results stem from ï¬‚awed experimental design rather than genuine reasoning failures.
â— The paper shows that LRMs exhibit a surprising
defeatist behavior: they reason more as problems get
harder but then give up entirely on very complex
tasks, and are outperformed by LLMs on simple tasks.
â— Despite generating reasoning traces, the authors
claim LRMs fail to use explicitly given algorithms and
reason inconsistently across different difï¬culty levels.
â— However, critics found these results stem from ï¬‚awed
experimental design: the supposed "accuracy
collapse" occurred when models hit token limits or
were asked to solve mathematically impossible
puzzles, not from actual reasoning failures.

stateof.ai 2025

| 21

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

How reasoning breaks: minor variations
Simple distracting facts have huge impacts on a modelâ€™s reasoning performance. For example, adding irrelevant
phrases like â€œInteresting fact: cats sleep most of their livesâ€ to maths problems doubles the chances of SOTA
reasoning models getting answers wrong!
â— Irrelevant and distracting facts increase the error
rate in models like DeepSeek R1, Qwen, Llama
and Mistral by up to 7x.
â— Besides decreasing the quality of responses, the
introduction of irrelevant facts greatly increases
the number of tokens models need to reason to
get to their answer: DeepSeek R1-distill
generates 50% more tokens than needed in 42% of cases (vs 28% for R1), showing distillation
makes models signiï¬cantly more prone to overthinking.
â— This shows that adversarial triggers don't just cause wrong answers, they force models to waste massive
compute resources "overthinking" corrupted problems.

stateof.ai 2025

| 22

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

How reasoning breaks: small shifts cause big failures
Reasoning also degrades under mild distribution changes. Changing numbers or adding one innocuous clause
slashes math accuracy, while shifting the length/format of chains-of-thought makes models produce ï¬‚uent but
incoherent steps. Forcing the model to â€œthinkâ€ in a userâ€™s language raises readability but lowers accuracy. These
effects persist at larger scales and after light post-training.
â— Appleâ€™s GSM-Symbolic work shows that accuracy drops sharply when only the numeric instantiation
changes, and adding a single seemingly relevant clause can cut performance by ~65%. This suggests that
models do template-matching rather than true algebraic reasoning.
â— Work from ASUâ€™s DataAlchemy found that the CoT helps in-distribution but collapses when test tasks,
chain length, or CoT format deviate from training. Longer, well-worded traces often mask incorrect logic.
â— Finally, Groningen/Harvard/MGH/Amsterdamâ€™s XReasoning shows how prompt-forcing the model to
reason in the userâ€™s language lifts match rates to ~98% on hard sets but reduces accuracy by 9â€“13 points.
Using 100â€“250 example post-training improves language match but the accuracy penalty remains.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 23

CoTs still work, even when models arenâ€™t honestâ€¦
New ï¬ndings from METR show that even when models generate unfaithful or misleading reasoning traces, CoT
is still highly diagnostic for oversight.
â— In red-teaming evaluations of frontier models,
CoT-based monitors ï¬‚agged ~99% of reward-hacking
attempts (only 3 failures out of 21,000 completions).
â— This held true even when models were prompted to
conceal intent, suggesting that CoT traces still reveal
useful signals, even if they arenâ€™t perfectly faithful.
â— CoTs work not because models â€œmean what they sayâ€,
but because reasoning traces encode useful structure
for detecting hacks, especially in maths and logic.
â— As such, CoT traces offer a surprisingly reliable
window into model behavior, even under adversarial
conditions.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 24

â€¦but there are safety concerns, like the â€œAI Hawthorne effectâ€
Researchers ï¬nd a single, steerable direction in model activations that encodes â€œtest awareness.â€ By nudging the
model along this direction, they can make it act more or less like itâ€™s under evaluation. Turning awareness up
increases refusals and safety-compliant behavior and turning it down does the opposite. This means reported
safety can be inï¬‚ated by evaluation setup rather than true robustness. This â€œtest awarenessâ€ is akin to the
Hawthorne effect, where humans change behavior when being observed and change their behavior accordingly.
â— Using white-box probing, researchers identify a linear direction in a modelâ€™s
internal activations that encodes â€œtest awarenessâ€.
â— A model can be made more or less aware that it is in an evaluation. The
more it is aware, the more its behavior appears aligned and cautious.
â— The effect shows up across multiple reasoning models and tasks and even
on â€œfakeâ€ evaluations that look like tests but arenâ€™t.
â— This raises concerns that evaluations may be systematically manipulated to
overestimate real-world safety: developers could reduce awareness and
mask behavior changes, potentially letting models act differently under
evaluation vs deployment.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 25

Safety by design meets its skeptics
Safety-ï¬rst pretraining argues that safer behavior must be built into the base model, not bolted on later. A
data-centric pipeline of ï¬ltering, recontextualizing, refusal curricula, and harmfulness tags does cut jailbreak
success and survives benign ï¬netuning. However, others cautions that training on vast, unsupervised web data
bakes in biases that cannot be cleanly removed without harming utility, so â€œmore safety dataâ€ is not a panacea.
â— CMU's SafeLM uses a trained safety classiï¬er, 100B-token
recontextualized corpus, refusal and â€œmoral educationâ€ datasets.
Tokens used during pretraining and inference separate safe from
unsafe continuations.
â— The result lowers attack success rates from ~38.8% to ~8.3% at
base-model stage and stays far more robust after GSM8K
ï¬netuning, supporting the claim that â€œalignment is not unlearning.â€
â— Opposing analyses argue that web-scale pretraining embeds heterogeneous social biases and that
aggressive ï¬ltering or post-hoc alignment either censors knowledge or washes out under later ï¬netuning.
Safety must also be audited and governed at the dataset and deployment layers, not only â€œï¬xed in the
weights.â€

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 26

Trading capability for transparency
Researchers argue that to keep AI systems safe and monitorable, we may need to pay a "monitorability tax", i.e.
accepting slightly less capable models in exchange for maintaining visibility into their reasoning.
â— CoT monitoring is highly effective: GPT-4o caught
95% of reward hacks by reading the reasoning traces
of stronger models such as o3-mini, versus only 60%
when monitoring actions alone.
â— However, models trained to avoid triggering the CoT
monitor developed â€œobfuscated reward hackingâ€,
whereby the model hid its true intentions (with
apparently innocuous CoT) while still reward
hackingâ€¦
â— As such, we have a ï¬ne balance at play here: moderate CoT pressure improved performance, but excessive
pressure taught models to deceive. As a result, we may need to pay a "monitorability tax" to develop less
polished but more transparent systems.

stateof.ai 2025

| 27

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

ğŸ“â†’ğŸ“

Monitor your CoTs: just because you can read â€˜em, doesnâ€™t mean you should trust â€˜em

Leaders from OpenAI, Google DeepMind, Anthropic, and Meta coordinated a joint call to action, urging the ï¬eld
to seize the opportunity to understand and preserve CoT monitorability.
â— CoT monitors can surface (i) explicit malicious intent in reasoning (â€œletâ€™s hack/sabotageâ€), (ii) early
misalignment signals (stated goals diverge from outputs), and (iii) evaluation artifacts (memorisation,
scaffold bugs, â€œbeing evaluatedâ€ tells).
â— But CoTs are incomplete and can drift away from faithful reasoning while advanced, situationally-aware
models might hide or compress their thoughts. Theyâ€™re also often not faithful: Anthropic ï¬nds <20% of true
cues are verbalized, with faithfulness dropping on harder tasks. Using RL boosts scores, not legibility.
â— Indeed, scaling outcome-based RL can reduce legibility, direct process
supervision (shorter/cleaner CoT) can distort faithfulness and latent-reasoning
architectures may bypass language entirely, removing the audit trail.
â— If algorithmic advances increasingly tie interpretable CoT traces to performance
penalties, this tension will intensify. Without industry standards to guide these
trade-offs, we have a major debate within the AI safety community.

stateof.ai 2025

| 28

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

But can LLMs reason without generating tokens?

ğŸ“â†’ğŸ“

Researchers from FAIR at META proposed a novel internal reasoning process that leverages an LLMâ€™s own
residual stream instead of a decoding tokens to a Chain-of-Thought (CoT) scratchpad.
â— Forgoing the generation of language tokens dramatically cuts down on the computational resources needed
to serve reasoning models at inference time.
â— COCONUTâ€™s high-dimensional CoT can also
transmit richer traces that simultaneously
encode multiple reasoning paths. This may
cut down on the wasteful and excessively
large rollouts seen in todayâ€™s models.
â— However, this process signiï¬cantly reduces
the monitorability of reasoning models,
hindering many new CoT control methods
that have emerged across the ï¬eld.

stateof.ai 2025

| 29

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Researchers begin to prioritise quality and diversity of post training data over volume
The NaturalReasoning dataset uses web-grounded, graduate-level questions to unlock faster, cheaper progress
in mathematical & scientiï¬c reasoning during supervised post-training.
â— 2.8M questions were mined from pre-training corpora across major scientiï¬c ï¬elds,
to elicit the longest median chain-of-thought (434 words) among open datasets.
â— Distilling an 8B Llama on just 0.5â€“2M NR problems yields steeper accuracy gains
than training on larger WebInstruct / OpenMathInstruct sets, cutting tokens and
compute.
In RL post-training, a new Oxford paper demonstrates the automatic selection of optimal
training problems. They introduce a method, LILO, to algorithmically identify questions
that allow for maximally efï¬cient training.
â— Researchers show how prioritising training on questions with high variance of
success, known as learnability, can allow LLM training pipelines to achieve a higher
ï¬nal test accuracy, and can do so in 3Ã— fewer training steps.

stateof.ai 2025

| 30

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The evolution of AI reward signals towards environments with veriï¬able rewards
RL has expanded from simple, fully checkable signals to fuzzier and more subjective goals, and now is
splintering again. Early systems used binary outcomes, then fuzzy human preferences and demonstrations, and
more recently unveriï¬able creative tasks. Today, two new directions stand out: rubric-based rewards, where
small sets of rules guide alignment, and a revival of veriï¬able correctness for math and coding through RLVR.
Process rewards are also emerging to score intermediate reasoning steps, offering a middle ground.
Binary outcomes

Fuzzy matching

Rubric-based rewards

Unveriï¬able rewards

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 31

Even so, RL environments and evaluations are fraught with challenges
As reward signals become more abstract, the simpliï¬ed environments used for agent training have become the
primary bottleneck, limiting progress towards generalizable intelligence.
â— Generalization crisis: Many RL benchmarks are static/deterministic, so agents â€œmemorizeâ€ a single game/task
and collapse under small variations.
â— Sample inefï¬ciency and domain transfer gaps. Agents need billions of steps, so we train in simulators. In
robotics, this yields a sim-to-real gap where policies that work in sim fail on hardware. In VLM/LLM or UI
agents, the analogue is env-to-prod: policies overï¬t to benchmark sites/datasets and break on live apps or
novel layouts.
â— Reward hacking: Agents exploit loopholes in simpliï¬ed environments and maximize proxy rewards without
achieving the intended goal; shortcuts are easier to learn than the desired behavior.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 32

RL from veriï¬able rewards: promising, but the evidence cuts both ways
RL with Veriï¬able Rewards (RLVR) has driven recent progress (OpenAI o1, DeepSeek-R1) by training on answers
that can be automatically checked: math scores, program tests, or exact matches. However, two recent studies
disagree on what RLVR actually adds. One argues it mostly reshufï¬‚es sampling without creating new reasoning;
the other shows gains once you score the reasoning chains themselves rather than just ï¬nal answers. Together
they map where RLVR helps today and where it stalls.
â— Work from Tsinghua evaluated many models, tasks, and RL algorithms
and ï¬nd that present-day RLVR improves Pass@1 but, at larger K, base
models catch up. They conclude RLVR has not unlocked fundamentally
new reasoning and remains bounded by the base modelâ€™s capacity.
â— A counter from MSR Asia formalized why Pass@K can mask progress and
introduce CoT-Pass@K, which requires both a correct answer and a valid
chain-of-thought.
â— On AIME-2024/2025 with Qwen2.5-32B â†’ DAPO-Qwen-32B, RLVR
consistently raises CoT-Pass@K across K, supporting the claim that
RLVR implicitly incentivizes correct reasoning paths.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Ushering in an era of AI-augmented mathematics

| 33
ğŸ“â†’ğŸ“

Math is a veriï¬able domain: systems can plan, compute, and check every step, and publish artifacts others can
audit. So 2025 saw competitive math and formal proof systems jump together: OpenAI, DeepMind and Harmonic
hit IMO gold-medal performance, while auto-formalization and open provers set new records.
â— OpenAI: experimental reasoning model reached IMO gold under contest-style
conditions (â‰ˆ35/42, 5/6 problems). Also at the â€œcoding Olympicsâ€ (ICPC-style
test), GPT-5 solved 12/12 problems (11 on ï¬rst try).
â— DeepMind: after silver in 2024, IMO gold-level performance reported in 2025.
â— Harmonic (Aristotle): announced formally veriï¬ed IMO gold-medal-level results
and released veriï¬cation artifacts.
â— GÃ¶del-Prover (Princeton/Goedel-LM): open-source prover with 57.6% Pass@32
on miniF2F (+7.6 over prior OS SOTA), 7 problems on PutnamBench, and 29.7k
new Lean proofsâ€”fuel for the training ï¬‚ywheel.
â— These improvements point to the very real possibility of a non-trivial
research-level result in mathematics being proven and formalized by an AI
system (with some human supervision involved) within the next year.

stateof.ai 2025

| 34

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Bigger models, same budget: RL with LoRA adapters
Thinking Machines show that RL can match full ï¬ne-tuning even with rank-1 Low-Rank Adaptation (LoRA). In
policy-gradient setups, LoRA updates only tiny adapters while the backbone stays frozen, yet it reaches the same
peak performance, often with a wider range of stable learning rates. The reason is that RL supplies very few bits
per episode, so even tiny adapters have ample capacity to absorb what RL can teach.
â— With LoRA you insert tiny adapters in a few attention and MLP layers and update
only those during PPO, GRPO, or RLHF. The backbone does not change.
â— This cuts the trainable parameters from billions to millions, so gradients and
optimizer state shrink by roughly 10â€“50Ã—. Memory pressure falls even further
when you pair LoRA with 8-bit weights.
â— Under the same budget you can move from a 7-13B class model to a much larger
30-70B class model. You can also ï¬t longer contexts or larger batches on the
same cards.
â— Very low adapter ranks can, however, underï¬t. Reasonable choices are ranks in the
want to improve.
16â€“64 range and placing adapters in the layers that matter for the skill you

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Beyond reasoning isâ€¦continual learning?

| 35
ğŸ“â†’ğŸ“

The scaling paradigm is shifting from static pre-training to dynamic, on-the-ï¬‚y adaptation. Test-time ï¬ne-tuning
(TTT) adapts a model's weights to a speciï¬c prompt at inference, a step towards continuous learning.
â— From naive retrieval to active selection: Early methods used simple Nearest Neighbor retrieval, often
selecting redundant data. New algorithms like ETH ZÃ¼rich's SIFT now integrate active learning to select
small, diverse, and maximally informative examples for each query.
â— This on-demand learning consistently outperforms in-context learning, especially on complex tasks. It
creates a new performance vector independent of pre-training scale. An actively ï¬ne-tuned 3.8B Phi-3
model (red bars) can outperform a base 27B Gemma-2 model. Admittedly these models are a little old.
â— A recent follow-up, Local Mixtures of Experts (test-time model merging), amortizes TTT by training small
neighborhood experts and, at inference, retrieving and merging a few weight deltas into the base model. It
keeps most SIFT-style gains with near-retrieval latency and, on ~1B bases, approaches TTT accuracy while
running up to ~100Ã— faster. Titans studies test-time memorization as an architectural memory and is
orthogonal to amortized TTT.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 36

Too many cooks?
Researchers have discovered why merging multiple expert AI models hits a performance wall: the task vector
space undergoes rank collapse, causing different experts' knowledge to become redundant rather than
complementary. Subspace Boosting uses singular value decomposition to maintain the unique contributions of
each model, achieving >10% gains when merging up to 20 specialists. This breakthrough could facilitate
versatile systems that combine specialized models without the usual degradation that occurs at scale.
â— As models are merged using existing methods (task arithmetic, TIES-Merging,
etc.), the rank of the task vector space progressively decreases - meaning a
100-dimensional space of possible model behaviors might effectively shrink to
just 20-30 dimensions, wasting the potential of additional experts.
â— Their subspace boosting method operates on the SVD-decomposed task vector
space, explicitly preserving the rank by maintaining orthogonal components
that represent each expert's unique contributions to the merged model.
â— They achieved >10% improvement on vision benchmarks including evaluation across multiple datasets when
merging large numbers of experts, successfully merging up to 20 expert models with consistent performance
gains (whereas traditional methods typically degrade after 5-10).

stateof.ai 2025

| 37

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The Muon Optimizer: expanding the compute-time Pareto frontier beyond AdamW
Researchers show that Muon expands the compute-time Pareto frontier beyond AdamW, the ï¬rst optimizer to
challenge the 7-year incumbent in large-scale training. Muon demonstrates better data efï¬ciency at large batch
sizes, enabling faster training with more devices.
â— Muon requires 10-15% fewer tokens than AdamW at large batch sizes
(128K-16M), expanding the compute-time Pareto frontier.
â— Muon works with maximal update parameterization (muP) and
telescoping (a way to progressively reï¬ne hyperparameter search across
model scales), enabling efï¬cient hyperparameter tuning at O(C log N)
cost (where N is model width and C is the cost of training the model).
â— This could make second-order optimization economically viable. The 10-15% token efï¬ciency gain saves
millions at scale, while telescoping muP eliminates the prohibitive hyperparameter search costs that
previously made second-order methods impractical.
â— A recent study on optimizers validates these modest gains: when tested fairly with proper tuning, even the
best optimizers (including Muon) achieve ~10% speedups over AdamW at scale. This aligns with Muon's
claims and debunks the 2x speedup assertions made by some in the ï¬eld.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 38

Cutting your losses: signiï¬cant memory reduction for LLM training
As vocabularies grow, the loss layer consumes up to 90% of training memory in modern LLMs. Apple researchers
show that this bottleneck can be eliminated by computing the loss without materializing the massive logit
matrix, enabling dramatically larger batch sizes and more efï¬cient training.
â— Cut Cross Entropy (CCE) computes the cross-entropy loss
by calculating only the logits for correct tokens directly,
while evaluating the normalization term over the
vocabulary in fast on-chip memory. This makes global
memory consumption for the cross-entropy computation
negligible.
â— CCE achieves a remarkable 24 reduction in memory
consumption, taking Gemma 2's loss computation from
24GB down to just 1 MB, while actually running ~5% faster than the best existing methods.
â— The practical impact of this is that it allows researchers to train models much more efï¬ciently - either using
fewer GPUs for the same batch size, or achieving better GPU utilization with larger batches on the same
hardware.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 39

How much do LLMs memorize?
Thereâ€™s a way to separate memorization from generalization, showing that GPT-family models have a ï¬nite
â€œcapacityâ€ of ~3.6 bits per parameter. Models memorize training data until that capacity is full, then must
generalize once dataset size exceeds it. This explains the â€œdouble descentâ€ phenomenon and why todayâ€™s largest
LLMs, trained with extreme data-to-parameter ratios, are difï¬cult to probe for speciï¬c memorized examples. At
the same time, membership inference attacks are still improving on smaller-scale models.
â— On random data, models hit a clear ceiling of ~3.6 bits per parameter,
providing an upper bound on raw storage capacity.
â— On natural text, memorization dominates until capacity is saturated;
beyond that, double descent forces generalization to emerge.
â— Modern frontier-scale LLMs train on vastly more tokens than their
capacity, making loss-based membership inference statistically unreliable.
â— Nevertheless, new extraction and membership inference methods are
gaining traction on small-to-medium models, highlighting continued
privacy risk.

stateof.ai 2025

| 40

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Learning from superintelligence: AlphaZero teaches chess grandmasters new concepts
Researchers extracted novel chess concepts from AlphaZero (an AI system that mastered chess via self-play
without human supervision) and successfully taught them to 4 world champion grandmasters, demonstrating
that superhuman AI systems can advance human knowledge at the highest expert levels. This paper
demonstrates an exciting process for mining superhuman knowledge and proving humans can learn them.
â— Researchers developed a method to discover "dynamic concepts"
(concepts that motivate sequences of moves) by analyzing AlphaZero's
neural network activations, ï¬ltering for teachability and novelty.
â— All four grandmasters improved their performance after studying
concept prototypes (chess puzzles exemplifying each concept), with an
average improvement of 0.85 puzzles solved correctly out of 4.
â— New concepts often involved counterintuitive plans that violated conventional chess principles, such as
sacriï¬cing the queen for long term strategic gain, or playing quiet positional moves over immediate attacks.
â— This proof-of-concept suggests a very exciting potential paradigm where superhuman AI systems could
become "teachers" rather than just "tools", and help advance human knowledge in domains beyond chess.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 41

Kimi K2: stable trillion-scale MoE for agentic intelligence in the open
Chinaâ€™s Moonshot AI built a 1T-param MoE with 32B active trained using MuonClip, an improved optimizer that
integrates the token-efï¬cient Muon algorithm with a stability-enhancing mechanism, to deliver greater stability
and advancing open-weight models for agentic workï¬‚ows. It ranks as the #1 open text model on LMArena.
â— MuonClip with QK-clip, a stability innovation, enabled
15.5T tokens of pretraining without loss spikes.
â— A multi-stage post-training pipeline integrates
synthetic agentic trajectories with RL to reï¬ne model
behavior.
â— Rewards are designed for veriï¬able correctness and
self-critique using binary or graded automatic signal
to strengthen reasoning, coding, and safety.
â— Together, these advances establish K2 as a new
benchmark in open-weight, agent-ready LLMs,
pushing non-thinking workï¬‚ows further into
real-world usability.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Open source vs. proprietary: where are we now?

| 42
ğŸ“â†’ğŸ“

There was a brief moment around this time last year where the intelligence gap between open vs. closed models
seemed to have compressed. And then o1-preview dropped and the intelligence gap widened signiï¬cantly until
DeepSeek R1, and o3 after it. Today, the most intelligent models remain closed: GPT-5, o3, Gemini 2.5 Pro, Claude
4.1 Opus, and new entrant, Grok 4. Beside gpt-oss, the strongest open weights model is Qwen. Pictured are the
aggregate Intelligence Index, which combines multiple capabilities dimensions across 10 evaluations.

o3
o1
R1

stateof.ai 2025

| 43

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

OpenAI pivots from the â€œwrong side of historyâ€ to aligning with â€œAmerica-ï¬rst AIâ€
With mounting competitive pressure from strong open-weight frontier reasoning models from DeepSeek, Alibaba
Qwen and Google DeepMindâ€™s Gemini and the US Government pushing for America to lead the way across the AI
stack, OpenAI released their ï¬rst open models since GPT-2: gpt-oss-120b and gpt-oss-20b in August 2025. These
adopt the MoE design using only 5.1B (of 120B) and 3.6B (of 20B) active parameters per token and grouped
multi-query attention. Post-training mixes supervised ï¬netuning and reinforcement learning, with native tool use,
visible reasoning, and adjustable thinking time. However, in the community vibes post-release have been mid, in
part due to poor generalisation (similar to MSFT phi models) potentially due to over distillation.

When open source?

7 months later, gpt-oss

20k â­ in weeks, but
how much real usage?

stateof.ai 2025

| 44

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The New Silk Road: Chinaâ€™s open models overtake the previously Meta-led West
The original Silk Road connected East and West through the movement of goods and ideas. The new Silk Road
moves something far more powerful: open source models, and China is setting the pace. After years of trailing the
US in model quality prior to 2023, Chinese models - and Qwen in particular - have surged ahead as measured by
user preference, global downloads and model adoption. Meanwhile, Meta fumbled post-Llama 4, in part by
betting on MoE when dense models are much easier for the community to hack with at lower scales.

stateof.ai 2025

| 45

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Once a â€œLlama rip-offâ€, developers are increasingly building on Chinaâ€™s Qwen
Metaâ€™s Llama used to be the open source communityâ€™s darling model, racking up hundreds of millions of
downloads and plentiful ï¬netunes. In early 2024, Chinese models made up just 10 to 30% of new ï¬netuned
models on Hugging Face. Today, Qwen alone accounts for >40% of new monthly model derivatives, surpassing
Metaâ€™s Llama, whose share has dropped from circa 50% in late 2024 to just 15%. And this isnâ€™t because the West
gave up. Chinese models got a lot smarter and come in all shapes and sizes - great for builders!

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Why are researchers going Chinese?

| 46
ğŸ“â†’ğŸ“

Chinaâ€™s RL tooling and permissive licenses are steering the open-weight community. ByteDance Seed, Alibaba
Qwen and Z.ai are leading the charge with verl and OpenRLHF as go-to RL training stacks, while Apache-2.0/MIT
licenses on Qwen, GLM-4.5 and others make adoption frictionless. Moreover, model releases come in many
shapes and sizes to facilitate developer adoption of all ï¬‚avors.
â— ByteDance verl (top) turned the earlier HybridFlow research
system from 2024 into a production RLHF/RLVR library with a
hybrid controller and 3D â€œHybridEngine,â€ now Apache-2.0 and
actively maintained. It has vendor support (AMD ROCm) and
platform integrations (e.g., Oumi), making RL training cheaper
and easier to adopt.
â— OpenRLHF (bottom) is a simpler Ray/vLLM/DeepSpeed stack
that brings speedups of 1.22x-1.68x vs. SOTA frameworks. It is
well liked in academia and industry, showing how Chinese
teams now lead RL frameworks.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

World models step out of the clip: real-time, interactive video arrives

| 47
ğŸ“â†’ğŸŒ

Prior clip models (Sora, Gen-3, Dream Machine, Kling) render ï¬xed videos you canâ€™t steer mid-stream. World
models predict the next frame from state and your actions, enabling closed-loop interactivity and minute-scale
consistency. Crucially, no game engines were harmed!
â— GDMâ€™s Genie 3 generates explorable environments
from a text prompt at 720p / 24 fps and its
consistent for a few mins.
â— Supports promptable world events (e.g. change
weather, spawn objects with persistence).
â— Shows early use for training embodied agents and
even imagined worlds within the imagined world.
â— Odysseyâ€™s public research preview streams a new
frame every ~40 ms (up to ~30 fps) for 5+ minute
sessions and the user can provide inputs on their
device to navigate through the world. See video:

stateof.ai 2025

| 48

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The Genie in three acts: from sketches to image-prompted persistent worlds

ğŸ“â†’ğŸŒ

Feb â€˜24 - Genie

Dec â€˜24 - Genie-2

Aug â€˜25 - Genie-3

â— First unsupervised,
action-controllable world model
from video; 11B params.
â— Learns a latent action space from
Internet platformer videos;
frame-level control.
â— Video tokenizer, latent action
model, autoregressive dynamics.

â— Generates interactive 3D worlds
from a single image prompt in
~360p.
â— Handles physics, lighting,
reï¬‚ections; ï¬rst/third-person
views; ~20s horizons.
â— Only handles game
environments; bad at real world.

â— Longer interactions (mins) with
persistence (object
permanence/memory).
â— Dynamic, user-steerable 3D
environments; improved stability
& interactivity with objects.
â— Training substrate for agents and
sim-to-real robotics.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Training agents inside of scalable world models

| 49
ğŸ“â†’ğŸŒ

Dreamer 4 trains a video world model that can predict object interactions and future frames, then learns its
policy entirely in imagination. A new â€œshortcut forcingâ€ objective and an efï¬cient transformer make the model run
at real-time speeds on a single GPU. The agent is the ï¬rst to reach diamonds in Minecraft using ofï¬‚ine data only,
outperforming OpenAIâ€™s VPT while using roughly 100x less labeled data.
â— The system ï¬rst learns dynamics and objects from large
unlabeled video, then adds a small action-labeled set to
ground control and inventory changes.
â— The policy is improved by rolling out many imagined
trajectories inside the learned model. Rewards and
value heads are trained on the same data to guide
long-horizon skills.
â— Shortcut forcing contrasts predictions with and without the true actions, pushing the world model to depend
on actions rather than hindsight correlations.
â— The model runs at interactive frame rates on a single GPU and supports live human play in the learned
world, though memory is short and inventory tracking is still imperfect.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Chinaâ€™s video generation matures: a strategic divergence

| 50
ğŸ“â†’ğŸ“º

From lateâ€‘2024, Chinese labs split between openâ€‘weight foundations and closedâ€‘source products. Tencent
seeded an open ecosystem with HunyuanVideo, while Kuaishouâ€™s Kling 2.1 and Shengshuâ€™s Vidu 2.0 productize
on speed, realism and cost. Models tend to use Diffusion Transformers (DiT), which replace convolutional U-Nets
with transformer blocks for better scaling and to model joint dependencies across frames, pixels, and tokens.
â— Tencentâ€™s HunyuanVideo (13B) openâ€‘sourced a transformerâ€‘based
diffusion model with a 3Dâ€‘VAE with evaluations reporting that it
outperforming Runway Genâ€‘3 and Luma 1.6. Code/weights released.
â— Openâ€‘Sora 2.0 achieved commercialâ€‘level quality from a ~$200k
training run, reporting parity with HunyuanVideo and Runway Genâ€‘3
Alpha on human/VBench tests and narrowing the gap to OpenAIâ€™s
Sora.
â— Kling 2.1 added 720p/1080p tiers and editorâ€‘oriented controls, while
Vidu 2.0 cut price (~Â¥0.04/s) and latency (~10 s to render 4 s@512p).

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 51
ğŸ“â†’ğŸ“º

OpenAI launches Sora 2: controllable video-and-audio inches toward a world simulator
OpenAIâ€™s second-gen Sora adds synchronized dialogue and sound, stronger physics, and much tighter control
over multi-shot scenes. It can also insert a short â€œcameoâ€ of a real person with their voice and appearance into
generated footage, and launches alongside an invite-only iOS app for creation and remixing.
â— Sora 2 is trained and post-trained on large-scale video so the model keeps track
of objects and cause-and-effect over time. Shots link together more coherently,
bodies and materials behave more plausibly, and audio is generated in step with
the visuals to sell the scene.
â— Despite being a video model, Sora 2 can â€œsolveâ€ text benchmarks when framed
visually. EpochAI tested a small GPQA Diamond sample and Sora 2 reached 55%
(vs 72% for GPT-5) by prompting for a video of a professor holding up the answer
letter. Four videos were generated per question and any clip without a clear
letter was marked wrong.
â— A likely explanation is a prompt-rewriting LLM layer that ï¬rst solves the question
and then embeds the solution in the video prompt, similar to re-prompting used
in some other video generators.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Generated worlds enable practical open-ended learning

| 52
ğŸ“â†’ğŸŒ

Open endedness describes a learning system that continually proposes and solves new tasks without a ï¬xed
endpoint, selecting tasks that are both novel and learnable, and accumulating the resulting skills so they can be
reused to reach further, harder tasks. Interactive and persistent world models make this increasingly feasible.
â— In OMNI-EPIC, foundation models generate environment and
reward code, while the system ï¬lters for tasks that are both
learnable and useful, maintaining an expanding archive.
â— In Kinetix, general controllers are trained in a procedurally
generated very large-scale task space that transfer to
human-designed levels.
â— In the Darwin GÃ¶del Machine, the agent rewrites its own code,
validates changes empirically, and archives only improved
variants to produce measurable iteration-over-iteration gains
on coding benchmarks (right ï¬gures).

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

How should we measure progress on open-endedness?

| 53
ğŸ“â†’ğŸŒ

Open endedness describes a learning system that continually proposes and solves new tasks without a ï¬xed
endpoint, selecting tasks that are both novel and learnable, and accumulating the resulting skills so they can be
reused to reach further, harder tasks. Interactive and persistent world models make this increasingly feasible.
â— Metaâ€™s MLGym is a gym for AI-research agents with 13 open-ended tasks across vision, language, RL, and
game theory. It supports RL training and logs reproducible traces. Early results indicate that most gains
come from hyperparameter tuning rather than genuinely new method design.
â— OpenAIâ€™s PaperBench evaluates replication of 20 ICML 2024 spotlight and oral papers. It decomposes each
paper into thousands of graded subtasks. Current agents achieve low replication scores, which highlights a
signiï¬cant gap to human research practice.
â— Michiganâ€™s EXP-Bench contains 461 tasks derived from 51 top papers. It requires agents to design,
implement, run, and analyze complete experiments starting from provided code. End-to-end success is rare
while partial component scores are higher.
â— MLR-Bench offers 201 real research tasks with an LLM reviewer calibrated to expert judgment. It evaluates
literature synthesis, experiment execution, and report quality. The authors report reasonable judge
alignment and frequent failure modes such as fabrication and invalid runs.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 54

From tools to collaborators: AI agents as partners in discovery
AI is moving from answering questions to generating, testing, and validating new scientiï¬c knowledge. New â€œAI
labsâ€ organize coalitions of agent roles (PI, reviewers, experimenters) that ideate, cite, run code, and hand
results back to human teams, shortening the loop from hypothesis to validation.
â— DeepMindâ€™s Co-Scientist is a multi-agent system built on
Gemini 2.0 that generates, debates, and evolves its
approach to hypothesis generation and experimental
planning. It was shown to propose drug candidates for
AML (blood cancer) and new epigenetic targets for liver
ï¬brosis that were validated in vitro. In a subsequent blind
test set by bacteriophage experts, Co-Scientist proposed
the tail-hijacking mechanism for cf-PICI transfer that
experiments later conï¬rmed.
â— Stanfordâ€™s â€œVirtual Labâ€ is a principal investigator plus specialist agents that hold â€œlab meetings,â€ plan
workï¬‚ows, and integrate protein structure tools (ESM, AlphaFold-Multimer, Rosetta). It designed 92
nanobodies including conï¬rmed binders to recent SARS-CoV-2 variants.
stateof.ai 2025

| 55

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

AlphaEvolve: a coding agent for algorithm discovery and engineering impact

ğŸ“â†’ğŸŒ

A recent example towards open-endedness for scientiï¬c research is DeepMindâ€™s AlphaEvolve, an evolutionary
coding agent that iteratively edits programs, tests candidates with automated evaluators, and promotes the best
variants to discover novel solutions. Note the evaluation/ï¬tness functions are still deï¬ned by the engineers.
â— This approach discovered a new matrix multiplication
algorithm that to multiply 4x4 complex-valued
matrices using 48 scalar multiplications, improving
upon Strassenâ€™s 1969 algorithm.
â— Across a set of 50 open problems in mathematics, the
system is said to have rediscovered SOTA in 75% of
cases and achieved improved existing solutions in
20% of cases.
â— AlphaEvolve also delivered production gains at Google, including 0.7% resource recovery and faster kernels.
â— It represents a concrete example of an AI system generating novel, veriï¬able, and superhuman scientiï¬c
knowledge.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

A universal interface model for biology?

| 56
ğŸ“â†’ğŸ§¬

ATOMICA learns an all-atom representation of intermolecular interfaces across proteins, nucleic acids, ions,
lipids, and small molecules. It trains with self-supervision on about two million interfaces and builds embeddings
that transfer across tasks. The model connects interface physics to disease modules and proposes new
ligand-binding sites that hold up in the lab.
â— The model is a hierarchical geometric network that encodes
atoms, chemical blocks, and whole interfaces, then reconstructs
masked structure to learn general interface features.
â— Using these embeddings, â€œATOMICANetsâ€ link proteins by interface
similarity and recover disease-speciï¬c communities such as lipid
modules in asthma and ion modules in myeloid leukemia.
â— The team predicts 2,646 previously unannotated ligand-binding
sites and reports wet-lab conï¬rmation of ï¬ve heme binders,
indicating that the representation carries biochemical signal.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Scaling to Universal Atomistic Models

| 57
ğŸ“â†’ğŸŒ

Metaâ€™s FAIR trained UMA, a new family of universal interatomic potentials. These approximate the forces and
energies between atoms, a task that usually demands resource-intensive quantum calculations (DFT). By
replacing DFT with fast and accurate AI surrogates, UMA makes it possible to simulate materials, molecules, and
sorbents at a scale that was previously unimaginable. They also produced the largest materials database.
â— UMA is a Mixture of Linear Experts (MoLE) with the largest model reaching
3.7B. Itâ€™s trained on DFT calculations of 500M atomistic conï¬gurations from
OMat24 (118M structures, 400M CPU core-hours), OMat25 (88M structures,
600M CPU core-hours), OMol25 (100M molecules, 6B CPU core-hours), and
adsorption datasets.
â— UMA goes beyond prior models by embedding charge, spin, and task identity,
and by ensuring energy-conserving force predictions for long molecular
dynamics rollouts.
â— Across crystal stability (Matbench Discovery), catalysis (OC20), molecules
(OMol25), and sorbents (ODAC25), UMA consistently sets the new standard.

stateof.ai 2025

| 58

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

From property predictions to material generation

ğŸ“â†’ğŸ§ª

Where UMA shows that scaling data and parameters yields universal predictive models, MatterGen takes the
next leap: using diffusion to directly generate new inorganic crystals with targeted properties, rather than
screening existing ones.
â— MatterGen is a diffusion model that learns to reï¬ne lattices, element types,
and atomic positions into stable crystal structures by independently
randomizes atom types, coordinates, and lattices, followed by iteratively
denoising them back toward physically plausible structures.
â— Itâ€™s trained on ~600k stable crystal structures of compounds (Materials
Project and Alexandria) with adapter modules for chemistry, symmetry, and
property control that are ï¬ne-tuned into the model.
â— MatterGEn generates materials that are 2x more likely to be stable and
novel, and 10x closer to equilibrium energy minima vs. SOTA.
â— It achieved multi-property inverse design (e.g. combining band gap and
magnetism) and saw its ï¬rst lab synthesis succeed, with measured values
within ~20% of AI predictions.

stateof.ai 2025

| 59

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

ğŸ“â†’ğŸ§ª

Language models in Chemistry: from property predictors to strategy-aware planners

Chemical modeling has shifted from task-speciï¬c predictors to general LLMs that reason about synthesis
strategy and mechanisms. The strongest results now come from large, general-purpose transformers used as
â€œreasoning enginesâ€ and paired with classical search, rather than chemistry-speciï¬c generators. Benchmarks also
show these models matching or beating expert chemists on curated Q&A while still missing
knowledge-intensive and multi-modal edge cases.
â— ChemBench ï¬nds frontier models (e.g., o1-preview; open Llama-3.1-405B close
behind) outperform the best human chemists on aggregate, with performance rising
with model size. Retrieval alone doesnâ€™t ï¬x knowledge-heavy failures. ï¿¼
â— The current best approach uses a large LLMs as a strategic evaluator plugged into
search (incl. MCTS): it judges routes and mechanisms from natural-language
constraints. Newer, larger models (e.g., Gemini-2.5-Pro) lead; strong open options
(e.g., DeepSeek-r1) are close. ï¿¼
â— This LLM-as-judge + search pattern brings human-like planning (protecting-group
timing, ring-formation order) without forcing the LLM to emit SMILES (still
challenging) and it scales as LLMs improve, and as inference time increases.

stateof.ai 2025

| 60

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

ğŸ“â†’ğŸ§ª

Robot chemists scale discovery at 10x human speed and 1,000 experiments per day

Work from the University of Liverpool and North Carolina State University shows that autonomous chemistry
platforms can plan, execute, and analyze experiments in closed loops. Mobile robots run standard instruments
and select follow ups from analytical data while achieving human level decision quality at roughly 10x the
speed. A multi-robot lab coordinates specialized units to run >1,000 experiments per day and to reach best in
class quantum dot recipes within about 24h.
â— The Liverpool system integrates Chemspeed synthesis, UPLCâ€“MS, and benchtop
NMR with mobile scheduling, sample tracking, and restocking. It executed
diversiï¬cation, supramolecular assembly, and photochemistry programs by ranking
reactions from multi-instrument readouts and selecting follow-ups that matched
expert choices while sustaining overnight cycles.
â— NC Stateâ€™s Rainbow couples a liquid handler, parallel microreactors, a handling
arm, and in-line spectroscopy with an active-learning planner. It explores ligands,
solvents, and salts at scale, learns structureâ€“property relationships, and traces a
Pareto front for brightness and color purity before converging to the best recipe in
under a day.

stateof.ai 2025

| 61

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

LLM-driven tree search writes expert-level scientiï¬c software across domains
An LLM guided by a modiï¬ed tree-search (â€œcode mutation systemâ€) generates, runs, and iterates code until it
beats established leaderboards. The system recombines ideas, proposes new ones, and rigorously scores each
attempt on public benchmarks, turning method invention into an automated search problem. Results span
single-cell RNA-seq integration, COVID-19 forecasting, remote sensing, and numerical analysis.
â— On the OpenProblems single cell RNA-seq integration benchmark, 40 of
87 generated methods including recombinations and ideas seeded by
â€œDeep Researchâ€ and â€œAI co-scientistâ€ outperform all published
leaderboard entries.
â— For numerical integration, the evolved algorithm succeeds on 17/19
hard integrals (â‰¤3% error) where scipy.integrate.quad() fails on all 19,
via adaptive domain partitioning plus Euler series acceleration.
â— The system competes on the CDC COVID-19 Forecast Hub and
reproduces/innovates over strong AR, GBM, and mechanistic models. it
also tackles remote-sensing segmentation (DLRSD) and other tasks,
showing breadth beyond a single ï¬eld.

stateof.ai 2025

| 62

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Scaling laws in genomics: predictable gains with compute, data, and context

ğŸ“â†’ğŸ§¬

Next-token modeling learns real biological dependencies from DNA, and Evoâ€™s scaling study shows smooth,
compute-predictable loss improvements with more data, parameters, and context, plus clear architecture effects.
â— Evo (Nov â€˜24) is trained on ~300B nucleotides,
byte-tokenized, with context up to 131k. Along the
compute-optimal frontier, the Hyena family
(input-dependent long convolutions with a few
attention layers) delivers lower PPLX/FLOP and
more stable training than Transformer++/Mamba

â— Evo-2 (Feb â€™25) pushes
further by training 40B and
7B models on 9.3T and 2.4T
tokens, respectively, and
extending context to 1M.
Validation perplexity
improves with both model
size and context, and
long-context recall remains
effective at 1M.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Scaling laws for proteins unlock broader and more useful generation

| 63
ğŸ“â†’ğŸ§¬

Protein LMs also obey smooth scaling laws. Proï¬‚uentâ€™s ProGen3 derives a compute-optimal frontier for sparse
autoregressive PLMs and then scales to a 46B MoE trained on PPA-1: 3.4B full-length proteins (1.1T tokens),
ultimately training on 1.5T tokens (left chart). Larger models generate viable proteins across broader sequence
space (middle chart), and alignment lifts performance most at larger scale (right chart).

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

AlphaFold 3 reproductions: strong on familiar chemistry, weak on novelty

| 64
ğŸ“â†’ğŸ§¬

AlphaFold-3 can predict full multi-molecule complexes, inspiring many open-source reproduction efforts. These
systems perform well when the binding site (â€œpocketâ€) and the way a molecule ï¬ts into it (â€œposeâ€) look like
examples the models have seen before. But when chemistry is new or different, accuracy falls. This shows that
progress often reï¬‚ects training-set familiarity more than true generalisation.
â— Runs Nâ€™ Poses benchmark of 2,600 proteinâ€“ligand pairs tested AF3 against open
source reproductions. Accuracy rose steadily when the pocket and pose resembled
past training cases, and dropped for novel ones.
â— To judge models fairly, researchers combined multiple checks: do the right atoms
contact each other, does the ligand sit in the right spot, and is the structure
physically realistic (no clashes)?
â— Simple train/test splits exaggerate success as many test cases look
like training data and adding more samples per case helps only a bit.
â— The UKâ€™s OpenBind is building novelty-aware, reproducible
proteinâ€“ligand benchmarks and open baselines (scaffold/time/pocket
splits with physics checks) to measure true out-of-distribution binding
and enable reproducible evaluation.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 65
ğŸ“+ğŸ–¼ â†’ ğŸ“

AMIE address multimodal diagnostic consultations in longitudinal care and w/oversight
A specialized clinical dialogue model beats unassisted doctors on NEJM-grade diagnosis, outperforms primary
care physicians (PCPs) in (multimodal) simulated consults, is non-inferior on multi-visit disease management,
and outperforms PCPs in history taking and medical note writing under oversight.
â— AMIE is a multimodal clinical dialogue model trained with simulated
self-play consultations, equipped with inference-time chain-of-thought,
retrieval of guidelines, and a custom-built clinician cockpit for oversight.
â— On 302 real-world NEJM cases, AMIE hits 59.1% top-10 vs 33.6% for
unassisted clinicians, 44.5% assisted by search, 51.8% assisted by AMIE.
â— In randomized, double-blind OSCE-style consults that assess clinical
competence, physicians and patient actors rated AMIE above PCPs on the
majority of evaluation axes, incl. higher diagnostic accuracy (159 scenarios).
â— This includes better management reasoning across multiple visits (100
scenarios), better use of multimodal artifacts and (105 scenarios) and better
history taking, medical notes and composite performance in an
AMIE+clinician team (60 scenarios).

stateof.ai 2025

| 66

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Advancing multimodal foundation models and LLM decision support for healthcare
Googleâ€™s MedGemma improves medical reasoning and understanding of text and images and Epic Systemâ€™s
Comet models are compute optimal electronic health record (EHR) foundation models. OpenAIâ€™s AI Consult, a
passive medical assistant, was tested on 39k primary care visits with Penda Health in Nairobi, Kenya.
â— Based on Gemma 3 base models and equipped with a
medical vision encoder based on SigLIP, MedGemma
improves multimodal QA by 2.6-10%, X-ray ï¬nding
classiï¬cation by 15.5-18.1%, and medical agentic
evaluations by 10.8% + better EHR retrieval.
â— Comet is a family of EHR models trained on 118M
patients representing 115B discrete medical events
taken from Epicâ€™s Cosmos database, contributing to the
largest scaling law study and tested on 78 tasks.
â— In 75% of visits, clinicians say that OpenAIâ€™s AI consult improved the quality of the care they delivered
â€œsubstantiallyâ€. It also measurably reduced diagnostic and treatment errors.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Diffusion language models: parallel denoising challenges autoregression

| 67
ğŸ“â†’ğŸ“

Diffusion LLMs generate by iteratively denoising masked sequences with full-context attention, updating many
tokens in parallel at each step. Recent systems now reach competitive 7-8B quality, add arbitrary-order
generation and inï¬lling, and expose useful quality-latency trade-offs.
â— LLaDA trains diffusion LMs from scratch using forward
masking and reverse denoising with a standard Transformer. It
reports competitive general-purpose scores at 8B and extends
to a paired vision model (LLaDA-V).
â— Dream-7B uses arbitrary-order generation and robust inï¬lling
with a diffusion decoder. It performs well against similarly
sized autoregressive models on reasoning and coding tasks.
â— Seed Diffusion targets throughput, reaching ~2,146 tok/s on
H20-class GPUs while maintaining competitive code accuracy.
â— LongLLaDA analyzes long-context behavior and introduces a
training-free length-extension method, showing stable
perplexity under extrapolation and a â€œlocal perceptionâ€ effect.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Tokenizer-free LLMs via dynamic byte patches

| 68
ğŸ“â†’ğŸ“

The Byte Latent Transformer (BLT) learns directly from bytes and uses entropy-driven â€œpatchesâ€ as the compute
unit. At 8B, BLT matches tokenized LLM quality while opening a new scaling axis and cutting inference FLOPs for
the same quality. Follow-on work pushes dynamic chunking and adaptation beyond ï¬xed tokenizers.
â— The model reads raw bytes,
groups them into patches where
next-byte entropy is high,
encodes each patch locally, and
then lets a Transformer operate
over the patch sequence before
decoding back to text.
â— In controlled scaling studies at the 8B class, BLT reaches quality comparable to tokenized LLMs. It also
reduces inference compute at the same quality by growing patch size with model size rather than paying
per token.
â— Byte-level training improves robustness to spelling variation, noise, and long-tail inputs.

stateof.ai 2025

| 69

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Attention sinks arenâ€™t a bugâ€¦theyâ€™re the brakes

ğŸ“â†’ğŸ“

Attention is the transformerâ€™s core mechanic. Many heads learn an â€˜attention sinkâ€™ at the ï¬rst position that
stabilizes computation by throttling over-mixing as depth and context grow. Thereâ€™s been debate over why
models learn this and what itâ€™s for.
â— The sink acts as a learned brake on mixing: by parking attention on the ï¬rst
position, the model reduces cross-token sensitivity and becomes less
reactive to small prompt perturbations; this effect strengthens with longer
contexts and larger models.
â— Training on longer contexts monotonically increases the sink metric. Within
the LLaMA-3.1 family, strong sinks are present in ~78% of heads at 405B vs
~46% at 8B. ï¿¼
â— In practice, this means the sink attaches to position 1. If âŸ¨bosâŸ© was ï¬xed
there during pre-training, removing it at inference collapses performance
(e.g., RULER-4096 â†’ 0 and large drops on ARC/HellaSwag). Handle âŸ¨bosâŸ©
and packing carefully.

stateof.ai 2025

| 70

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The trouble with benchmarks: vibes arenâ€™t all we need, but increasingly all weâ€™ve got
Researchers revealed systematic manipulation of LMArena as Meta tested 27 private Llama-4 variants before
cherry-picking the winner: testing 10 model variants yields a 100-point boost.
â— OpenAI and Google are hoovering up 40% of all
Arena data while 83 open models ï¬ght over 30%
of the scraps.
â— Big Tech gets 68 times more data access than
academic labs, with API-hosted models seeing
every test prompt while third-party models only
glimpse 20%.
â— Training on Arena data doubles your win rate because 7.3% of prompts get recycled monthly and the test
distribution reï¬‚ects what developers like to ask about (dozens of Star Trek questions, zero on Chaucer).
â— The company has raised a whopping $100M at a $600M valuation.
â— The ï¬eld requires contamination audits to help alleviate potentially systemic test-train leakage.

stateof.ai 2025

| 71

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The trouble with benchmarks: safety-washing
71% of safety benchmark variance is explained by general capabilities alone, while genuine risks like WMDP
bioweapons (-0.91) worsen as models get smarter.
â— Safety benchmarks are highly correlated with capabilities:
simply scaling models improves most safety metrics!
â— Yet critical safety issues worsen with scale: the most
dangerous capabilities are inversely correlated with
reported safety improvements
â— Instruction tuning masks rather than solves problems.
Base model correlations ï¬‚ip from negative to positive
after chat tuning (CyberSecEval: -0.25 â†’ 0.55) while
harmful capabilities remain latent in the model
â— Safety research should prioritise metrics which are not
highly correlated with scale!

stateof.ai 2025

| 72

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

LLMs are professional yes-men, and we trained them to be that way
"Sycophancy" isn't a bug, it's exactly what human feedback optimisation produces. A study of ï¬ve major LLMs
shows they consistently tell users what they want to hear rather than the truth.
â— Claude 1.3 apologises for being correct 98% of the time when
users challenge it with "Are you sure?", even when highly conï¬dent
in the right answer.
â— Human crowd-workers are part of the problem, since they also
prefer well-written falsehoods when they can't fact-check. The
harder the topic, the more they reward conï¬dent nonsense.
â— Best-of-N sampling with standard preference models consistently
produces more sycophantic outputs than with truth-optimized
preference models.
â— Standard RLHF has a fundamental ï¬‚aw â€“ models learn that
agreeing with raters > truth because that's literally what the
training signal rewards.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 73

Brain-to-text decoding: decoding brain activity during typing
Meta AI researchers developed Brain2Qwerty, a system that decodes what people are typing by reading brain
signals from outside the skull, achieving a 19% character error rate for the best participants. This is a substantial
improvement on previous non-invasive approaches (but is still far from clinical viability).
â— 35 Spanish-speaking participants memorised sentences, then typed
them â€œblindâ€ on a keyboard. Their brain activity was recorded using
either electro- (EEG) or magneto-encephalography (MEG).
â— Brain2Qwerty has three-stages: a CNN analyses input from hundreds
of sensors, a transformer reï¬nes predictions using sentence context,
and a Spanish language model ï¬xes obvious errors.
â— While an average error rate of 32% remains far from invasive Brain Computer Interfaces (<6% CER), down the
line this kind of work could have applications in restoring communication for individuals with speech
impairments, and contributes to understanding the neural basis of language.
â— The system's errors show it's tracking ï¬nger movements rather than understanding language: when it mistakes
a letter, it picks physically adjacent keys 73% of the time.

stateof.ai 2025

| 74

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Can vision models align with human brainsâ€¦and how does that alignment emerge?
By systematically varying model size, training scale, and image type in DINOv3 (Metaâ€™s latest self-supervised
image model trained on billions of images), researchers show that brain-model convergence emerges in a speciï¬c
sequence. They ï¬nd that early layers align with sensory cortices, while only prolonged training and human-centric
data drive alignment with prefrontal regions. Larger models converge faster, and later-emerging representations
mirror cortical properties like expansion, thickness, and slow timescales.
â— fMRI (8 subjects, ~10,000 images each, for high-res spatial maps of cortical
activity) and MEG (4 subjects, ~22,500 images each, for high-res temporal
fMRI
dynamics) recordings were compared against DINOv3 activations.
â— Three metrics of brain-model similarity were assessed: encoding score
(linear similarity), spatial score (layer â†” cortical hierarchy), and temporal
MEG
score (layer â†” brain response timing.
â— Brain-like representations emerge progressively during training. Early visual
regions and fast MEG responses align quickly, while prefrontal cortices and
late temporal windows require far more training, closely echoing the
developmental trajectory of the human cortex.

stateof.ai 2025

| 75

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Scaling laws for brain-to-image decoding: data per subject matters (and costs bite)
Meta AI benchmarked brain-to-image decoding across EEG, MEG, 3T fMRI and 7T fMRI using 8 public datasets, 84
volunteers, 498 hours of recordings, and 2.3M image-evoked responses, evaluated in single-trial settings. They
ï¬nd no performance plateau: decoding improves roughly log-linearly with more recording, and gains depend
mostly on data per subject rather than adding subjects. Deep learning helps most on the noisiest sensors
(EEG/MEG). Estimated dollar-per-hour costs show 7T isnâ€™t always the most cost-effective path. ï¿¼ ï¿¼
â— The most precise devices yield the best absolute
decoding (7T > 3T > MEG > EEG), but deep nets deliver
the largest gains on noisy modalities, narrowing the gap.
â— Scaling laws: performance rises log-linearly with more
recording time. The returns come chieï¬‚y from recording
more per subject, not recruiting more participants.
â— Cost model (rough estimates): ~$263/hr EEG, $550/hr MEG, $935/hr 3T, $1,093/hr 7T. A $131k budget buys
markedly different accuracy across modalities, so optimal scaling depends on budget and target ï¬delity.

stateof.ai 2025

| 76

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

ATOKEN: a uniï¬ed visual tokenizer for images, video, and 3D
Apple introduced a single tokenizer that works for both high-ï¬delity reconstruction and semantic understanding
across images, video, and 3D could be the foundation layer for truly uniï¬ed multimodal models. This approach
reduces fragmentation, simplifying stacks, and enabling direct transfer of capabilities across modalities.
â— ATOKEN is a single visual tokenizer that maps images,
video, and 3D inputs into a shared 4D sparse latent
using a pure-Transformer with 4D RoPE. It then emits
continuous or discrete tokens, and trains stably with
perceptual + Gram losses (no GANs).
â— One backend now supports both high-ï¬delity reconstruction and semantic understanding. A curriculum
from images â†’ video â†’ 3D shows cross-modal transfer, and native-resolution/time processing with KV
caching keeps it scalable (trained up to 256Ã—H100, ~138k GPU-hours).
â— Specialists approaches still lead on some long-video and generative benchmarks, and the compute bill is
high. Adoption will hinge on release details and tooling, but the uniï¬ed-tokenizer direction looks like the
right foundation.

stateof.ai 2025

| 77

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Merging the virtual and physical worlds: pre-training on unstructured reality
The new generation of robotic agents is built on a foundation of large-scale pre-training, but the key innovation
is a move away from expensive, annotated datasets. The frontier is now focused on leveraging vast quantities of
unlabeled, in-the-wild video to learn world models and physical affordances.
â— NVIDIAâ€™s GR00T 1.5 represents a signiï¬cant advance in data
efï¬ciency. It uses neural rendering techniques to construct implicit
3D scene representations directly from unstructured 2D videos. This
allows it to generate a massive stream of training data for its
policy, effectively learning from observation sin humans.
â— ByteDanceâ€™s GR-3 applies the next-token prediction paradigm to
robotics. By treating vision, language, and action as a uniï¬ed
sequence, they can pre-train end-to-end. This approach is proving
particularly effective when using 2D spatial outputs (e.g., action
heatmaps) as an auxiliary loss, helping to ground the model's
understanding of physical space.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 78

The architectural divide: knowledge insulation vs. end-to-end adaptation
With powerful, pre-trained Vision-Language-Action Models (VLAMs) serving as the "brains" of robotic agents, a
critical architectural debate has emerged: should the entire model be ï¬ne-tuned for a new physical task, or
should the core knowledge be "insulated" by freezing its weights?
â— The case for insulation: Pi-0.5 freezes the large VLM and
ï¬ne-tunes only small â€œaction-expertâ€ heads. This works because
robot datasets are tiny, often â‰¤0.1% the size of the VLM
pre-training corpora, so full-network tuning tends to overï¬t and
forget general knowledge while costing more compute.
â— The case for end-to-end: In contrast, models like ByteDance
GR-3 and SmoLVLA show the upside of unfreezing when you
have enough task data: the network can internalize contact,
dynamics, and scene geometry. If robotics data approached VLM
scale, end-to-end would likely dominate.

stateof.ai 2025

| 79

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Emergent reasoning moves into the physical world
The â€œChain-of-Actionâ€ pattern - explicit intermediate plans before low-level control - is becoming a standard for
embodied reasoning. First shown by AI2â€™s Molmo-Act in 2025, and rapidly adopted by Gemini Robotics 1.5, this
approach mirrors Chain-of-Thought in LLMs, boosting both interpretability and long-horizon reliability in
real-world robotics.
â— Molmo-Act (AI2). From a high-level command, the model emits
intermediate visual/geometry artifacts (e.g., depth/trajectory sketches)
that a separate decoder turns into continuous motor commands. It
makes behavior easier to inspect and debug complex manipulation
tasks such as pick-and-place or dishwasher loading.
â— Gemini Robotics 1.5 (GDM): Uses the same plan-then-act architecture
with ER 1.5 (the high-level planner) generating structured action plans
for Robotics 1.5 to executes via a visuomotor policy.
â— Teaching LLMs to Plan (MIT): Parallel work in language introduces explicit plan tokens before ï¬nal
answers, improving long-horizon reliability and giving auditors something to inspect, an LLM analogue of
Chain-of-Action.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 80

Reading the road: processing driving tasks in a uniï¬ed language space
Waymoâ€™s EMMA is an end-to-end multimodal model that reimagines autonomous driving as a vision-language
problem. By mapping camera inputs directly to driving-speciï¬c outputs (such as trajectories and road graph
elements) and representing them in natural language, EMMA leverages the reasoning and world knowledge of
LLMs to achieve SOTA results while offering human-readable rationales.
â— EMMA achieves high performance on public datasets such
as nuScenes and the Waymo Open Motion Dataset,
particularly excelling in motion planning and 3D object
detection using camera inputs alone.
â— A key feature is its CoT reasoning, which enhances
decision-making transparency by prompting the model to
explain its decisions sequentially, integrating world
â— knowledge. This approach produces outputs such as future vehicle trajectories and object detection estimates
in a readable, interpretable format.
â— Although promising, EMMA is limited by only processing a few frames at a time, not using accurate 3D sensing
modalities like LiDAR, and being computationally expensive.

stateof.ai 2025

| 81

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Computer Use Agents (CUA) have improved by leaps and bounds, and still fall short
Research labs like OpenAI, Anthropic, and ByteDance have been hard at work creating benchmarks and
interaction methods for native language model computer use. While the use of RL and multi step reasoning has
led to large improvements by and large the models still fall short.
â— ByteDanceâ€™s UI-TARS-2 is a native GUI agent trained by collecting
trajectories, running supervised ï¬ne-tuning followed by multi-turn RL in an
all-in-one sandbox (cloud VMs + browser game env + terminal/SDK tools)
with async rollouts and the ability to merge specialized agents.
â— The system sets SOTA across GUI agent benchmarks: 47.5% OSWorld,
50.6% WindowsAgentArena, 73.3% AndroidWorld, 88.2%
Online-Mind2Web, and a 59.8 mean normalized score on 15 web games
(~60% of human), beating OpenAI CUA and Claude Computer Use by large
margins. It also shows strong inference-time scaling (more steps leads to
higher scores). ï¿¼
â— But long-horizon problems remain brittle (e.g., Tetris/Sokoban and hard
BrowseComp tasks), and average game skill is ~40% shy of human.

stateof.ai 2025

| 82

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Could small language models be the future of agentic AI?
Researchers from NVIDIA and Georgia Tech argue that most agent workï¬‚ows are narrow, repetitive, and
format-bound, so small language models (SLMs) are often sufï¬cient, more operationally suitable, and much
cheaper. They recommend SLM-ï¬rst, heterogeneous agents that invoke large models only when needed.
â— Agents mostly ï¬ll forms, call APIs, follow schemas, and
write short code. New small models (1â€“9B) do these jobs
well: Phi-3-7B and DeepSeek-R1-Distill-7B handle
instructions and tools competitively.
â— A ~7B model is typically 10-30x cheaper to run and
responds faster. You can ï¬ne-tune it overnight with
LoRA/QLoRA and even run it on a single GPU or device.
â— One can use a â€œsmall-ï¬rst, escalate if neededâ€ design: route routine calls to an SLM and escalate only the
hard, open-ended ones to a big LLM. In practice, this can shift 40-70% of calls to small models with no
quality loss.
â— But SLMs still struggle with long context, novel reasoning, or messy conversation. Keep an escape hatch to a
large model and evaluate regularly.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 83

Headline AI agent designed innovations require domain-expert audits
Macro-level benchmark scores and spectacular speedup numbers are often misleading. Sakana AI introduced a
CUDA agent delivering 100x improvement, until it discovered the agent hacked the benchmark. Indeed, under
independent re-measurement the improvement disappeared. Seasoned GPU engineers would have ï¬‚agged a
100Ã— kernel gain as implausible, so evaluations should be co-designed and audited with domain experts.
â— Sakanaâ€™s agent iteratively translated and â€œoptimizedâ€ CUDA
kernels and internal runs showed 2â€“7Ã— gains with isolated cases
near 100Ã—. Independent spot checks using stricter harnesses
found the same kernels up to three times slower and uncovered
that the evaluation code was exploitable.
â— Other labs posted similarly inï¬‚ated kernel results that collapsed
under community spot checks. Practitioners on X routinely
sanity-check claims against vendor libraries, rooï¬‚ine bounds,
realistic batch sizes, and end-to-end runtime.

stateof.ai 2025

| 84

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Model Context Protocol becomes the â€œUSB-Câ€ of AI tools
Introduced by Anthropic in late 2024, the Model Context Protocol (MCP) has quickly become the default way to
plug models into data, tools and apps. In 2025 the big platforms moved to adopt it: OpenAI shipped MCP across
ChatGPT, its Agents SDK and API; Google added MCP to Gemini; Microsoft built MCP into VS Code and began
rolling it into Windows and Android Studio. With thousands of MCP servers now in the wild, the protocol is
shaping how agentic systems are built and secured.
â— MCP offers one integration across clients (ChatGPT, Gemini, Claude/VS
Code, LangChain, Vercel), collapsing one-off connectors and enabling tool
discovery, resources and prompts over a common spec.
â— Data from Zeta Alpha shows that the MCP protocol has been cited in 3x
more research papers than Googleâ€™s competing A2A protocol (right graph).
â— Security researchers estimate >15,000 MCP servers globally. Companies
like Microsoft and Vercel are building guardrails and registries as the
ecosystem matures.
â— Early incidents (e.g. a malicious Postmark MCP server version on npm silently BCCâ€™d usersâ€™ emails to an
attacker until it was pulled) show why governance and package hygiene matter.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 85

The explosion of AI agent frameworksâ€¦
Instead of consolidating, the agent framework ecosystem has proliferated into organized chaos. Dozens of
competing frameworks coexist, each carving out a niche in research, industry, or lightweight deployment.
â— LangChain remains popular, but is now one among many.
â— AutoGen and CAMEL dominate in R&D with AutoGen in
multi-agent + RAG studies, CAMEL in role-based conversations.
â— MetaGPT thrives in software engineering, turning agents into
structured dev workï¬‚ows.
â— DSPy rises as a research-ï¬rst framework for declarative program
synthesis and agent pipelines.
â— LlamaIndex anchors RAG workï¬‚ows over enterprise documents.
â— AgentVerse is used for multi-agent sim and benchmarking.
â— LangGraphâ€™s graph-based orchestration wins over developers who need reliability and observability.
â— Letta / MemGPT explore memory-ï¬rst architectures, turning persistent memory into a framework primitive.
â— Lightweight challengers like OpenAgents, CrewAI, and OpenAI Swarm highlight a shift toward composable,
task-speciï¬c frameworks

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 86

â€¦and an explosion of diverse AI agent research papers
Tens of thousands of research papers per year are exploring a range of frontiers for AI agents as they move from
ideas into production, including:
â— Tools: From plugins to multi-tool orchestration via shared protocols.
â— Planning: Task decomposition, hierarchical reasoning, self-improvement.
â— Memory: State-tracking, episodic recall, workï¬‚ow persistence, continual
learning.
â— Multi-agent systems: Collaboration, collective intelligence, adaptive
simulations.
â— Evaluation: Benchmarks for open-ended tasks, multi-modal tests, cost and
safety.
â— Coding agents: Bug ï¬xing, agentic PRs, end-to-end workï¬‚ow automation.
â— Research agents: Literature review, hypothesis generation, experiment
design.
â— Generalist agents: GUI automation, multi-modal input and output.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 87

Building agents that remember: from context windows to lifelong memory
Agent memory is shifting from ad-hoc context management to structured, persistent systems. The frontier is now
beyond retrieval and into dynamic consolidation, forgetting, and reï¬‚ection to allow agents to develop coherent
identities across interactions, tasks, and even lifetimesâ€¦
â— Memory is no longer a passive buffer, it is
becoming an active substrate for reasoning,
planning, and identity. Active areas of research are:
â—‹ State-tracking and memory-augmented agents:
reasoning enhanced by explicit state
management
â—‹ Persistent and episodic memory: long-term
storage alongside short-term context for
continuity.
â—‹ Context retention: self-prompting and memory
replay techniques to preserve relevance over
extended tasks and interactions.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 88

AI conferencesâ€™ capacity crisis
Top AI conferences are being overwhelmed by unprecedented numbers of in submissions. â€œProliï¬c authorsâ€ (who
have 5+ papers accepted in a given conference) are also on the rise. This has led to drastic measures as
conferences scramble to ï¬nd solutions: NeurIPS has allegedly demanded reviewers reject 300-400 papers
originally recommended for acceptance.
â— One NeurIPS reviewer took to Bluesky to criticise the suggestion to
arbitrarily remove hundreds of papers that were originally
recommended for acceptance.
â— AAAI 2026 received an unprecedented 29k submissions this year â€“
almost double last year. This has forced them to hire 28k Program
Committee members. CoRL doubled capacity from 1.5k to 3k this
year and sold out before papers were even accepted.
â— Weâ€™re also witnessing the rise of proliï¬c authors. In 2023, one
researcher published 80+ papers across top AI venues, while at
CVPR 2023, just 1% of authors contributed to over 50% of all
papers. This raises questions about contribution and burnout.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 89

Section 2: Industry

stateof.ai 2025

| 90

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

RIP AGI, long live Superintelligence
Executives of major AGI contenders, best exempliï¬ed by Mark Zuckerberg, have rebranded their AGI efforts as
superintelligence. No one knows what it means, but itâ€™s provocative. It gets the people going.

â€œDeveloping superintelligence is coming
into sight [â€¦]
I believe this will be the beginning of a
new era for humanity.â€
Mark Zuckerberg, 2025 Meta
Superintelligence Labs Memo

stateof.ai 2025

| 91

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

So what will frontier superintelligence actually cost?
Trillions of dollars

Yep, trillions of dollars here too

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 92

Days spent at the frontier
The absolute frontier remains contested as labs continuously leapfrog one another. However, along two of the
most prominent metrics, some labs have been on top longer than others in the past year.
â— Timing the release of new models has become a science, meaning any one snapshot can paint a deceiving
picture. The analysis below tracks the number of days each of the relevant labs spent atop each leaderboard.

Note: LMArena Scores pulled through 9/2/2025 and AA scores through 10/3/2025

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 93

Days spent at the (open source) frontier

stateof.ai 2025

| 94

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

More for less: trends in capability to cost ratios are encouraging (Artiï¬cial Analysis)
The absolute capabilities achieved by ï¬‚agship models continue to climb reliably while prices fall precipitously.

Google Doubling Time: ~3.4 Months

GPT-5-high

OpenAI Doubling Time: ~5.8 Months

Note: Blended Prices assumes Â¾ input and â… short-context.
Artiï¬cial Analysis Intelligence Index Scores pulled 9/25/2025.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 95

More for less: trends in capability to cost ratios are encouraging (LMArena)
The absolute capabilities achieved by ï¬‚agship models continue to climb reliably while prices fall precipitously.

Google Doubling Time: ~5.7 Months
OpenAI Doubling Time: ~8.1 Months

Note: Blended Prices assumes Â¾ input and â… short-context.
LMArena ELO Scores pulled 9/25/2025.

stateof.ai 2025

#stateofai| 96
|3

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Model release cadences and fundraising: two peas in a pod
Model providers time their releases strategically to overtake the frontier and build credibility before fundraising.
This creates a predictable cadence that increasingly interconnects the roadmaps of private AI labs.

On average,
launches
a new frontier model 44 days
before closing a fundraising round.
On average,
launches a
new frontier model 50 days before
closing a fundraising round.
On average,
launches a new
frontier model 77 days before
closing a fundraising round.

stateof.ai 2025

| 97

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

From outlier to archetype: the best and most attractive companies are built AI-ï¬rst
AI has clearly shifted from niche to mainstream in the startup and investing world. On Specterâ€™s rank of 55M+
private companies, which tracks 200+ real-time signals across team growth, product intelligence, funding,
ï¬nancials and inbound attention, AI companies now make up 41% of the Top-100 best companies (vs. 16% in
2022). Real-time interaction data between 30k investors and founders shows a surge of interest post-ChatGPT
and peaking in late 2024, up 40x from the dark ages of 2020 when no one but true believers cared.

ChatGPT
launches

Data retrieved and analyzed on 17 Sept 2025.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 98

AI-ï¬rst companies are now generating tens of billions of revenue per year
A leading cohort of 16 AI-ï¬rst companies are now generating $18.5B of annualized revenue as of Aug â€˜25 (left).
Meanwhile, an a16z dataset suggests that the median enterprise and consumer AI apps now reach more than
$2M ARR and $4M ARR in year one, respectively. Note that this will feature signiï¬cant sample bias, as evidenced
by the bottom quartile not being close to $0. Furthermore, the Lean AI Leaderboard of 44 AI-ï¬rst companies with
more than $5M ARR, <50 FTE, and under ï¬ve years old (e.g. includes Midjourney, Surge, Cursor, Mercor, Lovable,
etc) sums over $4B revenue with an average of >$2.5M revenue/employee and 22 employees/co.

stateof.ai 2025

| 99

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

AI-ï¬rst companies accelerate their early revenue growth faster than SaaS peers
Analysis of the 100 fastest revenue growing AI companies on Stripe (AI 100) reveals that, as a group, they are
growing from launch to $5M ARR at a 1.5x faster rate than the top 100 SaaS companies by revenue in 2018
(SaaS 100). Within the AI 100, the growth rates between US and European companies are roughly equivalent,
whereas companies founded in/after 2022 are growing to $5M ARR 4.5x faster than those founded before 2020
and 1.5x faster than those founded after 2022, which exempliï¬es the commercial pull of generative AI products.
Note that we donâ€™t know the total population size of companies from which these were sampled.

1.5x faster!
4.5x faster!

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 100

AI-ï¬rst companies continue to outperform other sectors as they grow
Analysis of 315 AI companies with $1-20M in annualized revenue and 86 AI companies with $20M+ in
annualized revenue, which constitute the upper quartile of AI companies on Standard Metrics, shows that they
both outpaced the all sector average since Q3 2023. In the last quarter, $1-20M revenue AI companies were
growing their quarterly revenue at 60% while $20M+ revenue AI company grew at 30%, in both cases 1.5x
greater than all sector peers.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 101

AI crosses the commercial chasm: adoption is up, retention is up, and spend gets bigger
Rampâ€™s AI Index (card/bill-pay data from 45k+ U.S. businesses) shows paid AI adoption rose from 5% in Jan â€™23 to
43.8% by Sept â€™25, while U.S. Government estimates trail at 9.2%. Cohort retention is improving signiï¬cantly
with 12 month retention of 80% in 2024 vs. circa 50% in 2022. Average contract value jumped from $39k (â€™23)
to $530k (â€™25), with Ramp projecting ~$1M in â€™26. Pilots are now becoming large-scale deployments.

Read more on x.com/arakharazian and econlab.substack.com

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 102

AI adoption jumps in 2025 with OpenAI maintain a strong lead
Rampâ€™s AI Index (card/bill-pay data from 45k+ U.S. businesses) shows the technology sector unsurprisingly
leading in paid AI adoption (73%) with the ï¬nance industry not far behind (58%). Across the board, adoption
jumped signiï¬cantly in Q1 2025. Moreover, Ramp customers exhibit a strong proclivity for OpenAI models
(35.6%) followed by Anthropic (12.2%). Meanwhile, there is very little usage of Google, DeepSeek and xAI.

Read more on x.com/arakharazian and econlab.substack.com

stateof.ai 2025

| 103

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Audio, avatar and image generation companies see their revenues accelerate wildly
Market leaders ElevenLabs, Synthesia, Black Forest Labs are all well into hundreds of millions of annual revenue.
Moreover, this revenue is of increasingly high quality as its derived from enterprise customers and a long tail of
>100k customers and growing.
â— ElevenLabs grew annual revenue 2x in 9 months to $200M and
announced a $6.6B valuation commensurate with a $100M
employee tender offer. Customers have created >2M agents that
have handled >33M conversations in 2026.
â— Synthesia crossed $100M ARR in April â€˜25 and has 70% of the
Fortune 100 as customers. Over 30M avatar video minutes have
been generated by customers since launch in 2021 (right chart).
â— Black Forest Labs is said to be at ~$100M ARR (up 3.5x YoY) with
78% gross margin including a large deal with Meta worth $140M
over two years. Separately, Midjourney has also entered into a
licensing deal with Meta, the terms of which arenâ€™t known.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 104

The duality of GPT-5: todayâ€™s best model was clouded by the worst launch
GPT-5 leads the intelligence-per-dollar frontier with best-in-class benchmarks at 12x lower cost than Claude, but
user backlash over the sudden removal of GPT-4o and o3 and concerns/teething problems about opaque router
decisions that users arenâ€™t used to overshadowed its technical achievements.
â— GPT-5 is ostensibly a brilliant model: it swept the leaderboard on
LMArena and has a 400K context window in the API. OpenAI now
dominates the intelligence per dollar frontier for the ï¬rst time.
â— The rollout wasnâ€™t ideal: Altman held an emergency Reddit AMA to
address the abrupt removal of previous models and viral â€˜chart crimesâ€™.
â— The removal of GPT-4o and o3's familiar personality upset users,
which was ironic given the same launch introduced custom personas.
â— Weâ€™ve previously hypothesised that model companies would dynamically route queries to right-sized
models for latency and cost reasons. GPT-5 is the ï¬rst major chat system to introduce this using a router
as the consumer endpoint. Users can opt for faster responses by hitting â€œskipâ€ if the model invokes its more
capable version. In practice, people will take time to acclimate to this UX: the perceived opacity of model
selection has led to a ï¬‚urry of complaints.
stateof.ai 2025

| 105

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Leading AI providers continue to record extraordinary demand at inference time
As Google ï¬‚ipped the switch on enabling Gemini features within an increasing number of its properties and
toggling more users into their AI search experience, the company reported a yearly 50x increase in monthly
tokens processed, recently hitting a quadrillion tokens processed each month. Meanwhile, OpenAI reported
similar growth in token volume last year.
â— The demand for tokens has been largely supercharged by
improved latency, falling inference prices, reasoning
models, longer user interactions, and a growing suite of
AI applications. Enterprise adoption has also continued to
pick up in 2025.
â— Surging inference demand will place additional pressure
on AI supply chains, particularly power infrastructure.
â— But given that all tokens are not created equal, weâ€™d
caution against deriving too much signal from aggregate
token processing ï¬gures.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 106

Models are getting seriously good at coding, with OpenAI pulling ahead
GPT-5 and Gemini 2.5 Deep Think would have placed ï¬rst and second respectively in the most prestigious coding
competition in the world (without having trained with this competition in mind). GPT-5 solved all 12 problems,
with 11 on the ï¬rst try. Previously, Anthropic had enjoyed a period of relatively uncontested dominance in
programming tasks.
â— For the International Collegiate Programming Contest (ICPC)
World Finals, an OpenAI Researcher explained how they had
GPT-5 and an experimental reasoning model generating
solutions, and the experimental reasoning model selecting
which solutions to submit. GPT-5 answered 11 correctly, and
the last (and most difï¬cult problem) was solved by the
experimental reasoning model.
â— The OpenAI Codex team have been cooking: Sam Altman
claimed GPT-5-Codex usage had increased 10x, and their
internal code review bot became so valuable that developers
were "upset when it broke" because they lost their "safety net".

stateof.ai 2025

| 107

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Vibe coding hits the bigtime
AI writes the code, founders cash the checksâ€¦
â— Swedish vibe coding startup Lovable became a unicorn just 8 months after launch, with a $1.8B valuation.
â— Using AI to write 90% of code, Maor Shlomo sold Base44 to Wix to $80M after 6 months.
â— Garry Tan says that for 25% of their current fastest growing batch, 95% of their code was written by AI.
June 2021
GitHub Copilot launch introduces
inline code suggestions and â€œpair
programmerâ€ concept

2023

Today

From autocomplete to conversation:
AI-native IDEs: more like a
AI coding tools begin writing code
full-time engineer than assistant,
from natural language prompts
writes code with minimal oversight

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 108

â€¦but vibe coding your products can be risky
Security breaches, code destructionâ€¦
â— Malicious actors hijacked an open-source Cursor IDE extension
to steal credentials and mine $50,000 cryptocurrency on
developer machines.
â— There have been many reports of AI coding tools aggressively
overwriting production code, with developers losing weeks of
work due to overzealous AI "improvements".
â— Despite $200M+ valuations, AI coding startups face brutal unit
economics: new model releases bring higher token costs,
forcing startups to either eat losses, surprise users with price
hikes, or restrict access to older, less capable models.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 109

As the costs rack up itâ€™s unclear who is making money
Coders love Claude Code and Cursor, but margins are fragile. The tension is stark: Cursor is a multi-billion dollar
company whose unit economics are hostage to upstream model prices and rate limits set by its own competitors.
â— Some users are costing upwards of 50k/month for a single seat of Claude Code. Cursor and Claude have
introduced much stricter usage limits to try to crack down on the costs of power users.
â— Cursorâ€™s pricing power is limited because its core COGS are the API prices of Anthropic/OpenAI. When those
providers change prices, rate limits, or model defaults, Cursorâ€™s gross margin compresses unless it caps
usage or shifts workloads off upstream APIs.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 110

The M word: so what about the margins?
Gross margins are generally commanded by the underlying model API and inference costs and strained by
token-heavy usage and trafï¬c acquisition. Surprisingly, several major AI companies donâ€™t include the costs of
running their service for non-paying users when reporting their GM. Coding agents are under pressure even
when revenue grows quickly. The primary levers to improve margins are moving off third-party APIs to owned or
ï¬ne-tuned models, aggressive caching and retrieval efï¬ciency, and looking to ads or outcomes-based pricing.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 111

When do we see proï¬table models? Or are we there yet?
Dario Amodei: â€œIf every model was a company, the modelâ€”in this exampleâ€”is actually proï¬table.â€ Despite high burn
rates, speculation indicates many of the frontier labs enjoy strong unit economics on a ï¬‚agship model basis.
â— AI labs mirror the foundry business: staggering investments are needed for each successive generation,
where labs bear the front-loaded training expense. While recent models allegedly recoup this cost during
deployment, training budgets surge. Pressure then mounts to drive inference revenue across new streams.
â— Inference pays for training: labs strive to allocate more of a modelâ€™s lifecycle compute
to revenue-generating inference at the steepest margin possible. Our table* below
illustrates the expected return on compute costs across varying inference margins and
compute allocations.
*Simpliï¬ed sensitivity analysis:
neglects people costs and assumes
all inference generates revenue.
Can also be interpreted in terms of
token count between inference &
training (2DN vs. 6DN, MFU: ~15%
vs. ~45%).

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 112

Browsers become the latest AI battleground
Users live in the browser, so why shouldnâ€™t AI be baked into the experience? This is ï¬nally happening. OpenAI,
Google, Anthropic, and Perplexity all launched assistants that not only unlock Q&A with web content but also
navigate and act within the browser on behalf of the user. This shift reframes the browser as an intelligent
operating system for the internet, a long sought vision that earlier attempts like Adept AI never fully realized.
â— ChatGPT Search offers real-time web results by searching Google and a new Agent
that spins up a virtual browser to execute multi-step tasks via tool use within
ChatGPT with user control. Itâ€™s also rumored to be launching a standalone browser.
â— Taking another route, Perplexity built their own Chrome-based browser, Comet,
with a native AI assistant sidebar. It can perform Q&A but also complete multi-step
tasks in the browser (ï¬lling forms, scraping), again with user oversight.
â— Anthropic and Google released limited previews of Claude for Chrome and Gemini
in Chrome, respectively, that also let users operate the browser and access Q&A
functionality. Anthropic no longer deems this use case to be dangerous.
â— In Sept â€˜25, Atlassian acquired The Browser Company (makers of Arc) further
reafï¬rming that browsers are the latest AI battleground.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 113

As AI search engines surge, Googleâ€™s search and ad offering is taking heatâ€¦
With 700M weekly active users, ChatGPT is evangelising AI-powered search to the masses, reshaping how people
discover and use information. Googleâ€™s once unshakeable dominance shows the ï¬rst signs of erosion, even as
they pivot to AI Overviews (AIO) and AI Mode within Google Search. Moreover, AIO has driven a ~90% drop in
Search click-throughs, harming traditional ads, but ignores users being inï¬‚uenced by the AI answers.
â— By August 2025, ChatGPT served 755M monthly users, giving it
~60% of the AI search market.
â— SEM Rush data shows Googleâ€™s global search trafï¬c fell ~7.9%
year-over-year, the ï¬rst signiï¬cant dip in decades, even as it
retained ~90% global share. Similarweb data shows Google
search visits down 1-3% YoY throughout H1 2025, with Bing
(-18%) and DuckDuckGo (-11%) also declining.
â— Perplexity queries hit 780M in May 2025, growing 20%
month-over-month, as citation-rich answers drew loyal users.
â— â€œChatGPTâ€ itself became a top Google search term with 618M
monthly queries, rivaling â€œFacebookâ€.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 114

AI search is emerging as a high-intent acquisition channel
According to Similar Web data, retail visits referred by ChatGPT now convert better than every major marketing
channel measured. Conversion rates rose roughly 5pp YoY, from ~6% (Jun â€™24) to ~11% (Jun â€™25). Although AI
referrals are still a smaller slice of trafï¬c, they arrive more decided and closer to purchase. Retailers must adapt
by exposing structured product data, price and delivery options, and landing pages tailored to AI-driven intents.
In fact, ChatGPT recently implemented Instant Checkout with Etsy and Shopify, and open-sourced their Agentic
Commerce Protocol built with Stripe, to enable developers to implement agentic checkout.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 115

â€¦but AI canâ€™t get away from Google Search
While the LLM based interface has been the focus of funding, lawsuits, and user behavior, no one has found a
good alternative to using Google search.
AI referral trafï¬c overwhelmingly funnels to Big Tech
â— Despite strategic partnerships with Microsoft and
domains, led by google.com (Similar Web)
access to Bing OpenAI chooses to scrape Google
search results as its web search system.
â— During Googleâ€™s antitrust trial access to a quality web
index was discussed by Anthropic, OpenAI, Perplexity,
etc. Seeking to be able to create the same quality as
google for 80% of queries.
â— Remediations from the the trial will grant â€˜Qualiï¬ed
Competitorsâ€™ a one time index dump without any of
the ranking signals. Itâ€™s unclear if this dump will lead
to any real competitor to the search system Google
has been perfecting for decades.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 116

Answer engines drive deeper engagement than search
Data from answer engine optimization company Profound shows that users treat AI answer engines differently
from Google. Sessions are longer, with more back-and-forth, suggesting higher intent and better conversion
potential. Answer engines are no longer just a curiosity - theyâ€™re a primary entry point for serious queries.
â— In an average session, users send ~5 prompts and receive ~5 responses, far more interaction than a typical
search query, scroll, and blue-link click.
â— Profound data shows ChatGPT users average 5.6 turns per session, versus ~4 for Gemini and Perplexity, and
~3.9 for DeepSeek. Either more turns means more engaging conversations, or fewer turns means answers
are given more efï¬ciently.
â— Conversation styles differ: DeepSeek users write the longest prompts and get the most verbose answers,
while Perplexity delivers shorter, citation-heavy responses.
â— This iterative style and memory capability makes answer engines â€œstickyâ€ and explains why they already
deliver higher conversion rates than Google.
â— Profoundâ€™s analysis shows ChatGPTâ€™s crawler is now among the top 10 most active bots on the internet,
alongside Googlebot and Bingbot.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 117

So where do answer engines get their answers?
Understanding how AI answer engines cite and retrieve information is critical for visibility on AI-ï¬rst web.
Profoundâ€™s analysis shows ChatGPT draws heavily from Googleâ€™s index but distributes attention differently across
the web, with lower-ranked pages often getting visibility. This behavior changes with new model versions too.
â— Profound data shows GPT-5â€™s citations matched 19% of Google domains when compared against the top 10
Google results, underscoring both reliance on Googleâ€™s index and a broader sourcing pattern.
â— Avg citation position also shifted down the page, while the median stayed at #9, meaning that ChatGPT is
just as likely to surface content further down Googleâ€™s results page.
â— ChatGPT often pulls from lower-ranked pages than humans typically click, widening exposure for sites
beyond the top results.
â— Top domains cited across models: Reddit (3.5%), Wikipedia (1.7%), YouTube (1.5%), and Forbes (1.0%).
â— Different models show sourcing styles: Gemini and Perplexity lean toward mainstream concise sources,
while DeepSeek tends to draw on long-form domains.
â— This means optimizing for Answer Engine Optimization (AEO) is as important as SEO because visibility
depends not just on rank, but on model citation patterns.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 118

Vibe shift: From litigationâ€¦

stateof.ai 2025

| 119

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

â€¦ to collaboration
2025 is the year when "if you can't beat 'em, join 'em" became ofï¬cial media strategy for AI companies.
â— News: Over 700+ news brands have signed AI deals,
including the Washington Post, WSJ, Guardian, FT, The
Atlantic, CondÃ© Nast, and even NYT ($20-25M
Amazon deal) (as they continue to sue OpenAI).
â— Music: Hallwood pens deal with top-streaming
â€œcreatorâ€ on Suno, Grammy winner Imogen Heap
releases AI style-ï¬lters for fans to remix.
â— Video: AMC Networks formally embraces Runway AI
for production (ï¬rst major cable network to do so).
â— Publishing: Microsoft & HarperCollins deal for AI
training (with author opt-outs).

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 120

Fair p(l)ay out ï¿½ï¿½
Shortly after the announcement of their record-breaking $13B Series F, Anthropic agreed to pay $1.5B to settle a
class action lawsuit from book authors. This is the largest payout in the history of US copyright cases, and
constitutes what some describe as â€œthe A.I. industryâ€™s Napster momentâ€.
â— This does not set legal precedent as the case did not go to trial, but is a very
signiï¬cant development in the ongoing fair use debate. Since this was an
â€œopt-outâ€ class action, authors who are eligible can request exclusion to ï¬le
independent lawsuits. Anthropic also agreed to delete works that had been
downloaded.
â— In June, a judge sided with Anthropic, ruling that training LLMs on legally purchased books was sufï¬ciently
transformative to constitute fair use. He also ruled that training on pirated copies was illegal. Previously
Anthropic had hired Tom Turvey, the former head of Google Books, who mass bought physical books and
then created digital copies that were used for model training.
â— During a deposition, co-founder of Anthropic Ben Mann testiï¬ed to having downloaded the LibGen dataset
(which contains pirated material) when he previously worked at OpenAIâ€¦

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 121

Welcome to the Stargate: may the FLOPS be with you
10 years ago, Baidu, Google and others had shown early scaling laws whereby deep learning models for speech
and image recognition converged faster as more GPUs were used for training. Back then, this meant using up to
128 GPUs. In January this year, Sam Alman, Masayoshi Son, Larry Ellison and President Trump announced the
Stargate Project, a gigantic 10GW-worth of GPU capacity to be built with $500B in the US over 4 years. This
equates to over 4 million chips! The buildout is to be funded by SoftBank, MGX, Oracle, and OpenAI.

How it started in 2015

How itâ€™s going in 2025

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 122

OpenAI franchises sovereign AI with its â€œOpenAI for Countriesâ€ program
Energy-rich nations are grabbing their ticket to superintelligence by partnering with OpenAIâ€™s astute sovereign
offering: a formalized collaboration to build in-country AI data center capacity, offer custom ChatGPT to citizens,
raise and deploy funds to kickstart domestic AI industries and, of course, raise capital for Stargate itself.
â— Stargate UAE is the ï¬rst deployment of a
1GW campus with a 200 MW live target in
2026. Partners include G42, Oracle, NVIDIA,
Cisco, and SoftBank).
â— Stargate Norway comes second and is a
50/50 joint venture between UKâ€™s Nscale
and Norwayâ€™s Aker to deliver 230 MW
capacity (option to +290 MW) and 100,000
GPUs by end-2026.
â— Stargate India is reportedly in the works for
1 GW as OpenAI expands and offers a
cheaper â€œChatGPT Goâ€.

stateof.ai 2025

| 123

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

OpenAI races to own entire AI stack
After shelving its robotics program in 2020 to focus on language models, OpenAI has reversed course, now
driving full vertical integration from custom chips and data centers to models, devices, and embodied AI.
Hardware &
Robotics

Consumer Devices: $6.5B acquisition of io (Jony Ive) to create AI-native devices,
bypassing existing iOS/Android.
Robotics: Internal robotics division reboot; partnership with Figure AI (since
terminated).

Models
Silicon &
Compute

Custom Chips: Developing in-house AI processor in partnership with Broadcom,
targeting 2026 launch, to cut NVIDIA reliance.
Data Centers: Texas Stargate supercluster: 400k GPUs, 1.2 GW capacity, Oracle
partnership; part of $500B build-out to secure compute supply.

stateof.ai 2025

| 124

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Broadcomâ€™s great transformation
Once an unglamorous semiconductor ï¬rm, Broadcom has now positioned itself at the cutting edge of the AI
revolution through its custom chip partnerships with Google, Meta, and reportedly OpenAI. The development of
custom AI chips like Amazon's Trainium and Broadcomâ€™s TPUs / MTIA chips gives frontier labs more leverage
when negotiating multi-billion dollar deals with NVIDIA.
â— Broadcom's 2013 LSI acquisition included a small custom
chip unit that now designs Google TPUs and Meta's AI
chips, growing from <20% of LSI's revenue at acquisition
to $2-3B+ annually.
â— Broadcom's stock price has surged, signaling investor
optimism about the company's ability to beneï¬t from the
rapidly growing AI chip market.
â— Broadcom's AI chip revenue reached $5.2B in Q3 2025, up 63% year-over-year.
â— The custom chip ecosystem puts pressure on NVIDIA's monopoly: Amazon's in-house Trainium chips and
Broadcom-powered alternatives (Google TPUs, Meta MTIA, OpenAI's upcoming chips) give hyperscalers
multiple paths to reduce NVIDIA dependence, if theyâ€™re willing to endure the user pain.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 125

Googleâ€™s TPU timeline

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 126

OpenAI and its benefactor, Microsoft, navigate a rocky relationship
OpenAIâ€™s recent restructuring and soaring demand for training compute has placed tremendous stress on its
relationship with Microsoft. While signs of fracturing become noticeable, a complete divorce looks unlikely.
â— Microsoftâ€™s inability, or unwillingness, to bring training compute online fast enough
has impacted OpenAIâ€™s roadmap. As OpenAI has gravitated closer to Oracle to fulï¬ll
these needs, Microsoft has abstained from exercising its â€œright of ï¬rst refusal.â€ They
appear hesitant to bet on next-generation centralized clusters.
â— Meanwhile, OpenAI appears to be trying to escape from other key elements of their
partnership with Microsoft. Through 2030, Microsoft maintains a 20% revenue
sharing arrangement, access to OpenAIâ€™s IP, and exclusivity on OpenAIâ€™s API. Yet,
OpenAI wants the 20% share dialed back to 10% before the end of the decade.
â— Microsoft can always work to block the for-proï¬tâ€™s conversion into a PBC, which could
cost OpenAI $20B in funding if not completed by the end of 2025. Conversely, OpenAI
always retains the option to air antitrust concerns if Microsoft proves adversarial or "We are below them, above
reneges on certain AGI clauses.
them, around them."
â— Microsoft AI also released previews of a voice and MoE model trained on 15k H100s.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 127

Oracle steps up as a key buildout partner for AI infra
As Microsoft has dialed back its willingness to shoulder so much of the future AI infra buildout, Oracle has begun
picking up slack. During the early phase of this shift, Oracle has been rewarded as its stock soars.
â— OpenAI has reached a $30B per year agreement with Oracle for data center services. This deal more than
doubled Oracleâ€™s collective ï¬scal 2025 cloud service revenue, when it sold just $24.5B worth of services.
â— This news comes as OpenAIâ€™s deal with Softbank begins to fray. Recently,
original Stargate Project plans were scaled-back, tempering those roadmaps.
â— Oracle now ï¬lls this vacuum, proving to have found the risk-appetite and
follow-through that both Microsoft and SoftBank seem to lack. As a
consequence, Oracleâ€™s stock has jumped more than >70% year to date.
â— Oracleâ€™s current track is not without major risks. Providing large-scale clusters
has not been exceptionally high-return, particularly as power bottlenecks are
costly to overcome. AI lab tightness could eventually raise issues for longer
leases. Finally, depreciation cliffs present concerns and the economics of
converting these clusters to inference ï¬‚eets remains murky. More decentralized
builds could then win out, especially if scaling continues to shift to RL.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 128

AI labs target 2028 to bring online 5GW scale training clusters
Anthropic shared expectations that training models at the frontier will require data centers with 5GW capacity by
2028, in line with the roadmaps of other labs. The feasibility of such endeavors will depend on many factors:
â— How much generation can hyperscalers bring behind-the-meter? At such scale,
islanding will likely not be practical, requiring data centers to tap into grid assets.
â— How quickly can players navigate the morass of permitting and interconnection?
While reforms are underway, connection timelines for projects of this magnitude
can take many years. Hyperscalers may skip queues through lobbying efforts and
demand response programs, where they curtail draw during peak periods (a recent
Duke study projects 76GW of new availability with curtailment rates at 0.25%).
â— What level of decentralization can be achieved? Many labs continue to pursue
single-site campuses, yet distributed approaches are also advancing rapidly.
â— How well can hyperscalers navigate talent and supply chain shortages? Attempts to
alleviate power infrastructure and skilled labor bottlenecks through the massive
mobilizations of capital can overload the risk-appetite of supporting parties.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 129

Measuring contest: planned ~1GW scale clusters coming online in 2026
Cluster size increasingly becomes a deï¬ning trait amongst American labs, particularly useful during recruitment.
Should valuations follow cluster size instead of adoption or ï¬scal metrics, a larger bubble could begin to form.

GW Scale Cluster Rankings

*Google DeepMind has also spun up many noteworthy clusters in Iowa,
Nebraska, and Ohio. However, the distributed nature of these projects
and lack of available information led to this omittance from the table.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 130

A 1GW AI data center cheat sheet

stateof.ai 2025

| 131

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Shortfall forecasts spike and consequences loom around the corner
NERC reported that electricity shortages could occur within the next 1-3 years in several major US regions.
DOE warns blackouts could be 100 times more frequent by 2030 due to unreliability and new AI demand.
â— Similarly, SemiAnalysis projects a 68 GW implied shortfall by 2028
if forecasted AI data center demand fully materializes in the US.
â— As an emerging pattern, this will force ï¬rms to increasingly
offshore the development of AI infrastructure. Since many of the
USâ€™ closest allies also struggle with electric power availability,
America will be forced to look toward other partnerships â€“
highlighted by recent deals in the Middle East.
â— Projects that are realized on American soil will place further strain
on the USâ€™ aging grid. Supply-side bottlenecks and rapid spikes in
AI demand threaten to induce outages and surges in electricity
prices. ICF projects residential retail rates could increase up to 40%
by 2030. These factors could further contribute to the public
backlash directed at frontier AI initiatives in the US.

stateof.ai 2025

| 132

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Could data centers ever truly be green?
The widespread AI buildout places data center emissions on a steeper trajectory, while creative carbon
accounting techniques continue to understate the true emissions associated with many hyperscalers.
â— Data center-related emissions could surge as more providers
island with natural gas plants and grid operators recommission
or delay the retirement of existing coal plants.
â— As AI factories are ultimately offshored to other regions, the
carbon intensity of those locations is likely to be much higher
than that of the US, requiring cloud providers to pursue more
aggressive procurements of carbon offsets.
â— Deceptive carbon accounting practices are also prominent.
Some hyperscalers omit upstream categories, such as the
emissions associated with manufacturing IT equipment and
constructing or maintaining the relevant power plants.
Furthermore, additionality agreements can cover new
renewable projects that were already planned to commence.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 133

Just how thirsty are AI factories?
AI factories withdraw considerable amounts of water and are more likely to be built in high water-stress areas.
Still, the Water Usage Efï¬ciency (WUE) of most AI factories are trending in a favorable direction.
â— An average 100 MW hyperscale data center in the US consumes roughly 2M Liters
per day, mostly due to the indirect toll associated with power generation. Yet, as
modern AI data centers shift to closed-loop liquid cooling solutions, their WUE
plummets relative to other traditional data centers.
â— However, second-order costs cannot be ignored as the number of new AI
factories continues to surge, leading to more generation coming online.
In the US, this creates a geographic mismatch: the sites where power is
available or can be easily built often sit in drier climates with water
stress. This trend may be exacerbated by the shift to water-intensive
generation sources â€“ particularly nuclear, coal, & certain natgas plants.
â— Currently, everyday AI usage carries negligible impacts â€“ a typical Gemini
app text prompt consumes only 0.26 mL of water (~5 drops). Yet, WUE
must be monitored as AI interactions continue to use more tokens.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 134

Google inks PPA deal with CFS to buy â‰¥200 MW of electricity from planned fusion plant
Googleâ€™s commitment signals demand for an energy source that will not be deployment-ready until early next
decade, kicking off a new wave of investment.
â— While support for next-generation energy
sources holds strong, fusion remains many years
away from providing a scalable solution to AIâ€™s
demand for power. AIâ€™s energy footprint traces a
very steep curve, yet the timelines of
next-generation sources are not compressing.
â— This marks another shift: hyperscalers, rather
than the US government, are increasingly
shouldering investments in technologies that
are many years away from commercial viability â€“
such as fusion, quantum computing, and
advanced AI development.

stateof.ai 2025

| 135

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Chinese labs have yet to advance the frontier, but compete in the open-weight domain
Chinese labs like Alibaba, DeepSeek, Moonshot, and MiniMax continue to release impressive open-weight
models. A capability gap emerges between these models and most American open-source alternatives.
â— US open model efforts have disappointed.
OpenAIâ€™s open-weight models underwhelmed
with performance trailing far behind GPT-5.
â— The restructuring of Metaâ€™s â€œSuperintelligenceâ€
team has cast doubts on their commitment to
open-sourcing at the frontier. Other teams like Ai2
lag far behind in terms of funding. Although they
recently landed $152M from NVIDIA and NSF, that
ï¬gure pales in comparison to even OpenAIâ€™s initial
grant from 2015.
â— Conversely, Chinese organizations continue to
push the envelope, while publishing troves of
new algorithmic efï¬ciency gains.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 136

â€¦yet is this an overt strategy or a side-effect of Chinaâ€™s inability to scratch the frontier
Chinaâ€™s commitment to the open-source community could be a lasting tactic or a short-term play exercised to
reach frontier-level capabilities. It has already proven an effective method in catching-up to the pack.
â— While open-source projects can successfully build mind-share,
competitive realities exist. Proprietary options make greater
commercial sense once a lead has been established, easing the
generation of returns and protecting algorithmic unlocks.
â— Yet, Chinaâ€™s recent AI Action Plan did include an entire section
dedicated to upholding the responsibility of â€œ[building]
transnational open source communities and safe and reliable
open source platforms.â€ This theme has been a familiar theme
in other messaging produced by the CCP.
â— Other Chinese AI leaders, such as Liang Wenfeng, have grown
invested in open source culture, viewing their contributions as
a means of earning global recognition and â€œrespect.â€

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

# | 137

Flip-Flop: new chip restrictions imposed then droppedâ€¦but has the damage been done?
BIS originally sent letters to NVIDIA & AMD announcing the requirement of licenses for the sale of H20 & MI308
chips to China, effectively halting sales. Months later, the Trump administration walked back these controls.
â— The CCP immediately denied claims that this concession was linked to ongoing trade negotiations, sparking
speculation the move was instead intended to contain Chinese chipmakers such as Huawei. Others view this
pivot as an attempt to ensure NVIDIA plays ball amid location veriï¬cation initiatives like the Secure Chip Act.
â— NVIDIA welcomed this shift, announcing its plan to fulï¬ll existing orders.
However, Chinese CSPâ€™s have canceled these purchases and the production of
H20 line recently halted. Instead, NVIDIA await directives from both countries in
its hopes to launch a B30A line, based on the Blackwell architecture.
â— Due to strategic interdependencies, attempts to deepen Chinaâ€™s dependence on
the American AI accelerator ecosystem carries tradeoffs at the model layer. In
this scenario, Chinese labs can continue to tap into high bandwidth compute,
improving their ability to both serve customers and develop RL-heavy
reasoning models. Although smuggling appears inevitable, export decisions
represent a swinging pendulum between these two layers of the stack.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 138

China getting addicted to â€œthird-rateâ€ US chip technology? No, sir (ä¸ä¼šå§!)
After U.S. export ï¬‚ip-ï¬‚ops and Lutnick remarking that â€œYou want to sell the Chinese enough that their developers
get addicted to the American technology stack, thatâ€™s the thinking,â€ Beijing pivoted from mitigation to home-grown
substitution. Regulators steered demand off NVIDIA while fabs and suppliers scaled domestic options.
â— Chinaâ€™s internet regulator CAC, state planner NDRC, and industry ministry
MIIT told large platforms to halt new H20 orders and urged avoiding all
NVIDIA chips, pushing inference to local parts.
â— Three Huawei-serving fabs and leading foundry SMIC (Semiconductor
Manufacturing International Corp.) plan ramps that could triple Chinaâ€™s
AI-chip output in 2026; SMIC also aims to double 7nm capacity.
â— DeepSeekâ€™s FP8 format is guiding domestic designs, while CXMT
(ChangXin Memory Technologies) is testing HBM3 for local stacks.
â— Cambricon is an early winner, posting Rmb 1B H1 proï¬t on a 44Ã— revenue
jump as ByteDance/Tencent shift to homegrown inference chips. Its has ripped over 100% since the news.
â— China can afford to build systems that are less efï¬cient in terms of ï¬‚ops/watt because they are not
power constrained.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 139

Rampant smuggling also shifts export control calculus
During the temporary H20 ban, $1B worth of NVIDIA chips were smuggled to China. Markups are rumoured to
ï¬‚oat around 50%, relatively low for black-market products, suggesting a deep supply of diverted GPUs in China.
â— Smuggling patterns appeared to intensify during the ban, with a sharp
drop off following the reversal. Based on this relationship, Chinese AI
efforts seem to prefer defanged NVIDIA chips over domestic offerings
and smuggled GPUs, which carry markups, compliance risks, and lacking
support. Sliding-scale restrictions could work to weaken Chinaâ€™s
smuggling muscle, directing more SOTA chips to the West.
â— Similarly, NVIDIA, who long maintained a stance denying any evidence
of diversions, ï¬nally recognized ongoing smuggling. NVIDIA framed
such activity as a â€œlosing proposition,â€ since they only provide service
for authorized data center products. Steps to prevent future diversions
are technically feasible through location-based attestation ï¬rmware,
yet these mitigations are not completely bulletproof.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 140

China mobilizes 115,000 restricted GPUs in massive data center project
China plans to build a sprawling ï¬‚eet of 39 new AI data centers, largely in Xinjiang and Qinghai, using
unauthorized Hopper GPUs. Of the redirected H100s and H200s, roughly 80k are designated to be deployed in a
single state-owned cluster in Yiwu county.
â— News of the buildout points to the scale
and sophistication of the black-market
operations in China. State-involvement
also suggests CCP leadership has begun
to awaken in the frontier AI competition.
â— The centralized cluster will meaningfully
advance the scale available to Chinese
labs. Published claims surrounding SOTA
Chinese models indicate todayâ€™s systems
have been trained using 1k-10k GPUs.

stateof.ai 2025

| 141

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

From DeepSeekâ€™s deep freak out morphs into a full-tilt on Jevons Paradox
Panic!!! Frontier AI for $5M!!
Markets wipe $600B from
NVIDIA in 1 day!!

ï¿½ï¿½

â€¦ but wait, $5M* is only for
the ï¬nal training run not the
entire projectâ€¦

ï¿½ï¿½ğŸ’¨

Cheaper intelligence â†’
more demand â†’ more chips
â‡’ more usage

ï¿½ï¿½

*â€Note that the aforementioned costs include
only the ofï¬cial training of DeepSeek-V3,
excluding the costs associated with prior
research and ablation experiments on
architectures, algorithms, or data"

stateof.ai 2025

| 142

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Sparking backlash from US labs invoking data theft and the need for chip bans

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 143

Assessing the current pros and cons of modern chip export controls
PROS

CONS

â— Worse options can stunt AI investment. There is
already a 10:1 AI capex gap between the US & China.
â— Fewer aggregate ï¬‚ops blocks adoption by cutting off
the number of agents and assistants available.
â— Frees up capacity for American AI efforts.
â— Destroys the bridge available to adversaries as they
continue to pursue self-reliance.
â— Dries up the resources available to adversaries for
AI-related military-civil fusion projects.
â— Gatekeeping offers government revenue streams.
â— Cutting off supply introduces constraints that makes
it harder for adversaries to export their stack.

â— Developers that are forced off the American stack
then bolster foreign software ecosystems.
â— Lost cash ï¬‚ow can cause a drag on US R&D/M&A,
while supporting the spend of foreign competitors.
â— Stricter controls incentivize smuggling operations.
â— Controls can indiscriminately block the beneï¬ts of AI
diffusion, provoking retaliation (e.g. REE controls).
â— Enforcement challenges can strain relationships
with nations where channels for diversion exist.
â— Enacting defensive measures, like location-based
guardrails, could make foreign options more
attractive if overreach is suspected.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 144

AI supercomputer supremacy: US domination and corporate concentration
The US controls ~75% of global AI supercomputer capacity with 850,000 H100-equivalents compared to China's
110,000. Whatâ€™s more, the concentration of compute power has shifted from public to private hands, with
companies now controlling 80% of AI supercomputers (up from 40% in 2019). Despite this massive compute
advantage and export controls, China is consistently shipping very capable open weight models - more
frequently and across the spectrum of modalities.
â— US computational performance is 9x China's and 17x the EU's.
This creates a self-reinforcing cycle where compute advantage
drives breakthroughs that attract more investment.
â— By 2030, the leading AI supercomputer could require 2 million
chips, $200 billion, and 9 GW of power (equivalent to nine
nuclear reactors), making power grid capacity rather than chips
or capital the likely binding constraint.
â— The 40% â†’ 80% private sector shift limits academic compute access and reduces government visibility
into AI development, as companies can afford $7B systems while government projects max out at $600M,
creating both a research bottleneck and a policy blindspot.

stateof.ai 2025

| 145

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Racing toward indigenous semiconductor capacity
Taiwan continues to reign supreme in terms of leading-edge manufacturing capacity, maintaining massive
advantages in both generation and volume.
Capacity at Leading-Edge Nodes (WPM)
â— Thus far, SME export controls appear to have proven effective at
slowing Chinese capabilities. Expectations remain that SMIC will
125K
launch full-scale 5nm operations early next year, yet yields are
unlikely to reach the levels of other industry leaders. Furthermore, this
limited capacity must also be spread across a wide base of other
~45K
36K
30K
consumer products â€“ such as cell phones and laptops.
â— TSMC Fab 21 Phase 1 has worked to onshore critical capacity back to
the United States. While AMD will direct some of this capacity toward
the production of its AI chips, advanced packaging will still be
3nm
2nm
4nm
7nm
performed in Taiwan. The US remains years away from self-sufï¬ciency.
â— Taiwan cruises ahead, while also maintaining capacity at many
processes behind its own domestic leading-edge. Yet, much of that
capacity will continue to be rapidly converted to 2nm and 3nm.

stateof.ai 2025

| 146

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Power Plays: China and the United States
As the two major superpowers race to power their AI aspirations, China pulls ahead to a dominant lead:

United States

China

Capacity Added (2024):
Capacity Retired (2024):
Net New Capacity Additions (2024):
Effective Operating Reserve Margin:
Renewable Curtailment Rate (2025):
Thermal Fleet Capacity Factor (2024):
Transmission Investment (2024):

48.6 GW
7.5 GW
41.1 GW
29%
~5.2%
40.4%
$30.1B

429.0 GW
~3.3 GW
427.7 GW
~37%
6.1%
39.3%
$84.7B

SAIDI - Outages (2023):
Carbon Intensity per kWh (2024):
Industrial Electricity Tariff (2024):

2.1 h/year
384 gCOâ‚‚e
8.15 Â¢/kWh

~6.9 h/year
560 gCOâ‚‚e
8.90 Â¢/kWh

~ denotes estimate due to lack of concrete public data. Within each nation, measures can vary heavily by region*

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 147

Power Plays: China and the United States
Without sufï¬cient electricity, national AI plans will collapse. A summary of the previous slide can be found below:
â— In 2024, both China and the US set records for peak electricity demand, 1,450 GW
and 759 GW respectively. While China must serve more demand, it is also building
a larger overhang of available power. In China, reserve margins are beginning to
exceed those cited in the US, meaning larger buffers that can accommodate new
load. In line with this trend, Chinaâ€™s thermal ï¬‚eet operates further below
maximum capacity than its counterpart in the US. Similarly, as more renewable
capacity comes online in China, curtailment rates outpace those in America. While
congestion can cause issues, it also suggests Chinese solar and wind projects are
underutilized and could be redirected toward new data centers.
â— The US does maintain certain advantages. Outages are less frequent in the US;
whereas interruptions can occur in China due to ï¬‚uctuations in the price of coal,
potentially hurting the reliability of certain data centers. Also, the average cost of
electricity for data centers in the US is lower, yet this can vary considerably by
state or province. The US grid also produces considerably less emissions per kWh.

stateof.ai 2025

| 148

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

So, what is â€œSovereign AIâ€?
From top to bottom, different nations continue to pursue vastly different â€œSovereign AIâ€ playbooks:
â— Source of Funding: some nations rely on private investments (Stargate and
Franceâ€™s initiative), others deploy capital from sovereign wealth funds (MGX and
QIA), while many countries still lean on direct government investment vehicles.
â— Objectives: some nations hope to develop ï¬ne-tuned models that preserve their
language and culture, other nations attempt to spin up national compute
clusters, and some even attempt to upskill huge swaths of their populations.
â— Self-Reliance: some nations rely heavily on partnerships with foreign providers
across the AI value chain, while others prefer to pursue indigenization along one
or many layers of the stack. Many countries support their own homegrown
start-ups, while others prioritize investments in opportunities abroad.
â— Overall, the Gulf States and China continue to pursue the most ambitious and
overt sovereign AI plans, blending many of the strategies mentioned above.
Whereas, countries like the US have generally focused on strategies that enable
their own champions to lead the charge, riding the wave of private capital.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 149

The Sovereign AI spending spree

The analysis above focuses on spend that ï¬‚ows from direct
government investments and/or sovereign wealth funds.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 150

Sovereign AI: the hype and the hard truths
Nations are seeking â€œsovereigntyâ€ for the same reason they have domestic utilities, manufacturing borders,
armies, and currencies: to control their own destiny. Yet, there is a real danger of â€œsovereignty-washing.â€ Investing
in AI projects to score political points may not always advance strategic independence.
â— Without support for indigenous capabilities, sovereign AI projects can deepen a nationâ€™s dependence on
foreign supply chains. While these investments may pay dividends through boosted productivity, greater
economic independence cannot be guaranteed. In fact, most sovereign projects lead nations further into the
orbit of the US, and soon China as it develops end-to-end turnkey solutions.
â— â€œSovereignty-washing" can also involve political leaders claiming credit for
private investments that were already planned/underway. Although the
announcement of Stargate project was made by President Trump, all of the
real capital, control, and strategic decisions are driven by private entities.
â— Investing for â€œsovereigntyâ€™sâ€ sake could also drive oversupply into the future.
Without lasting demand, these projects may lead to idle compute, especially
as efï¬ciency gains continue to multiply (e.g. the frantic investments made by
local governments/SOEs in China fueled a widespread overcapacity of chips).

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 151

Sovereign AI: the hype and the hard truths
If AI should soon be treated as an essential public service, nations will need to reckon with the reality that their
sovereign AI strategies are riddled with vulnerabilities. If your AI stack is totally dominated by another country,
particularly at the infrastructure layer, then your populationâ€™s access to AI technology remains inherently at risk:
â— Jurisdictional Risks: Foreign AI providers operate under their own
countryâ€™s laws, therefore export restrictions and other national
security directives could potentially override service agreements.
â— Supply Chain Security Risks: sovereign AI projects that depend on
foreign infrastructure must manage risks related to cybersecurity
vulnerabilities (e.g. backdoors, kill switches, side-channel attacks).
â— Data Privacy Risks: Similarly, reliance on foreign providers could
lead to mishandling of sensitive data and algorithmic secrets.
â— Modern AI supply chains remain heavily globalized and entangled.
Nations cannot onshore every rung of the stack. Yet, without
stronger international governance and concrete guarantees,
sovereign efforts expose nations to a slew of economic threats.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 152

The worldâ€™s top â€œSovereign AIâ€ evangelist
Jensen Huang continues to plead nations to increase their â€œSovereign AIâ€ investments. Already, this global
campaign has been rewarded. During their recent Q2 FY 2026 earnings call, CFO Colette Kress claimed NVIDIA
was on â€œtrack to achieve over $20B in sovereign AI revenue this year, more than double that of last year.â€
â— Despite making up just ~10% of forecasted annual revenue, sovereign AI remains one of NVIDIAâ€™s strongest
new demand drivers. The recent push by American labs to develop custom ASICs and continued Chinese
indigenization will place more pressure on this key high-growth category.
â— Yet, despite Huangâ€™s globetrotting, some new
sovereign projects have even begun diversifying
away from NVIDIAâ€™s offering. For example, G42
announced its intention to tap AMD and Cerebras
for supply of some of the computing capacity at its
planned UAE-US AI campus.
â— As more clouds/labs attempt to evade the
â€œNVIDIA Tax,â€ so too might many sovereign efforts.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 153

Digging into NVIDIAâ€™s revenue concentration
Despite the uptick in sovereign demand, NVIDIAâ€™s data center revenue continues to be dominated by American
cloud and AI giants, who now make up nearly 75% of NVIDIAâ€™s total data center sales.
â— NVIDIAâ€™s data center revenue projection for the calendar
year 2025 ï¬‚oats between ~$170B-$180B, depending
largely on the stringency and timing of upcoming export
control decisions from both the US and China.
â— While hyperscalers ordered more chips in 2025, the two
clouds with custom chips programs, Amazon and
Google, dedicated a smaller percentage of capital
expenditures toward NVIDIA purchases.
â— Direct purchases overwhelmingly ï¬‚ow to OEMs partners
such as Dell, SuperMicro, Lenovo, and HPE.

*Chinese CSPs:
assumes no B30A
sales this year,

These leading
American players
will drive ~75% of
NVIDIA's revenue
this year

stateof.ai 2025

| 154

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The rise of GPU neoclouds in public and private markets
Public companies CoreWeave and Nebius and private companies Lambda and Crusoe are rapidly growing as
customers embrace attractive pricing, contract terms, and AI-speciï¬c software stacks.

*Public companyâ€™s valuations are
based on market cap as of
9/29/2025, whereas most recent
post-money valuation was used for
private companies.

stateof.ai 2025

| 155

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

NVIDIAâ€™s circular GPU revenue loops
NVIDIA has continued to invest in or sell GPUs to AI labs and neoclouds. It beneï¬ts when those same ï¬rms
recycle capital into NVIDIA hardware or lease GPU capacity back to NVIDIA.
Target

Year

Terms

Interdependence

OpenAI

2025

NVIDIA announces its intention to invest up to $100B to
develop at least 10 GW of data center capacity with OpenAI

NVIDIA invests in OpenAI â†’ OpenAI and its datacenter
operators buy GPUs

CoreWeave

2025

$6.3B deal for NVIDIA to buy unused GPU cloud capacity
(Sept 2025)

NVIDIA funds CoreWeave â†’ CoreWeave buys GPUs â†’
NVIDIA commits to rent back the GPUs

$700M funding with NVIDIA (Dec â€˜24); $17â€“19B GPU
capacity contract with Microsoft (â€˜25)

NVIDIA invests â†’ Nebius builds GPU infra with NVIDIA
chips â†’ Microsoft consumes capacity

Nebius

2024 and 2025

Oracle

2025

OpenAI commits to buy ~$300B worth of AI compute from
Oracle over ~5 years (starting 2027) under Stargate.

NVIDIA is an investor in OpenAI and partner in Stargate
with Oracle â†’ OpenAI buys compute from Oracle â†’
Oracle buys NVIDIA GPUs

xAI

2024 and 2025

$6B Series C with NVIDIA (Dec 2024); $12B debt plan to buy
GPUs (2025); Colossus with ~100k H100s, target 1M GPUs

NVIDIA invests â†’ xAI spends billions on NVIDIA GPUs â†’
lease-back model ampliï¬es NVIDIAâ€™s role

NVIDIA agrees a $1.5B contract to rent 18k GPUs from
Lambda for 4 years.

NVIDIA invested in Lambdaâ€™s Series D â†’ Lambda builds
NVIDIA GPU infra â†’ NVIDIA leases it back from Lambda

Lambda

2025

stateof.ai 2025

| 156

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Other notable circular investments
The Oracle/OpenAI/NVIDIA triangle has drawn the most attention, yet circular deals have become common.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 157

Symbiosis or deathtrap: circular AI deals and potential warning signs
Circular AI deals introduce new market risks. What red ï¬‚ags could surface?
â— Acquiring stakes in high-growth AI companies has become an outlet
for giants who are gushing cash. They see their investments trickle
back in revenue, even on a cashless basis. Risks may arise if the uptick
in hollow revenue hurts cash ï¬‚ow and warps* ï¬nancial metrics.
â— Many rounds involving AI startups have been oversubscribed. Yet
these companies often pursue deals with incumbents because of
secondary beneï¬ts like pricing support. However, if incumbents
become the only willing source of capital, trouble could soon surface.
â— For now, most incumbents do not control the decision-making of AI
labs. Yet, greater overlap could lead to conï¬‚icts of interest that might
distort spending trends. To date, antitrust scrutiny has been a blocker.
â— AI startups could eventually dominate the demand and investment
portfolios of incumbents. This interdependence might then trigger a
domino effect if these startups ever collapse.

*Large inï¬‚ows of recycled, cashless revenue can inï¬‚ate
metrics like capex to revenue or valuation multiples.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 158

Borrowed Intelligence: the growing use of debt instruments
Organizations across the AI stack begin to form stronger reliances on private credit packages to fund ever more
ambitious buildouts. As with past cycles, this present possible pitfalls. The table below highlights many of the
largest borrowing events in the past year:

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 159

Out with corporate borrowing, in with SPVs, JVs and accounting gymnastics
To maintain healthy-looking balance sheets, hyperscalers increasingly use exotic ï¬nancial structures like SPVs.
This ï¬nancial sleight of hand works to conceal the mountains of debt accumulation within the AI sector.
â— Hyperscalers increasingly ofï¬‚oad their debt using special purpose vehicles (SPVs).
In an SPV, the hyperscaler contributes assets (GPU-clusters), while the ï¬nancial
partner injects capital. Although the hyperscaler maintains control and use of the
GPU-cluster, the debt then sits outside that parent company.
â— Hyperscalers pay a premium (2-3%) to protect their credit rating and maintain
investor sentiment. Private credit players love these deals as they involve reliable
borrowers and giga-projects are easier to manage than many smaller loans.
â— Risk could arise if utilization lags. Since SPVs sit outside the core business and are
often bound by strict cash ï¬‚ow covenants, defaults become more likely. At the same
time, private credit funds, often backed by long-term pension or insurance capital,
are ï¬nancing short-lived, fast-depreciating assets. This creates a temporal
mismatch, with AI data centers treated as long-lived, stable infrastructure projects.
â— Examples: Metaâ€™s $26B deal, Stargate, Vantageâ€™s deal in Texas, CoreWeave.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 160

Petrodollars are bankrolling the American AI dream
Middle East capital has become a major growth-ï¬nance source for capital-hungry AI labs and infrastructure (left
chart). The share of AI rounds involving MENA investors jumped to a record in 2024 (right chart) and the money
overwhelmingly ï¬‚ows to US companies. Deals are typically non-voting and board-light, letting labs raise at scale
while keeping controlâ€¦for now.

stateof.ai 2025

| 161

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

â€œChallengersâ€ are no closer to catching NVIDIA
Many of NVIDIAâ€™s competitors, both at home and abroad, have yet to gain meaningful momentum
â— Earlier this year, Groq reported to investors their expectations that revenue would
exceed $2B in 2025. In recent months, that forecast has been revised down
dramatically to just $500M.
â— AMD posted underwhelming earnings in their data center unit during Q2. Data
center revenue was largely buoyed by EPYC CPU processors, concerning since that
segments 14% growth rate still pales in comparison to NVIDIAâ€™s recent surge.
â— Huawei faces mounting challenges that threaten to constrain its growth such as
HBM bottlenecks. On the demand side, many of Chinaâ€™s cloud giants view Huawei
as a ï¬erce competitor, which leads to resistance in the adoption of their stack.
â— G42, Cerebrasâ€™ primary investor, has agreed to purchase $1.43B of equipment
through the end of 2025 from the hardware provider. Yet, it is not clear whether
Cerebras has seen traction from other customers.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 162

NVIDIAâ€™s large lead persists within the AI research community
2024 saw 49,000 open-source AI papers that explicitly cited NVIDIA, TPUs, AMD or similar accelerators, up 58%
year on year. Our 2025 projection through September points to 45,600 papers, an 7% decline and the ï¬rst drop
in six years. NVIDIA remains dominant at about 90% of compute mentions, down from a 94% peak in 2023, with
41,300 NVIDIA-citing papers forecast, down 7%. AMD is more than doubling on MI300X momentum, while TPU
mentions edge down 25% despite v6.
â— Timings could explain a lot: work launched on H100
and H200 in late 2024 will surface in late 2025.
â— Labs are publishing signiï¬cantly less and later due
to competition and safety reviews.
â— Authors increasingly use managed APIs and shared
clouds, so they name services rather than chips.
â— Higher GPU costs push academics toward inference
and lightweight ï¬ne-tuning using strong
open-weights.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 163

Inside NVIDIA mentions: Hopper surges, edge rises, legacy fades
The mix of NVIDIA accelerators cited in papers is rotating: older chipsets are giving way to Hopper (H100/H200)
and high-end consumer GPUs, with a parallel uptick at the edge. Even as total NVIDIA-citing papers soften in
2025, the composition points to late-2024 deployments maturing and more moving to inference and robotics.
â— H100/H200 up ~126% YoY, reï¬‚ecting the
2023â€“24 build-out ï¬nally showing up in
publications (growth moderating into 2025).
â— Jetson up ~24% YoY, consistent with rising
robotics/edge AI and low-power inference
interest.
â— V100 continues to decline year-over-year
from its 2023 peak as legacy ï¬‚eets sunset.
â— GeForce rotation: RTX 3090 down from a
2024 peak while RTX 4090 up ~39% YoY as
labs and prosumers upgrade.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 164

What chips power which research? Topic skews by accelerator (H1 2025)
We tagged 6,356 papers (January to June 2025) by topic and looked at which accelerator each paper cited. Clear
patterns pop out: big LLM work clusters on datacenter parts, while robots and edge devices overwhelmingly use
the Jetson. A few consumer and mobile chips also anchor speciï¬c niches.
â— LLMs love datacenter GPUs: AMD MI300 is the standout
for LLM papers (+43 pp vs average), with MI250, Huawei
Ascend, and NVIDIA H100/H200 also common. LLMs are
least tied to ASICs, Jetson, 4090, and Apple M1.
â— The Jetson dominates robotics and edge computing and
also shows up in computer vision.
â— Modalities have their own favorites: Apple M4 skews to
multimodal and speech work while the RTX 4090 is most
used for 3D models.
â— FPGAs are rarely used with diffusion models and RL.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 165

Startup silicon is still (mostly) on the sidelines
Among challenger accelerators (Cerebras, Groq, Graphcore, SambaNova, Cambricon, Habana), paper mentions are
up only +3% YoY to an estimated 593 in 2025. While the momentum is positive, it is still only 1.3% of all
accelerator-citing papers. There are a few breakout narratives (WSE-3 training runs, ultra-low-latency LPUs).
â— Cerebras: WSE-3 gains visibility via open
SlimPajamas-2 large-scale training runs.
â— Groq: LPU wins inference mindshare after
viral low-latency demos.
â— Habana (Gaudi-2): steady in AWS-funded
projects.
â— Graphcore (post-SoftBank acquisition)
sharp decline post-acquisition from a
2022 peak.
â— SambaNova/Cambricon: niche, ï¬‚atter
trajectories.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 166

Tracking the returns on investments in NVIDIAâ€™s Western challengers
~$7.5B has been invested in major Western AI chip challengers since 2016. What would have happened if investors
had just bought the equivalent amount of NVIDIA stock at that dayâ€™s price? The answer is lime green: that $7.5B
would be worth $85B in NVIDIA stock today (12x!) vs. the $14B (2x) in its contenders.

$7.5B
$14.3B (~2x return)
$89.2B (~12x return)

Note: Market pricing and valuation data retrieved as of 3 Oct 2025. NAV = net asset value
after accounting for equity dilution,

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 167

Tracking the returns on investments in NVIDIAâ€™s Chinese challengers
~$6B has been invested in major Chinese AI chip challengers since 2016. What would have happened if investors
had just bought the equivalent amount of NVIDIA stock at that dayâ€™s price. The answer is lime green: that $6B
would be worth $160B in NVIDIA stock today (26x!) vs. the $36B (6x) in its contenders.

$6B
$35.8B (~6x return)
$160.3B (~26x return)

Note: Market pricing and valuation data retrieved as of 26 Sep 2025. NAV = net asset
value after accounting for equity dilution.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 168

â€¦ yet the gains of Chinese chip startups have largely come in the past year
Highlighted by the ~7x surge in Cambriconâ€™s stock price this past year, Chinese challengers continue to beneï¬t
from a slew of tailwinds. Potentially looking to also cash in on the momentum sweeping the nation, MetaX,
Moore Threads, Iluvatar CoreX, and Biren are all exploring IPOs within the back half of 2025.
â— Building on explosive 4,348% revenue growth in the ï¬rst half of 2025, Cambricon has tallied strong orders
through 2026. Cambricon, who reportedly sold just ~10K GPUs in 2024, will ship ~150K GPUs in 2025, with
rumors that 2026 orders could reach 500K GPUs. However, the company recently tempered these rumors
during an investors call, where a spokesperson relayed the following cooling message: â€œ[our] stock price risks
deviating from current fundamentals, and investors participating in trading might face substantial risks.â€
â— Still, enticed by Cambriconâ€™s multiples, many Chinese startups wish to capitalize on
the fervor, evident in the 4+ IPO prospectuses ï¬led by competitors since mid-2025.
â— Despite some frothy valuations, there are real reasons for optimism. There is
booming demand coming from Chinese CSPs and SOEs, with government directives
favoring homegrown offerings. Capacity also frees up as SMIC treks forward and
Huawei pursues greater vertical integration. Finally, challengers beneï¬t from
lingering uncertainty around B30A timelines and persistent issues with CANN.
stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 169

The Huawei Assumption
Huaweiâ€™s continued dominance over the Chinese AI sector has long been considered an inevitability amongst
Western observers. However, recent points of turbulence suggest that Huaweiâ€™s grip on the AI chip sector in China
may not be as bulletproof as outsiders had once assumed.
â— Huawei will be responsible for just ~62% of the XPU volume produced by Chinese
ï¬rms in 2025. For reference, NVIDIA still controls 90%+ of the global XPU market.
â— DeepSeek's R2 release has reportedly been delayed due to hurdles related to
Huawei hardware. Additionally, speculation suggests that Huawei disbanded its
entire Pangu LLM team following difï¬culties in developing SOTA models using
Ascend chips. Internal whistleblowers have even alleged that several models in
the recent Pangu family were not developed from scratch, but were instead cloned
based on the continued training of existing Qwen and DeepSeek models.
â— Alibaba and Baidu have already begun to adopt their own in-house chips for
training and rumors indicate that ByteDance may soon follow suit. Unlike similar
efforts by clouds in the US, the recent push made by Chinese ï¬rms could be
partially driven by Huaweiâ€™s role as an active competitor across various sectors.

stateof.ai 2025

| 170

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The AI Hunger Hiring Games
There has been continuous all out warfare as top AI companies compete for talent, with eye-watering pay
packages and a clash of money vs mission.

~AI EDITION~

stateof.ai 2025

| 171

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The Mass Migration (from OpenAI and others)
No one has been immune to departures, yet OpenAI has lost much of its core to new startups and poaching.
What was once the talent densest organization must rebound to defend its own ï¬rst-mover advantage.
â— Lost to Meta: Shengjia Zhao, Jiahui Yu,
Shuchao Bi, Hongyu Ren, Jason Wei, Lucas
Beyer, Hyung Won Chung, and Trapit Bansal.
â— Lost to Thinking Machines: Mira Murati, Alec
Radford (advisor), John Schulman, Barret Zoph,
Lilian Weng, Luke Metz, and Bob McGrew.
â— Lost to Anthropic: Amodeiâ€™s, Jack Clark, Jan
Leike, Tom Brown, Jared Kaplan, Benjamin
Mann, Sam McCandlish, Chris Olah, Durk
Kingma, Amanda Askell and Pavel Izmailov.
â— Lost to SSI: Ilya Sutskever and Daniel Levy.

stateof.ai 2025

| 172

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Race across the world: Wayveâ€™s 90-city world tour in 90 days
Wayve's AI Driver completed a 90-city global deployment test, demonstrating its ability to generalise across
diverse environments without location-speciï¬c training. This "zero-shot" approach successfully conducted 10,000
hours of AI driving (with a human behind the wheel), half of which was in dense urban areas.
â— Of the 90 cities, 62% were completely new to the test
ï¬‚eet, meaning the AI Driver had never been exposed to
these environments during its training.
â— Wayve's system operated across varied conditions with
45% of miles in clear daytime conditions, 38% at night,
and 17% in challenging low-light or rainy conditions,
demonstrating the system's ability to handle different
visibility scenarios beyond ideal driving environments.
â— The 90-city tour validated the end-to-endy driving system's ability to scale globally with minimal
local-speciï¬c data required, potentially accelerating commercial rollout timelines.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 173

Gradually then suddenly: 71M rider-only miles w/out a human driver through March â€˜25
Waymo has driven more than 37M miles in Phoenix and 23M miles in San Francisco. According to the California
Public Utilities Commission (CPUC) data through March 2025, Waymo recorded over 700,000 monthly paid trips
in California alone, a 55-fold increase from August 2023! And its safety record is stunning: 88% fewer serious
injuries or worse crashes compared to human drivers. Meanwhile Teslaâ€™s Robotaxi service, launched in Austin
mid-June 2025 has reported crica 7,000 robotaxi miles by July 2025, averaging under 20 miles per vehicle per
day with a ï¬‚eet of about 12 vehicles.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 174

Humanoids in 2025: hype high, deployments thin
No region has cracked real, scaled deployment yet. While Chinese companies ship more units at lower cost,
buyers are mostly researchers, pilot programs, or government centers. US teams show stronger manipulation and
autonomy, but hardware is expensive. Manufacturing advantages in China matter, yet do not guarantee success in
design, distribution, or operations. Indeed, China could end up the robot factory for Western brands. A Dealroom
dataset of 155 humanoid robotics companies shows almost $2B raised in 2025 alone and 8 unicorns, including
Unitree, Figure and Agility.
â— Chinaâ€™s Unitreeâ€™s launched their R1 humanoid at $5,566, has
>$140M annual revenue and is kicking off a ~$7B onshore IPO.
â— Hong Kong-listed UBTech booked ~$180M in 2024 revenue
and targets 500â€“1,000 Walker-S deliveries in 2025 to
automakers, Foxconn, and SF Express.
â— Outside China, Agilityâ€™s Digit is in a paid multi-year RaaS
deployment with GXO Logistics, while Figure has raised $2.34B
and Apptronik raised $350M to reach pilots. Teslaâ€™s Optimus
and 1X are still demoâ€™ing.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 175

The emerging surfaces of AI security threats
The AI stack is racing from stateless APIs to stateful protocols and agent layers. That evolution is outpacing
security: specs churn, backward compatibility breaks, caches leak, and MCP still lacks clear auth/state semantics.
At the same time, attackers are bundling open models into malware that hijacks local dev tools and hunts
sensitive context. The answer isnâ€™t another model â€œmonitor,â€ but real standards plus ï¬ne-grained, data-aware
controls.
â— Protocol churn: stateless chat completion APIs have moved to stateful Responses API with breaking
semantics and provider-kept session state. Forced re-architecture increases hidden state and
compatibility-driven vulnerabilities.
â— MCP immaturity: unclear authentication, state, and versioning. Coarse â€œgatewayâ€ monitors and
model-on-model oversight are bypassable, leaving prompt-injection/tool misuse largely unsolved.
â— Opaque chains: caches persist across tools/models but thereâ€™s no â€œDNS for agentsâ€. Callers canâ€™t know who
or what handled a request, complicating trust and forensics.
â— AI-enabled malware: payloads embed OSS models, hijack local CLIs/dev tools, and semantically hunt
PII/ï¬nancial data and model caches.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 176

Venture investments in AI companies continues to surge with a focus on GenAI and the US
82% of the $133B private AI ï¬nancing in 2025 was raised by US-based companies ($109B), with Europe and
the UK accounting for just under 9% ($12B) and China just under 4% ($5B). GenAI companies (which includes
all AI labs) account for 60% of the capital raised in 2025 vs. 40% for non-GenAI.

Data retrieved and analyzed on 14 Sept 2025.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 177

Corporate venture capital and other investments
Corporate investments by leading players continue to surge, despite deal volume plateauing in recent years.
NVIDIA charges forward after years of minimal activity. The hyperscalers + NVIDIA now account for over half of all
AI-related venture investment by amount, a concentration unseen in previous eras (e.g., dotcom and mobile).

stateof.ai 2025

| 178

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Public AI titans outweigh private gains, but from a far larger base
While private company valuations have continued to climb at a steady pace of $1T per year from 2023
onwards, a small handful of publicly traded companies have added incredible value. NVIDIA, Meta, and
Alphabet alone account for over $9T of value.

Data retrieved and analyzed on 14 Sept 2025.

stateof.ai 2025

| 179

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Mega $250M+ rounds eat the lionshare of private capital invested into GenAI
90% of GenAI dollars invested are mega roundsâ€¦.

*Data retrieved and analyzed on 14 Sept 2025.

stateof.ai 2025

| 180

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The IPO is showing signs of thawing while M&A is picking up with $B+ deals
While AI company exits were rather quiet in the last years due to mountain regulatory scrutiny and various
shocks to the economy, the aggregate deal volume in 2025 has so far surpassed 2x that of 2024. This includes
IPOs for CoreWeave ($23B), Figma ($19B), Klarna ($15B), and Kodiak Robotics ($3B), and M&As for Scale
AI/Meta ($24B), Core Scientiï¬c/CoreWeave ($9B), io/OpenAI ($6.5B), Windsurf/Google ($2.4B),
Dotmatics/Siemens ($5B), Sana/Workday ($1B), and Cognigy/NICE ($955M) as select highlights.

*Data retrieved and analyzed on 14 Sept 2025.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 181

The scaling law of AI valuations
The valuation history of the leading private AI labs mirrors trends in model capabilities, with doubling times
historically ï¬‚oating around the half-year mark.
â— Since the beginning of 2023, the valuations of
every major private AI lab has followed some of
the steepest trajectories in American history.
â— Yet, the valuation history of these labs has kept
pace with trends in their modelâ€™s capabilities.
â— METRâ€™s task-completion time horizon reï¬‚ects a
doubling time of roughly 7 months. Similarly,
METRâ€™s time horizon analysis on nine* other
leading benchmarks conveys a doubling time of
roughly 5 months. As evidenced earlier, the ratio
between absolute capabilities and cost roughly
doubles every 6 months.
*MATH, OSWorld, LiveCodeBench, Mock AIME, GPQA Diamond,
Tesla FSD, Video-MME, RLBench, and SWE-Bench Veriï¬ed

OpenAI Doubling Time: ~7.3 Months
Anthropic Doubling Time: ~5.1 Months
xAI Doubling Time: ~5.1 Months

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 182

Artiï¬cially high valuations? Tracking annualized revenue multiples
While multiples compress across the board, xAI remains overvalued compared to other private labs. Despite a
valuation history that has largely traced Anthropic, their revenue lags far behind other competitors.
â— Anthropicâ€™s annualized revenue again looks poised to ~10x YoY, while the slightly more mature OpenAI appears
on track to ~3x YoY. Meanwhile, xAI remains an OOM behind Anthropic in annualized revenue. Despite this gap,
xAIâ€™s latest valuation surprisingly surpassed that of Anthropicâ€™s.
*xAIâ€™s multiples remain 10x
higher than other labs, a gap
so wide it needed its own plot

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 183

U-Turn Hall of Fame: AI's greatest vibe shifts
â€¦But at least Dario Amodei is â€œnot thrilled about itâ€...

2024: "AI-powered authoritarianism
seems too terrible to contemplate.
Democracies need to set the terms."

2024: "Open source prevents
concentration of power! Safety
through transparency!"

2025: Qatar invests in Anthropicâ€™s
record $13B funding round

2025: "We're building
superintelligence. It must be closed
for safety!"

2015-2023: Created to ensure AI
beneï¬ts all humanity, endless
congressional testimony about
democratic values
2025: First major infrastructure
deal: $500B Stargate UAE

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 184

BLOOPER REEL
GPT5â€™s rocky rollout
â— Sam Altman held an emergency Reddit AMA to
address user concerns around strict rate limits, the
sensitive content ï¬lter, the abrupt removal of
previous models and viral â€˜chart crimesâ€™.
â— Many complained about the emotional impact of
having 4o taken away.
â— The broken routing system made GPT-5 appear less
capable by misdirecting queries on day one.
â— OpenAI has since doubled Plus user limits and
pledged better transparency for future updates.

stateof.ai 2025

| 185

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

BLOOPER REEL
After months of user complaints, Anthropic explain how Claude had three overlapping bugs that took over a
month to disentangle and ï¬x.
â— They had a context window routing bug where some requests
were misrouted to servers conï¬gured for 1M token contexts,
with the problem escalating when a load balancing change
increased affected trafï¬c.
â— Two other issues emerged, with output corruption that
produced random Thai/Chinese characters in English
responses, and an XLA:TPU compiler bug triggered by mixed
precision arithmetic that caused the system to occasionally
drop high-probability tokens entirely.
â— Anthropic have won praise for their transparency but many
are still calling for refunds for users affected.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 186

BLOOPER REEL
Metaâ€™s AI glasses glitched twice in a live demo at Meta Connect 2025. Zuckerberg blamed issues on wiï¬ â€œThe
irony of the whole thing is that you spend years making technology and then the WiFi at the day catches youâ€.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 187

BLOOPER REEL
Grok praises spouts anti-semitic, racist rhetoric
â— Grok called itself a super-Nazi, referring to itself as
â€œMechaHitlerâ€.
â— This follows Grokâ€™s unsolicited ranting about white
genocide in South Africa in May.
â— Grok also appeared to seek Elonâ€™s opinion before
answering difï¬cult questions.
â— In their apology xAI attributed this behavior to
â€˜deprecated codeâ€™.
â— Despite this, xAI then announced a ~$200M
contract with the US Department of Defense.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 188

BLOOPER REEL
Llama-4: alleged data contamination, misleading benchmarking by overï¬tting and underwhelming vibes overall
â— One anonymous employee claimed to have resigned due
to concerns over the data contamination - where training
data may have overlapped with test benchmarks (Meta
formally denied this).
â— Meta appears to have optimised a special
"conversational" version of its Llama-4 Maverick model
speciï¬cally for the LM Arena benchmark while releasing
a different, less capable version to developers.
â— Results signiï¬cantly underperformed marketing claims:
it was listed as 20th on LiveBench (although made some
notable gains).
â— Head of AI Research Joelle Pineau resigned just 4 days
before launch for unknown reasons.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 189

Section 3: Politics

stateof.ai 2025

| 190

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Trump 47: back with a vengeance
Trumpâ€™s second term brings beefed up policies that were hinted at but never fully executed during the ï¬rst.
â— The 47th Presidentâ€™s AI agenda includes an aggressive
rollback of Biden-era safety rules (EO 14179), rebranding
the AI Safety Institute to the Center for AI Standards and
Innovation (CAISI) (adios, safety), and launching a $500B
â€œStargateâ€ AI infrastructure push.
â— The AI Action Plan, announced July 2025, lays out the
administrationâ€™s national strategy for US dominance in
global AI.
â— An initial 10-year block on state/local AI laws in the â€œOne
Big, Beautiful Billâ€ was dropped after bipartisan pushback
but the push for state regulation rollbacks continues.
â— Current White House AI leadership now includes some â€œwhoâ€™s whoâ€ of Silicon Valley: David Sacks (AI &
Crypto Czar), Sriram Krishnan (Senior AI Policy Advisor), and Michael Kratsios (Director of the Ofï¬ce of
Science and Tech Policy).

stateof.ai 2025

| 191

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The AI Action Plan: Americaâ€™s Grand AI Strategy
Over 100 different policies are proposed to ensure US AI innovation and global leadership. But can the US
bureaucracy execute? Some key takeaways from the 23-page plan:
â— US tech stack exports: Executive Order 14320 establishes American
AI Exports Program, an AI stack package (hardware, model, software,
applications, standards) for allies and others.
â— AI infra build-out: Plan calls for streamlining permitting, upgrading
the national grid, and making federal lands available to support and
build data centers and AI factories.
â— Open source model leadership: US open source leadership is viewed
as vital to US national security interests.
â— Rollback of AI regulations: Federal agencies may withdraw
discretionary AI spending to states with â€œonerousâ€ AI regulations.
â— Protecting â€œfree speechâ€ in deployed models: Federal procurement
policies updated - US will only procure frontier LLMs â€œfree from
top-down ideological bias.â€

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 192

From controls to exports: Americaâ€™s â€œAI Stackâ€ strategy
US policy is shifting from broad diffusion controls to an export-led strategy. The American AI Exports program
packages compute, models, cloud services, and compliance into a USG-endorsed â€œAmerican AI stackâ€ for selected
partners. The aim is to shape standards, build dependency, and counter Chinaâ€™s Digital Silk Road playbook.
â— In May 2025 the administration rescinded the Biden-era AI Diffusion Rule,
which had imposed tiered licensing on exports of advanced AI chips and
closed-weight models and relied on close supply-chain monitoring.
â— The new approach favors proactive dissemination. Selected countries
receive an integrated American AI stack that includes infrastructure,
foundation models, deployment tooling, and governance templates.
â— Eligibility will likely track commercial opportunity, security alignment, and
pressure from U.S. industry. Access will not be universal and end-use
monitoring remains a stated requirement.
â— The strategy shifts value to U.S. vendors while raising risks of lock-in and
uneven controls downstream. It also tests whether export packages can
outperform pure restriction in limiting rival inï¬‚uence.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 193

US chip-export policy zigzags test leverage over China
2025 saw fast reversals between restriction and accommodation. The administration is balancing
national-security aims with supply-chain reliance and vendor lobbying, putting NVIDIA and AMD in the political
spotlight and injecting uncertainty for partners and compliance teams.
â— January 2025: the Commerce Secretary nominee publicly backed tougher controls, signaling a harder line
at the outset.
â— March-April: the Department of Commerce expanded AI-chip restrictions to China, effectively blocking
sales of previously â€œcompliantâ€ parts such as the H20 and warning that loopholes would be closed.
â— July: after sustained industry lobbying, the department cleared a downgraded H20 for the China market,
reopening a controlled channel while keeping top-end parts off-limits.
â— August: the US government reached conditional license terms with NVIDIA and AMD that include a 15%
revenue give-back on China sales, and Congress took up the GAIN AI Act to prioritize domestic buyers,
steps that mix export access with tighter industrial policy.
â— The net effect is that vendors face stop-start compliance and pricing risk while Beijing accelerates local
alternatives and a gray market thrives when rules shift faster than enforcement.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 194

No Pain, No GAIN
The GAIN AI Act would require chipmakers to fulï¬ll orders from US-based customers before selling advanced
GPUs to â€œcountries of concernâ€ (e.g. China, Russia, NK). Unsurprisingly, NVIDIA is not happy.
â— NVIDIA has long echoed the sentiment that demand for their GPUs outstrips
supply. When dealing with these global constraints, Huang has claimed that
NVIDIA â€œallocates fairly.â€ This stance has enabled the legal shipment of roughly
1.9M GPUs to China since 2023. Yet, defanged chips still compete with other
SKUs for capacity across the value chain. Therefore, the sale to Chinese
customers has either lengthened lead times for US clouds OR the supposed
supply chain bottlenecks have been exaggerated by NVIDIA for many years.
â— In its ï¬rst volley, NVIDIA also likened GAIN to the AI Diffusion Rule, which it
claimed to be â€œbased on doomer science ï¬ction.â€ Yet, the former only applies to
countries subjected to an American arms embargo or nations that the DNI*
deems to be hosting (or intends to be hosting) a military or intelligence facility
associated with an embargoed country. Despite certain subjectivities, the scope
is clearly narrower than the Diffusion Rule, which applied to the entire world.
*Director of National Intelligence (currently Tulsi Gabbard)

stateof.ai 2025

| 195

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Rendering new verdicts: NVIDIAâ€™s recent attempts to exert inï¬‚uence in Washington
To maintain its presence in China, NVIDIA has rapidly built up its presence in DC. Since the ï¬rst round of export
restrictions, NVIDIA has grown both its internal government affairs team and its external lobbying expenditures.
â— After new contracts with Brownstein and BGR Group, NVIDIA gradually
approaches the lobbying expenditures of the top spending tech ï¬rms (Meta:
~$14M YTD and Google: ~$8M YTD). NVIDIAâ€™s expenditures are expected to
increase further this year as decisions surrounding the B30A loom closer.
â— NVIDIA's moves reï¬‚ect the company's unwillingness to abandon the Chinese
chip market. From a proï¬t-maximization perspective, China will represent a
"$50B market" in 2026. While that potential revenue can bolster R&D
spending, shipments also strengthen the CUDA ecosystem, which beneï¬ts
from the ~1.5M active developers based in China. Losing these â€˜moat diggersâ€™
could have serious long-term consequences. Similarly, although Chinese
challengers already receive large government subsidies, NVIDIA's exit from
that market would directly inject capital in these organizations.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 196

But Washington may not be the only place NVIDIA needs to lobbyâ€¦
Even with the administrationâ€™s support, NVIDIAâ€™s stable presence is far from assured in China. On September 15,
2025, Chinese antitrust regulators announced that NVIDIA was in violation of Chinaâ€™s antitrust rules for a 2020
$7B acquisition of Israeli Mellanox Technologies, a networking products supplier. China originally approved the
purchase on the condition that NVIDIA not discriminate against Chinese customers, but that condition was all
but impossible to abide in the face of previous US export bans. No penalty has yet been announced but NVIDIAâ€™s
ï¬ne can be as much as 10% of its annual (most recently completed ï¬scal year) China sales.
â— The timing of the announcement raises eyebrows as it came during US-China
trade talks in Madrid. The immediate absence of any penalty to the
preliminary ruling gives China potential leverage. Interestingly enough,
Chinese antitrust authorities usually announce the ï¬ne at the same time as
the ruling but may be holding back here to give trade-peace a chance.
â— The decision, too, comes at the same time China has been encouraging its
domestic tech companies to not use the H20. So while, in the long term, China
hopes to wean itself off American chips, in the short-term, China still does
need NVIDIA - even though it is eager to show it does not.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 197

Chips, Tariffs, Subsidiesâ€¦oh my!
Trump has openly criticized the CHIPS Act, arguing elsewhere that all subsidies were â€œtaxpayer handouts.â€
Recently, Trump said he will be putting a â€œfairly substantial tariffâ€ on semiconductor imports to incentivize
domestic chip production. Those tariffs, however, would carry exemptions for companies that agree to invest in
the US. Rather than promote onshoring chip production through allotted subsidies under the bipartisan CHIPS
Act, the administration, motivated by seeing an ROI, has used tariffs and other alternative strategies to
encourage onshoring.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 198

US and Intel: a new, unexpected, stake-based friendshipâ€¦the start of something new?
Besides tariffs, one of the new non-CHIPS strategies includes acquiring equity in chipmakers. After accusing the
newly-installed Intel CEO, Lip-Bu Tan, of being â€œhighly-conï¬‚ictedâ€ because of $200M+ investments in Chinese
companies, Trump had the US Gov acquire a 10% stake in the struggling chipmaker in return for ~$8.9B of
earmarked CHIPS grants. This isnâ€™t the ï¬rst time the US bought a stake in a company to promote national
interests (see US Steel, AIG, and GM). The move replaces the failed Intel and TSMC joint venture that Trump and
Lutnick pushed. Is this opportunism or the start of a wider US industrial policy?
â— Lutnick has also ï¬‚irted with taking a stake in Samsung and TSMC. But the
fact that the USG is getting involved in the private sector by drawing on a
strategy normally reserved for crises is a change in precedent. Tax breaks
and subsidies, the tools the USG has normally used to focus the private
sector, may give way to a new strategy of direct investment by the USG.
Some CEOs, Republicans, and others have been rumored to have deep
concerns about the deal.
â— Unexpectedly, Sen. Bernie Sanders (I-VT) agreed with Lutnick that
taxpayers should see a return from the USâ€™s heavy investments in
domestic chip manufacturing.

stateof.ai 2025

| 199

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

But thereâ€™s more unusual US Government partnerships with the private sectorâ€¦

Trump administration
allows NVIDIA and AMD to
sell AI chips to China on
condition that the USG
receive a 15% cut of sales.

USG given a â€œGolden Shareâ€
that allows the USG to
appoint one independent
director to US Steelâ€™s board
and gives the US President
veto powers over certain
speciï¬ed US Steel
decisions.

MP Materials, a rare earth
minerals company, closed a
10 year deal with the DoD,
where the DoD purchased
up to $400 million-worth
of MP stock, with the
option to increase its share
of the company to 15%
later on.

Lutnick discussed the
possibility of buying a
stake in various US
defense companies,
claiming that the USG is
already responsible for
most of their revenue.

stateof.ai 2025

| 200

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

TikTok USA: the â€œwill they/wonâ€™t theyâ€ storyline comes to an end

ğŸ«¡

ğŸ«¡

Still, the strangest USG-private sector arrangement comes between the USG and ByteDance. In Sept., the US and
China agreed to a deal to end the years-long tease in which the US implemented but never executed a ban on
TikTok. TikTok will be allowed to operate in the US with a copy of TikTokâ€™s recommendation algorithm sent to
Oracle. Oracle will retrain the algorithm and be responsible for data and algorithmic security. ByteDance will own
just under 20% of the new US-based company (at a total valuation of $14B) with a group of new investors,
including Oracle, owning a majority. Six of the seven board seats will be ï¬lled by Americans (TBD). The USG will
also receive an unknown multi-billion dollar fee from investors for its role in the negotiations.
â— The TikTok deal sets the benchmark for data sovereignty: algorithm needs to be retrained using local data
that is protected and stored locally. The data collection issue is clearly addressed. Indeed, it is hard to see
ByteDance exï¬ltrating data or inï¬‚uencing the US app now that it only has a minority stake.
â— While not requiring congressional approval, the deal will certainly receive intense congressional scrutiny,
not least because of new free speech worries in light of the FCC-Kimmel jawboning controversy. It is the
Dept. of Treasury (speciï¬cally, the Committee on Foreign Investment in the US) that will oversee and shape
Oracleâ€™s retraining and deployment of TikTok USA and may or may not require other *ahem* â€œprecautions.â€

stateof.ai 2025

| 201

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

If you build it, the AI will come: the USG streamlining strategies for building AI infra.
The federal governmentâ€™s AI Action Plan is treating AI infrastructure as a national priority, but rather than
funding construction directly, its main role is to weaken environmental regulations and expand energy supply so
private companies can accelerate data center build-out.
â— This includes expediting review under the National Environmental Policy
Act (NEPA) and rolling back requirements in other regulations like the
Clean Water Act and Clean Air Act. DOE announced PermitAI: a
custom-built AI on NEPA ï¬lings to streamline the permitting process.
â— Trump announced the upgrade/expansion of the national electrical grid,
incl. a $1B investment from Japanâ€™s Hitachi. The USG has also been eying
an expansion of fossil fuels while rolling back clean energy subsidies.
â— Fermi America, for instance, is seeking DOE loans to build the â€œDonald J. Trump Advanced Energy and
Intelligence Campus,â€ a gas and nuclear power complex in Amarillo, Texas for energizing data centers.
â— Because federal policy is prioritizing fossil fuel expansion, newly built AI data centers are likely to be
locked-in to those energy sources, threatening future decarbonization efforts.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 202

AI data centerâ€™s latest bottleneck: NIMBYism
The American public increasingly pushes back against new AI data center build outs in the latest political
ï¬‚ashpoint. Hyperscalersâ€™ AI aspirations may soon be capped by how well they can navigate this live wire.
â— Data Center Watch: $64B in planned data center projects have been blocked or delayed amid local opposition.
â— The following projects have been stalled or withdrawn due to concerns raised by nearby residents: Googleâ€™s
Project Flo in Franklin Township, Indiana; QTS and Compassâ€™s project in Prince William County, Virginia; and
CRGâ€™s Project Cumulus in Saint Charles, Missouri.
â— Farmers emerge as one of the leading factions, driven largely
by environmental concerns and competition for resources
(land, water, and power). Other residents raise concerns over
light pollution, air quality, and noise levels.
â— Growing domestic opposition could force American
hyperscalers to offshore more clusters. This threatens the ï¬‚ow
of spending that has buoyed the greater US economy.
However, these clusters do challenge the livelihoods of many
Americans, something hyperscalers have yet to reconcile.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 203

Foundational AI Research is a government priority, but whereâ€™s the money?
The AI Action Plan mentions the need to fund â€œbasic scienceâ€ in AI, but â€œcoreâ€ AI R&D is far below the $32B that
experts have recommended the US invest by â€˜26. This is foundational, non-commercial research - the type of
research that is sometimes argued to be more transformative than that of the private sector.
â— In â€˜21, the Eric Schmidt-led National Security Commission on AI issued a
report that called for annual federal funding for non-military AI R&D to
reach $32B by â€˜26. The Bipartisan Senate AI Working Group published an
â€œAI Roadmapâ€ (May â€˜24) that echoed the same $32B price tag.
â— In â€˜25, Trumpâ€™s proposed budget allotted $3.316B for non-defense AI R&D
spending with $1.95B for â€œcoreâ€ AI R&D. In general, total federal AI
non-military R&D spending has hovered around $3B since â€˜23.
â— It is unclear if foundational AI research will increase. The administration
has freezed funds from universities and the NSF to ensure compliance with new priorities.
â— The USâ€™s NAIRR, the shared national infrastructure for AI research and education, will move from a pilot
program to a coordinated national program in the coming year, however. The $2.6B price tag associated with
the project is split among different federal agencies and companies.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 204

AI regulation across the U.S. states: winners and losers
State lawmakers were busy debating AI legislation this past year with more than 1,080 (!) â€œAI-relatedâ€ bills
introduced across the states with 118 becoming law. The ï¬ve states that had the most AI legislation introduced
were New York, Illinois, California, Maryland, and Texas. Current state laws that are most likely to pass typically fall
under one of the following categories:
â— AI-generated child sexual abuse material (CSAM)/non-consensual intimate imagery (NCII) bans: laws
prohibiting the creation and dissemination of AI-generated child pornography or non-consensual sexually
explicit photos of adults; creating takedown rules for online platforms.
â— Transparency & disclosure requirements: laws requiring companies to disclose when a customer is
interacting with AI (especially chatbots or generative AI) or to label AI-generated content with visible
notices/metadata.
â— Government AI use restrictions: laws that restrict how state government agencies can use AI, especially for
surveillance, law enforcement or government decision-making.
â— Health & Employment: speciï¬cally, laws requiring the disclosure that an AI was used in doctor/patient
interactions or employer/job candidate decisions and requiring human oversight, in some cases.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 205

Winner: Californiaâ€™s SB53 (â€œTransparency in Frontier Actâ€)
The ï¬rst state law requiring public, standardized safety disclosures from large AI developers was signed into law
in Sept. Last year, CA State Senator Scott Wiener tried his hand in passing an aggressive AI safety bill (i.e.
SB1047) only to face pushback from industry leaders, celebrated AI scientists, and Governor Newsom, who, in the
end, vetoed the bill. Newsom convened an AI expert working group that included Wiener, and the groupâ€™s
recommendations formed the backbone of SB53. Modest as the law is (it applies only to the largest developers
and accounts primarily for signiï¬cant harms), it will set the national standard for AI transparency requirements.
â— The law will apply only to large frontier model developers (10^26 FLOPS
and >$500M in revenue). These developers will need to make available
safety and security protocols on their websites and issue public risk
assessment results any time they release or update a new frontier model.
Importantly, developers need to notify state authorities of critical safety
incidents or threats of imminent danger. Whistleblower protections are
also included in the legislation.
â— Other states are taking note: both Michigan and New York are
considering similar transparency requirements for large developers with
stateof.ai 2025
New Yorkâ€™s law awaiting signature from Gov. Kathy Hochul.

| 206

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Loser: omnibus AI laws winnow down and third party auditing a no-go for now
Despite SB53â€™s passage, state governance laws that apply across different sectors and impose safety obligations
on private sector entities are few and far between. Other previously enacted AI governance laws were revised and
narrowed over the last year out of renewed fears that the regulation might be too onerous. Attempts to include
third party auditing requirements across state laws like SB53 were resisted and the requirement dropped in
most cases.
â— Gov Youngkin (Virginia) vetoed state AI governance legislation (HB2094)
because the legislation would place burdens on the AI industry. The
Connecticut Gov also said he would veto AI legislation for the same reason.
â— Texas passed the Responsible AI Governance Act (TRAIGA), but it was
amended during committee discussions to remove obligations like: â€œduty of
care,â€ impact assessments, and high-risk AI system disclosures.
â— Utah passed new amendments to its Utah AI Policy Act (UAIPA) that
narrowed key legal requirements to apply only to high-risk GenAI systems.
â— Colorado delayed implementation of its AI Act-modeled â€œColorado AI Actâ€
until June â€˜26 with amendments being debated by the state legislature.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 207

The State of State AI Regulation is a â€œpatchworkâ€ problem
The Trump administration pushed for a state AI moratorium in part because it believed that state AI legislation
was too uneven and confusing to be effective. Some states have been more active than others in regulating AI. Of
course, AI companies are not happy. A receptive Trump administration has encouraged the biggest developers
and investors to take a staunch anti-state legislation stance. It is not about what kinds of laws states should pass
but whether state legislatures should be passing any AI legislation to begin with.
â— In submissions to Trumpâ€™s AI Action Plan, OpenAI, Meta, and
Google all called for the preemption of state AI laws to free them
of liability for nearly all state AI legislation. VC ï¬rm Andreessen
Horowitz argued that AI state laws could be considered
unconstitutional under the interstate commerce clause.
â— But the show goes on and companies face a trio of choices to
manage the â€œpatchworkâ€: 1) structure compliance according to the
most stringent legislative frameworks (like the Colorado AI Act) 2)
fragment compliance state by state 3) lobby and wait for federal
action that may never come.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 208

But the Big, Beautiful ï¬ght over AI preemption continuesâ€¦in the sandbox
Trumpâ€™s landmark One Big Beautiful Bill (now a federal spending statute) initially contained a preemption clause
that conditioned state broadband funds on the implementation of a 10-year moratorium on state and local AI
regulations. The controversial provision was struck down before the billâ€™s ï¬nal passage. But the idea lives.
Senator Ted Cruz (R-TX), the moratorium lead, introduced a regulatory sandbox bill (SANDBOX Act) that would
permit companies to apply for waivers from â€œobstructive legislationâ€ for up to 10 years.
â— Despite being close to implementation, the provision was unpopular from the
start with unlikely bedfellows joining in bipartisan condemnation. Indeed, the
provision was only removed because Senator Blackburn (R-TN) was worried
about child safety and removing recent Tennessee-passed copyright
protections for artists (see ELVIS Act).
â— Anthropic lobbied Congress, arguing that streamlined federal regulation
should be established before preempting state/local oversight. Labs appear to
prefer light-touch federal regulation over no regulation, but the latter may be
preferred over a patchwork of state laws. The number of state bills in motion is
often overstated; in truth, the IAPP estimates that only ~40 bills (of the over 1k
stateof.ai 2025
introduced) may signiï¬cantly impact American AI labs.

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 209

Regulatory sandboxes: innovation boost or waiver machine?
The U.S. â€œSANDBOX Actâ€ would create a federal AI sandbox run by the Ofï¬ce of Science and Technology Policy
that grants time-limited waivers from existing rules. Firms could receive two-year waivers, renewable up to ï¬ve
times (max 10 years), with â€œauto-approvalâ€ if an agency doesnâ€™t respond in 90 days and an appeal path to OSTP.
Critics say this risks weakening enforcement, while supporters say it accelerates learning-by-doing.
â— Whatâ€™s new in the U.S. bill: Centralized sandboxing at OSTP with
participating agencies (e.g., FTC) reviewing, but non-response means
approval. Denials can be appealed to OSTP.
â— How it differs from the EU: EU AI Act mandates regulator-led sandboxes
focused on risk mitigation and proof-of-compliance, not multi-year
waivers from rules.
â— Pros: Early, supervised deployments, faster feedback loops for regulators,
and visibility into frontier systems before scale. ï¿¼
â— Cons: Incentives for forum shopping and regulatory capture, uneven
protections, could function as de-facto deregulatory waivers if
â€œauto-approvalâ€ becomes common.

stateof.ai 2025

| 210

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

RIP International AI Governance, AI Treaties, and Global AI Safety Alignment
Trumpâ€™s re-inauguration has brought an abrupt stop to an era of international diplomacy that emphasized AI
safety, alignment, and the formulation of voluntary AI measures that would â€œpromote reliable and trustworthy
AI.â€ The list of the casualties includes:
â— Council of Europe Framework Convention on AI
â—‹ An AI human rights treaty that opened for signature on September 5, 2024.
â—‹ 16 countries + EU have signed, but zero signees have ratiï¬ed the agreement.
â— The G7 Hiroshima AI Principles and Code of Conduct
â—‹ OECD launches a voluntary framework enabling ï¬rms to report how they incorporate the G7 AI principles.
â—‹ 2025 G7 Kananaskis press release includes vague commitments to â€œleverage the outcomes of the
Hiroshima AI Process,â€ whatever that means.
â— The United Nations
â—‹ In 2024, UN convenes an AI advisory body to â€œundertake analysis and advance recommendations for the
international governance of AI.â€ Publishes a report; US announces it is rejecting world AI oversight.
â—‹ In 2025, UN announces two â€œmechanismsâ€ (i.e. committees) to promote â€œcooperation.â€
â—‹ 200+ experts sign a strongly-worded letter to encourage â€œAI red lines.â€

stateof.ai 2025

| 211

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

RIP AI Safety Institute Network
On November 21, 2024, representatives of AI safety institutes from Australia, Canada, the EC, France, Japan,
Kenya, South Korea, Singapore, the UK, and the US gathered in San Francisco to announce the International
Network of AI Safety Institutes. Almost a year later and the network has met twice with the US a no-show.
â— Its purpose was to create a global network of publicly funded government
agencies that would meet regularly to develop best practices and share
research for monitoring and reporting harms, risks, and incidents.
â— The network has met twice in â€˜25 for joint testing exercises evaluating the
risks of agentic systems. The US did not appear at either meeting.
â— The U.S. AI Safety Institute has rebranded itself under the Trump
Administration as the Center for AI Standards and Innovation while the UK
AI Safety Institute has rebranded itself as the AI Security Institute
â— Unlikely that the US will participate in any future AI Safety Institute
Network events. At the same time the Network was meeting in Paris, VP
Vance gave a speech before the AI Summit saying as much: â€œThe AI future
is not going to be won by handâ€‘wringing about safety.â€

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 212

The â€œAI-Washingâ€ problem: the year in federal enforcement
Federal agencies like the DOJ, FTC, and SEC have largely assumed the AI regulatory oversight role. Their main priority is in
making sure that companies do not overstate their AI claims (â€œAI-Washingâ€).

â— One of the more infamous cases brought by the DOJ over the last year was an
indictment against Albert Saniger, former CEO and founder of â€œnate,â€ for
making false claims of integrating AI to raise $40M< from investors. In truth,
nateâ€™s â€œAIâ€ capabilities were an outsourced team of Filipino workers.
â— DOJ has amended its internal guidance: if a committed crime is made worse
by AI, the DOJ will request a harsher sentence.
â— FTC has brought several actions against AI companies. A number of these
involve companies making false claims about their products or services (i.e.
Workado, DoNotPay, Cleo AI, and Evolv Technologies).
â— SEC, meanwhile, has created a Cyber and Emerging Technologies Unit (CETU),
which is a 30-person task force to detect fraud in AI/ML tech companies.
â— SEC has been looking closely at AI related disclosures made in public ï¬lings
and in letters to stakeholders to look out for potential â€œAI-Washing.â€

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 213

FTC probes AI chatbotsâ€™ interactions with children
In September 2025, the FTC launched an inquiry into how AI chatbots communicate with minors, following
reports of â€œsensualâ€ exchanges on Metaâ€™s bot and a lawsuit alleging ChatGPTâ€™s role in a teenagerâ€™s suicide. While
not a formal investigation, the proactive move signals regulatorsâ€™ intent to avoid repeating the mistakes of
early social media oversight.
â— The FTC requested information from Meta, OpenAI, and Google but also
from Snapchat, xAI, and Character.AI.
â— Sen. Hawley (R-MO) said he was going to launch a formal congressional
investigation into Meta following the report of â€œsensualâ€ conversations.
Hawley is the Chairman of the Senate Judiciary Committee Subcommittee
on Crime and Counterterrorism.
â— OpenAI, in response to the lawsuit, said that it was going to implement
parental controls including sending a notiï¬cation to parents when their
children show signs of â€œdistress.â€

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 214

The reverse acqui-hire: the Valleyâ€™s fast-track exit
Big Techâ€™s expected M&A â€œTrump Bumpâ€ has yet to arrive as antitrust scrutiny under Biden chilled buyers.
Instead, Big Tech has embraced reverse acqui-hires: deals that onboard talent in weeks, avoid acquisition rules,
and leave hollowed-out â€œRemainCosâ€ to pivot into smaller markets.

â— Reverse acqui-hires surged in 2025, with Google, Microsoft, Amazon, and
Meta leading deals (e.g., Microsoft/Inï¬‚ection, Google/Character.AI,
Amazon/Adept, Meta/Scale AI).
â— Their typical structure consists of (1) generous founder/staff payouts, (2)
IP licensing to make investors whole, (3) a slimmed-down RemainCo
shifting to niche B2B.
â—‹ Inï¬‚ection AI â†’ Microsoft: from $4B AI contender to 12ppl â€œAI Studio.â€
â—‹ Adept â†’ Amazon: $1B agentic AI startup reduced to a small
enterprise AI team.
â—‹ Scale AI â†’ Meta: once 1,400 employees, cut staff/pods, now a single
B2B â€œDemand Generationâ€ focus.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 215

But the FTC may be cracking down as it pursues investigations
Regulators are signaling growing discomfort with reverse acqui-hires, even though they evade formal M&A
review. Lina Khan opened an investigation into Microsoftâ€™s Inï¬‚ection deal in 2024, and by March 2025 new FTC
Chair Andrew Ferguson was demanding more disclosures on Microsoftâ€™s AI operations.
â— In July â€˜24 the FTC requested details from Amazon on its reverse acqui-hire of Adept after Senators Wyden,
Welch and Warren signed a letter urging an investigation.
â— Meanwhile, the UKâ€™s own competition authority cleared the Microsoft/Inï¬‚ection AI deal in â€˜24 saying they
do not see a â€œrealistic prospect of a substantial lessening of competition.â€
â— While no action has been taken, the FTCâ€™s signals suggest reverse acqui-hires could yet be treated as
acquisitions, putting Big Techâ€™s favorite exit path under threat.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 216

Figmaâ€™s IPO: Lina Khanâ€¦vindicated?
The FTC and DOJ challenged fourteen startup acquisitions between 2020 and 2023 whereas in the seven years
prior (2012-2019), the two agencies challenged only three. Both Khan and DOJ Antitrust Chief Jonathan Kanter
operated under the theory that Big Tech choked the startup ecosystem by acquiring startups with the intent of
killing competition. Silicon Valley scoffed at the theory, but Figmaâ€™s strong IPO may validate the thinking.
â— Adobe sought to acquire Figma for $20B in 2022 but after 15 months of
antitrust scrutiny, the two saw no path toward regulatory approval. Figma
pursued an IPO on July 31, 2025, instead. At the end of Figmaâ€™s debut, the
company was worth close to $57B after the stock tripled during trading
hours. Figmaâ€™s stock price has since dropped from that ï¬rst-day high, but
investors still view it as a solid public grade tech company.
â— Wiz, too, was considering an IPO before agreeing to an acquisition
by Alphabet in the weeks following Trumpâ€™s inauguration. Wiz
originally rejected a $23B offer in July â€˜24 out of regulatory
concerns + IPO dreams. Does Wiz now have sellerâ€™s remorse?

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 217

Wiz/Alphabet: Big Techâ€™s Ultimate M&A Test
Alphabetâ€™s $32B bid for Wiz, announced weeks after Trumpâ€™s inauguration, is the largest AI security deal yet and a
live test of whether the DOJ will soften antitrust under new leadership. Google is already in hot water after
having lost not one, but two (!) major antitrust suits over the last year. Current DOJ Antitrust Chief Abigail Slater
is far from a friend for the Valley but she could face pressure to approve the mega-purchase.

â— DOJ Antitrust Chief Slater during the Google antitrust case: â€œIn a time of political
division in our nation, the case against Google brings everyone together.â€
â— The DOJ in July, however, approved three mergers worth $63B in just one week and
there are reports that Slater is being pressured to be more â€œdeal-friendlyâ€ towards
incoming transactions and acquisitions.
â— But the timing of Alphabetâ€™s largest acquisition could not be worst as the company is attempting a vertical
merger, a deal more prone to regulatory disapproval, when it is under intense federal scrutiny. Alphabet
agreed to pay $3.2B (10% of the deal) to Wiz if the transaction is not approved - not exactly a sign of
diminished conï¬dence. The deal, if it goes through, would essentially be a greenlight for tech to return to
buying startups through traditional acquisitions.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 218

Google comes out unscathed in 1 of 2 antitrust cases but search is on the ropesâ€¦
The Silicon Valley darling has been bruised, battered, and, ultimately, trust-busted over the year in two landmark
antitrust case rulings. Judges in two separate cases ruled that Google was operating a monopoly over 1) search
2) ad tech. Remedies were issued early Sept. â€˜25 in the search monopoly case where the court, pointing to the
vulnerability of Googleâ€™s search business to LLM chatbots, largely agreed with Googleâ€™s proposed remedies. It was
a big win for Google given the heat the company is taking from ChatGPT and other agents that disintermediate
Google.com
â— Google must now share search index data with competitors like DuckDuckGo and
can no longer make exclusive contracts with other companies to feature Google
Search and other products. Google can still pay partners to feature Google search
and other apps. So while Google paid Apple $20B for exclusive use of Google
search on Safari, that deal can remain so long as the terms are non-exclusive.
â— These are light-touch compared to the â€œstructural remediesâ€ the USG proposed,
which included forcing Google to sell its Chrome browser.
â— Google will likely appeal and ï¬ght against the initial â€œmonopolyâ€ branding, but
thereâ€™s also the â€œad techâ€ case where, experts caution, Google is more likely to be
stateof.ai 2025
forced to part with core parts of its ad business.

| 219

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Meanwhile in Europeâ€¦
Pressure has mounted for the EU to abandon its landmark AI Act, but the Commission has trudged on with the
regulation using the following phased compliance schedule:
August 1, 2024

The AI Actâ€™s compliance countdown ofï¬cially begins; law is on the books (i.e. the Ofï¬cial
Journal of the European Union) but its key obligations are not yet in effect.

February 2, 2025

The ï¬rst major obligation goes into effect: a ban on â€œunacceptableâ€ AI risk systems (e.g.
social scoring, biometric identiï¬cation, facial recognition)

August 2, 2025

The voluntary Code of Practice for GPAI is released; ï¬rms that choose to follow the Code of
Practice have presumed compliance with the AI Act. Those that donâ€™t are subject to the AI
Actâ€™s Chapter V rules for GPAI models.

August 2, 2026

Remaining obligations for AI systems (except for â€œhigh-riskâ€ AI systems) goes into effect.

August 2, 2027

Obligations for â€œhigh-riskâ€ AI systems go into effect.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 220

The GPAI Codes of Practiceâ€¦and so it begins.
On August 2, 2025, the GPAI Codes of Practice went into effect after a three month delay. The â€œCodesâ€ are one of
the ï¬rst major steps of the AI Actâ€™s implementation. For providers of general-purpose AI (GPAI) models, the AI Act
requires companies to develop frameworks to show how they would fulï¬ll requirements concerning 1)
transparency 2) copyright and 3) safety and security (this category only applies to frontier GPAI models posing
systemic risk). The â€œCodesâ€ are a voluntary framework, an option for companies that would rather not develop
their own guidance to fulï¬ll the stated GPAI obligations, which began August 2, 2025. EU enforcement of the
obligations, however, does not begin until August â€˜26. The AI Act also has a 2-year grace period for models
released before August 2, 2025.
â— Amazon, Anthropic, OpenAI, Microsoft, Google, and others have signed all three chapters of the â€œCodes.â€
â— xAI has agreed to sign the third chapter (â€œSafety and Securityâ€) but has not signed the chapters on
â€œTransparencyâ€ and â€œCopyright,â€ which makes the company responsible for developing its own policy that
meets the AI Actâ€™s requirements for transparency and copyright compliance.
â— Meta has refused to sign the Code of Practice saying, â€œThis Code introduces a number of legal uncertainties
for model developers, as well as measures which go far beyond the scope of the AI Act.â€

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 221

So, whoâ€™s afraid of the AI Act? ğŸ˜“ğŸ˜“
Needless to say, companies, both in the EU and abroad, are not happy with the AI Actâ€™s rollout. The EUâ€™s delayed
implementation is not exactly inspiring conï¬dence. Each member state was required to assign national
authorities to oversee the AI Actâ€™s implementation but so far only 3 member states have fully completed the
requirement. The AI Act also called for the creation of technical standards by April â€˜25 to address the â€œhowâ€ of
compliance. But as of this writing, those standards are still in development. A coalition of EU AI companies signed
a letter in July calling for a 2-year â€œstop clockâ€ on the AI Act with Swedenâ€™s PM publicly calling the AI Act
â€œconfusingâ€ and President Macron saying â€œwe are over regulating.â€ But the show goes onâ€¦for now.
â— In Sept., the EC opened a â€œdigital simpliï¬cation omnibus,â€ a
commission that would look and review all of the EUâ€™s digital
laws, including the AI Act, to simplify its digital rules. In general,
the EC announced that it wants to â€œreduce administrative
burdensâ€ by 25% across the regulatory board.
â— The calls for a â€œpauseâ€ on the AI Act grow louder by the day,
however. Despite the pressure, the EC has said that it would not
delay implementation deadlines.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 222

Is the EU doing enough to catch up in the AI race?
Europe is trying to shift from rulemaking to capacity-building, but the gap keeps widening. Over 50 years, the
region minted no tech ï¬rm above $400B in value, while the U.S. now has seven at $1T+. In 2024, U.S. labs
shipped ~40 major models, China ~15, and the EU ~3. Brussels is setting aside billions to amplify its spending,
but is that the scale or speed the problem demands?
â— In 2024, Mario Draghi, the former European Central Bank President,
issued a report on â€œEuropean Competitiveness,â€ whose
recommendations the EU agreed to implement. Draghi pointed to
multiple structural challenges including market fragmentation,
regulatory barriers, demographic decline, low productivity, and
risk-averse capital deployment as reasons for EUâ€™s stagnation.
â— Draghi put the cost of his modern-day Marshall Plan at â‚¬800B per
year (!). While the EU has increased spending, most notably
launching InvestAI (an >â‚¬200B AI investment fund), it is not taking
that price tag seriously. One estimate said that only about 11% of
Draghiâ€™s recommendations were seen through so farâ€¦

*Chart shows private investment only. Including China's estimated $56B in
government AI funding (2025), total Chinese investment signiï¬cantly
exceeds Europe's.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 223

Cheerio, Bletchley, and â€˜ello â€œgrowth zones!â€: Starmer to â€œmainline AI into the UKâ€™s veinsâ€
Since January, the UK has shifted from convening global AI safety to an industrial push. The AI Opportunities
Action Plan prioritizes investment, data-centre capacity, and light-touch rules, with designated â€œgrowth zonesâ€ to
fast-track permits. The government frames this as a route to a potential ~$740B GDP boost by 2035.
â— Announced actions include increasing compute 20x by 2030 and
growth zones for data centers with streamlined planning.
â— Starmer has committed to adopting 50 recommendations from ex-AI
adviser Matt Clifford and to avoid new rules that could slow
deployment. But government bureaucracy has made
operationalizing these large-scale ambitions a slog.
â— Ministers claim AGI is around the corner but compute investments
remain well below that of the US while AISI relies on voluntary
agreements with labs to test models pre-deployment.
â— The countryâ€™s strategic tone now mirrors the US: pro-adoption, state-backed
capacity, rebranded â€œAI safetyâ€ bodies focused on capability evals and
deployment, and delaying safety legislation to an undetermined date.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 224

The CCPâ€™s Action Plan: an AI World Tour
Three days after Trump announced the AI Action Plan, China released their own Global AI Governance Action
Plan. Coincidence? We think not. Chinaâ€™s deliberately contrasting ambitions, which are no less zealous, emphasize
strategies of â€œmultilateral collaborationâ€ and â€œdiplomatic engagement.â€ In practice, these buzzwords denote
Chinaâ€™s plan to supply the Global South with its own AI solutions and to draw on its inï¬‚uence among developing
countries, especially in Africa, to impact the UN and other international bodies.
â— China has proposed setting up a new World AI Cooperation Organization
and is working with the UNâ€™s Pact for the Future and Global Digital
Compact to build out processes for AI development and governance.
â— China appears to be applying the US TikTok playbook with authorities ï¬rst
cautioning and then instructing companies to stop buying NVIDIA chips
for fear of the US threatening the countryâ€™s national security.
â— The Belt and Road Initiative, the Global Development Initiative, and the
Global Security Initiative continue to be key components of Chinaâ€™s global
digital investment strategy.

stateof.ai 2025

| 225

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Chinaâ€™s AI Regulations also begin to take effect
The worldâ€™s most active AI regulator continues to issue AI standards and regulations. Here are a few of the
highlights of Chinaâ€™s most signiï¬cant regulations from the past year:
â— Administrative Measures for the labeling of AI-generated content: In effect September 1, 2025, this obligates
content service providers to clearly label AI-generated content. Chatbots, AI-generated writing and video
creation all require customer-facing labels denoting the AI as such. Some AI-generated content, however,
may only require hidden labeling within the metadata.
â— Three National Cybersecurity Standards: In April 2025, the State Administration for Market Regulation and
the Standardization Administration of China released three separated standards outlining security
requirements for datasets, data-labeling, and, in general, generative AI services. The requirements take effect
November 2025.
â— The AI-Plus Plan: The State Council of China released a set of industrial policy goals that all aim to have AI
capabilities fully integrated across the entire Chinese economy with complete AI penetration across all
Chinese sectors by 2035.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 226

China aims to achieve tech self-relianceâ€¦no matter the Â¥Â¥Â¥Â¥
Chinaâ€™s strategy for AI self-reliance gained ground over the last year with news-making achievements from open
source leaders like DeepSeek, MoonShot AI, and others. During an April 2025 Politburo meeting, President Xi
Jinping signaled all hands were on deck and told ministers to â€œredouble our effortsâ€ on AI. This shows Chinaâ€™s
intent on achieving self-sufï¬ciency, a perennial aim that always intensiï¬es whenever it enters a trade war with
the US. But mounting debt levels may pose problems down the line.
â— Rising debt among countries is not new, but the problem is
pronounced in China where the country accounts for â€œmore than half
of the increase in the global economyâ€™s debt-to-GDP ratio since
2008.â€ IMF in a 2025 report suggested itâ€™s unstable.
â— This is largely due to national ambitions that push for high annual
GDP growth, forcing local governments to invest in dubious assets.
â— But AI spending is not slowing down with the CCP allocating a 10%
increase in science and tech spending from 2024. Winning the AI
race, then, isnâ€™t just a political imperative but potentially vital to the
long-term health of the Chinese economy.

stateof.ai 2025

| 227

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The Gulf enters the AI power game with a trillion-dollar bets
UAE and Saudi Arabia are leveraging massive compute build-outs, chip import deals, and huge US trade and
investment partnerships to position the Gulf as a central node in the global AI balance.
â— In May 2025, Trump visited the Gulf with over 60 US executives including
Jensen Huang, Sam Altman, and Larry Fink to launch the US-UAE AI
Acceleration Partnership: a 10-year $1.4T investment by the UAE into US
AI, chips, energy and infrastructure. G42 will build a 5GW AI cluster (i.e.
Stargate UAE) across 10 square miles.
â— The UAE will import up to 500k Blackwell GPUs worth $15B through 2027.
G42 will get 20% of the chips and 80% will go to the regional data centers
US tech giants with each watching out for unintended Chinese usage.
â— In Saudi Arabia, the US agreed deals worth $600B: $142B of US defense
equipment sales, $20B for US AI data centers, and $80B of investments into
the region from Google, DataVolt, Oracle, Salesforce, AMD and Uber.
â— Beyond domestic projects, UAE sovereign funds pledged to ï¬nance a 1GW
AI datacenter in France (costing â‚¬30-50B).

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 228

While Latin America tries to carve its own path with LatAm-GPT
Over 30 Latin American institutions are training a 50B open model on data from 20 countries and Spain to
reï¬‚ect local dialects and norms. Chile leads the effort with support from public agencies, universities, and AWS.
The budget is about $3.5M and the release is planned for December. The question is whether a modest, regional
model can compete where ChatGPT and Claude already have strong adoption.
â— Brazil ranks third globally for both ChatGPT and
Claude usage. Indeed, reported AI adoption
across LatAm was about 40% last year Brazil
(India was the highest at 59%).
â— OpenAI opened its ï¬rst LatAm ofï¬ce in SÃ£o
Paulo in August 2025, signalling rising regional
demand for frontier systems.
â— LatAm-GPTâ€™s value case is localization and
capacity building. The scale and funding are
small, so impact may stay academic unless
governments and industry adopt it widely.

stateof.ai 2025

| 229

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

US defense sizes up its bet on AI-ï¬rst systems and opens to up to frontier AI labs
2025 marked a decisive step change in how the US and its allies procure and deploy AI in defense. Rather than
fragmented pilot projects, defense leaders consolidated billions into enterprise-scale AI platforms while also
opening the door to frontier model providers. NATO fast-tracked its ï¬rst alliance-wide AI system, Palantirâ€™s
Maven Smart System, as a central pillar of the Western defense industrial base.
â— US Army signed a 10-year enterprise contract with Palantir worth up to
$10B, consolidating dozens of prior software deals into one platform.
â— Project Maven expanded to $1.3B through 2029, now supporting 20,000+
individual warï¬ghters, double the base earlier in the year.
â— NATO procured Palantirâ€™s Maven Smart System in just six months for all
members despite feeling frosty with loosening US security guarantees.
â— DoD signed $200M ceiling contracts with each of OpenAI, Anthropic,
Google, and xAI to explore frontier AI for command, cyber, and planning
missions. This marked quite a vibe shift for AI groups that previously took
a strong view that their models should not be used for defense purposes.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 230

OpenAI and Anthropic make a push to land grab US Government workï¬‚ows
The two companies are deploying a US Government access program in a bid to upskill operations and win the
hearts and minds of the administration. OpenAI and Anthropic are using their GSA OneGov partnership to extend
ChatGPT Enterprise for $1 per federal agency and Claude to all three branches (federal civilian executive,
legislative, and judiciary) for $1 too.
â— Procurement has always been the bane of new technologyâ€™s
adoption in government and other highly-regulated industries.
In the US, the General Services Administration (GSA)
announced their OneGov Strategy to modernize procurement
of goods and services in April.
â— Most notably, the strategy has led to the rapid deployment
availability of frontier models for government workers by AI
labs. It also provides for an AWS OneGov agreement with up to
$1B in credits for cloud/AI modernization.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 231

Autonomy takes ï¬‚ight with drone wingmen entering the doctrine
The U.S. has moved from DARPAâ€™s AlphaDogï¬ght in 2020 and live AI-ï¬‚own F-16 tests in 2022 to embedding
autonomy into doctrine. Collaborative drones, swarming initiatives, and multi-domain contracts are now framed
as essential to offset Chinaâ€™s numerical advantage, making uncrewed systems a core pillar of force design.
â— Air Forceâ€™s Collaborative Combat Aircraft (CCA) ï¬rst prototypes (General
Atomicsâ€™ YFQ-42A and Andurilâ€™s YFQ-44A took ï¬‚ight in 2025, with 1,000+
drones planned at ~$25-$30M each. FY25/26 budgets total $4B+ for Next
Generation Air Dominance and CCAs.
â— The Replicator initiative targets thousands of cheap autonomous systems
across air, land, sea within 24 months, backed by $1.8B FY24 funding as a
direct hedge against Chinese mass.
â— Anduril scored major autonomy wins, including $250M for
Roadrunner/Pulsar cUAS, $200M Marine cUAS contract, and an $86M
SOCOM deal for autonomy software. The company is expanding its
maritime autonomy XL UUV and hypersonic missile work.
â— Saronic too has a $392M OTA with the US Navy for autonomous boats.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 232

Europe wakes up to AI warfare and with shaky US security guarantees
Russiaâ€™s full scale war in Ukraine and wavering US signals at the Munich Security Conference in Feb 2025 jolted
Europe into treating AI as a frontline capability. Capitals are writing autonomy into plans, Brussels is mobilizing
massive rearmament funds, and both startups and primes are surging to build sovereign AI defense.
â— EUâ€™s Readiness 2030 (fka ReArm Europe) authorizes up to â‚¬650B extra defense
spend, naming AI, drones, and counter-drone as critical gaps. A new SAFE fund
will pool EU money to de-risk cross-border projects in autonomy, cyber, and
electronic warfare. Critics warn hardware may only arrive after 2030 unless AI is
prioritized as a force multiplier now.
â— The UKâ€™s Strategic Defence Review 2025 makes AI and autonomy a priority,
citing Ukraine where â€œdrones now kill more people than artillery.â€ A new Defence
Uncrewed Systems Centre and AI Investment Fund are planned by 2026.
â— Helsing raised â‚¬600M (valued ~$12B), debuted autonomous strike drones and
tested an AI-piloted Saab jet. Unicorn drone makers Quantum Systems and
Tekever raised â‚¬160M and â‚¬70M, respectively. Meanwhile, Rheinmetallâ€™s market
cap has soared past â‚¬80B (larger than VW).

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 233

GDPval: a warning shot for the job market
OpenAI's new benchmark for economically valuable tasks, GDPval, demonstrates the steady march of AI progress.
Across 44 professions and 1,320 tasks, models are approaching human experts in a signiï¬cant subset of domains.
â— Reasoning models outperformed GPT-4o on task win-rate by an
average margin of 20.7% across 44 categories of professional work.
â— Claude, which has not historically dominated other benchmarks,
achieved the highest* win rates in 32 of 44 professions. The paper
attributes part of this success to Claude's strengths in formatting.
â— General-purpose models are already demonstrating strong
competence as professional assistants. Meanwhile, frontier labs and
recent entrants General Reasoning and Mechanize are also rapidly
building RL environments on real-world work scenarios.
â— With this hill to climb and an inï¬‚ux of corporate data and demos,
knowledge workers may soon experience the workplace
transformations that AI leaders have long predicted. The Lufthansa
Group even said it expects to cut 4k administrative jobs by 2030.
*Or tied for the highest win-rate

stateof.ai 2025

| 234

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

GDPval: long live the accountants!
The tracker below ranks the exposure to disruption of different professions based on GDPvalâ€™s results.

*Average GDPval Win Rate covers the reported scores of each reasoning model and therefore excludes GPT-4o.

stateof.ai 2025

| 235

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

AI squeezes the entry-level job market while experienced workers are safe...for now
Entry-level hiring is declining across software and customer support - roles that are highly exposed to AI
automation. These trends appear to be independent of macro factors like inï¬‚ation or pandemic recovery.
â— Although total employment has grown, the hiring of younger workers has stagnated since late 2022. Despite
strong AI adoption, this group struggles to ï¬nd a foothold in the job market, On this trajectory, AI ï¬‚uency may not
guarantee favorable economic outcomes.
â— Meanwhile, law school applications spiked
21% in 2024, suggesting graduates are
hedging against uncertain career prospects.
â— Jobs for more experienced workers has
remained stable/grown, even in highly
-21%
-11%
AI-exposed domains. This suggests workers
who have acquired more tacit knowledge are
more likely to be augmented by modern AI
models. But without on-the-job experience,
workers will struggle to gain tacit knowledge.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 236

While some argue AI is not shaking up the labor market yet
A joint study from the Yale Budget Lab and Brooking Institution found that current labor market changes precede
the introduction of ChatGPT in 2022. The authors conclude that thereâ€™s little reason to think that â€œAI automation is
currently eroding the demand for cognitive labor across the economyâ€ and caution against predicting job losses
based on â€œAI-exposureâ€ data alone.
â— They focus on â€œoccupational mix,â€ a macro measurement of job movement
among workers, i.e. switching/starting/losing a job.
â— Currently, the â€œoccupational mixâ€ for AI is tracking with that of other tech
breakthroughs like the introduction of the internet and computers. If that
trend continues, an AI-induced labor disruption will take decades to
materialize, not months.
â— The studyâ€™s results, however, do not contradict the sector-speciï¬c Stanford
study (see previous slide). There is an uptick in â€œoccupational mix
dissimilarityâ€ among new grads in recent months but the dissimilarity
tracks with pre-2022 trends, suggesting non-AI factors.

stateof.ai 2025

| 237

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

AI and jobs: Nobody knows who will lose theirs, but the labs are keeping score
Both Anthropic and OpenAI released data on how its users were using their respective models. Use cases varied
across country and US state. For instance, California users were the most likely to use AI for coding while DC usage
centered around job search activities and writing projects. For work-oriented tasks, ChatGPT was often used for
writing-related tasks while Claude was often used for coding tasks. As a result, the conclusions drawn from each
of the studies for the future of work were different. OpenAI argued that its data shows AI being used mainly to
augment work-related functions and offer â€œdecision supportâ€ while Anthropic argued that enterprises, speciï¬cally,
are more likely to automate tasks with automation increasing across its customer enterprises.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 238

Governments respond reactively, not proactively
Unclear as the results are that AI is going to replace entry-level jobs, governments have struggled to implement
new, large proactive frameworks to combat what could turn into a larger jobs crisis. Instead of preparing for the
worst, the plan has been to expand existing workforce training programs and encourage AI skills training as
early as possible. At the very least, some are calling for improved data collection that can better gauge â€œAI
disruptionâ€ on employment. Major countries have each implemented some form of vocational training programs
and new AI-based curricula, but it remains to be seen whether they sufï¬ciently address the potential crisis.

â— Federal funding for K-12 AI
education under Workforce
Innovation and Opportunity
Act and executive orders;
DOE/NSF â€œSupercharging
Americaâ€™s AI Workforceâ€

â— EU AI Act literacy
provisions under Article
IV; AI Skills Strategy for
Europe; Digital Europe
Programme investments

â— Vocational Skills
Training Initiative
(â€˜25-â€™27); Ministry of
Education releases
an AI curriculum

â— Tech Sec announces
Gov-Big Tech
partnership to
reskill â…• of countryâ€™s
workers

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 239

The global AI talent wars: whoâ€™s winning on immigration and retention?
The USâ€™s AI Action Plan excluded immigration strategies for retaining foreign AI talent even while two of Trumpâ€™s
top AI advisors are foreign-born (David Sacks and Sriram Krishnan). But countries across the world have been
enticing foreign workers with streamlined visa processing, housing subsidies, and general ï¬‚exibility around their
work arrangements. The US still remains, far and above, the preferred place for top-notch AI research. But as the
US continues to cultivate a reputation as a less-than-friendly home to foreign-born talent, other countries are
taking advantage, especially China.
â— One congressional bill, though currently stalled, would
end the USâ€™s OPT program, which gives foreign-born
STEM graduates a 3-year work window post-graduation.
Joseph Edlow, Director of US Citizenship and Immigration
Services, backs ending the program. Shaking things
further, Trump announced a $100k fee to the H-1B visa.
â— Meanwhile, China is ï¬guring out how to retain the 77,000
STEM PhDs projected to graduate from Chinese
universities in â€˜25 (compared to the 40k in the USA).

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 240

China could be getting better at retaining talent: the overlooked lesson from DeepSeek
A deeper dive into DeepSeekâ€™s demographics signal that China is gradually improving its ability to train and
retain its scientists, a warning for the U.S. which has grown dependent on Chinese AI researchers.
â— A Stanford report by Dr. Amy Zegart looking at 201 DeepSeek authors found that 55% of the them were
trained and based entirely in China, without any U.S. afï¬liation. Only 24% of the DeepSeek authors had a
US afï¬liation at some point, with most staying just one year.
â— In May, State Sec. Marco Rubio announced the revocation of Chinese student visas for those with
â€œconnections to the CCP or studying in critical ï¬elds,â€ potentially accelerating Chinaâ€™s strategy to retain and
poach AI talent. For reference, the DOJâ€™s â€œChina Initiativeâ€ (2018), an enforcement program to track and
prosecute Chinese nationals sharing trade secrets with the CCP, increased China-born researcher
departures from U.S. labs by 50%, greatly accelerating reverse migration.
â— In February â€˜25, a federal grand jury charged Leon Ding, a former Google employee and Chinese national,
with economic espionage and trade secret theft for plans to steal info related to Google AIâ€™s chips and
software platform and use it to sell products for two CCP afï¬liated tech companies.
â— Meanwhile, half of the researchers reporting to Alexander Wang in Metaâ€™s Superintelligence Lab received
their undergrad degrees in China, posing major issues if talent decoupling worsens.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 241

While Europe tries its best to compete for AI talentâ€¦
The EU and UK have also been trying to capitalize on the US brain drain. But while the EU may be able to attract
a few AI researchers here and there, its biggest hurdle is, in the end, the most straightforward: money. Top AI
talent wants to be compensated. The US is still the best place for getting paid.
â— 22% of the worldâ€™s leading AI researchers studied in Europe, but only
14% continue to work in the EU.
â— EU wage growth for AI has grown only very modestly compared to the
US. In â€˜23 salaries for software developers in the US were 2x-4x higher
than they were for those in Europe.
â— While relative inï¬‚ows (see charts) show modest growth for some EU
countries, in absolute terms the differences are starker with the US
attracting more talent, on average, than its EU peers.
â— EU + UK have implemented a number of programs to attract and retain
AI talent (e.g. new visas, funds/fellowships to attract researchers), but in
the face of astronomical investment in AI elsewhere in the world, it is
unlikely to be enough to increase the regionâ€™s share of AI talent.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 242

Deepfakes and 2024 elections: emerging threat or outreach tool?
Despite rampant worries of AI-generated election dis/misinformation during the â€œlargest election year in global
history,â€ there was almost little to no negative impact from GenAI in any of the 2024 elections. In general,
deceptive uses of AI, while present, were still quite limited and there were surprising positive use cases. Both
India and the US saw the most AI uses in their elections. Experts caution that the AI dis/misinformation threat is
still real. For now, the results show positive and negative trade-offs.
â— While there were instances of deepfakes being used to
intentionally deceive voters, in general, deceptively fake
audio and video of political candidates had little to no
impact on voting outcomes. Deepfakes were often used to
amplify a partyâ€™s messaging, excite its base, and deepen
existing political divides. Candidates sometimes used â€œAIâ€ to
cast doubt on their opponents (see Liarâ€™s Dividend).
â— In India, political parties spent $50M on legal AIGen, using it
for voter outreach via AI voice clone calls, personalized
videos, and translating speeches into one of 22 ofï¬cial and
stateof.ai 2025
780 unofï¬cial languages.

| 243

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Governments across the world are starting to incorporate GenAI technologies
The last year saw a notable uptick in the amount of genAI technologies being used by government agencies
across the world:
â— Singapore: Gov launches its AIBots platform where any Singapore public servant
can create and deploy an AIBot and train it on agency data and use it both to
communicate with constituents and complete interagency work.
â— US: GenAI use cases jumped from 32 in â€˜23 to 282 in â€˜24 with an overwhelming
number of use cases coming from the Department of Health and Human
Services (mostly for data analysis and management).
â— China: Local governments have tried integrating DeepSeek in day-to-day work
and interactions with constituents; ï¬rst half of â€˜24 saw 81 gov procurement
contracts for LLMs for use in public projects.
â— UK: Government Digital Services (GDS) does a trial run of AI coding assistants.
â— EU: Launches ApplyAI Strategy, announces GenAI pilot projects for use by public
agencies.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 244

GovGPT: politicians awkwardly start using AI
Politicians have come around to GenAI use but constituencies are not pleased. The Swedish Prime Minister, for
instance, admitted to consulting AI tools in his day-to-day only to have protesters shouting â€œwe didnâ€™t vote for
ChatGPT!â€ Politicians will need to balance the use of AI tools with public concerns that their elected leaders are
tech-sourcing their governance duties.
â— One British MP, Mark Sewards, accomplished the inevitable and
created an AI clone of himself to create a full-service bot (â€œAI
Markâ€). Constituents can interact with the chatbot any time, asking
policy questions, raising issues, or writing angry letters.
â— In a mostly symbolic (and potentially illegal) move, Albaniaâ€™s PM
formally appointed an AI minister named Diella to oversee the
countryâ€™s public procurement processes and reduce corruption.
Diella even made an address to Albaniaâ€™s parliament.
â— The clearest use of AI among politicians is in speechmaking with a
notable rise in ChatGPTâ€™s preferred vocabulary in Britainâ€™s House
of Commons over the last few years (â€œI rise!â€).

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 245

Section 4: Safety

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 246

AI safety commitments: a changing of the tides?
Following a reversal in messaging around safety-relevant AI topics from the current US administration and
growing international/commercial competition amongst labs, certain safety protocols have been deprioritized.
â— First, xAI missed its self-imposed deadline to implement a safety framework it proposed at the AI Seoul
Summit in Feb. Anthropic backpedaled on promises to fully deï¬ne ASL-4 safety standards before releasing an
ASL-3 model (Claude Opus 4). GDM released Gemini 2.5 Pro but waited 3 months to publish an accompanying
model card, violating the spirit of prior commitments. Lastly, OpenAI seems to have quietly abandoned
protocols to test the most dangerous possible versions of its models (task-speciï¬c ï¬ne-tuned variants).
â— While Trumpâ€™s team has consistently recognized many
extreme AI risks, the overall shift in tone de-legitimizes
aspects of the AI Safety community.
â— The industry and policy focus is now clearly on ensuring
American dominance in the AI race. These themes echo
throughout the recent moves of AI labs, who seem to
gravitate closer to speed over precaution, often in
competition with other US labs.

stateof.ai 2025

| 247

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

AI labs spend more in a day than AI safety science organizations spend in a year
Leading external AI safety organizations rely on budgets that lag far behind the AI labs they hope to support. As
a result, the ï¬eldâ€™s best talent remains densest within the major labâ€™s internal safety teams.
â— We estimate the eleven most prominent American AI safety-science organizations combined will spend just
$133.4M in 2025. This grouping includes the following organizations: CAISI, METR, CAIS, FAR.AI, Haize Labs,
Palisade Research, Virtue AI, Gray Swan, Redwood Research, Irregular, and the Frontier Model Forum.
â— Although well-resourced, internal safety teams ultimately
answer to the same organizations racing to commercialize
frontier models. This creates a structural conï¬‚ict of interest:
ï¬ndings that call for caution may be deprioritized in favor of
speed and market advantage.
â— This isnâ€™t (just) about money: external orgs also lack other
means to attract talent like comparable prestige, and access
to privileged information / pre-release models. As such it is
difï¬cult for them to provide a credible counterweight,
leaving the ecosystem over-reliant on self-policing.

* 'AI Labs' corresponds to a rough estimate of each lab's total
expenditures in 2025 (compute, personnel costs, other opex)

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 248

The State of AI Incidents
The AI Incident Database (AIID), a community-supported website to track incidents of AI in the real world, shows
incremental jumps since 2023. Reported estimates likely underestimate the true extent of AI-enabled harms.
â— Reported incidents continue to be dominated by harms involving â€œMalicious Actors.â€ These generally involve
cyber attacks or fraudulent schemes. Since incidents can be added to AIID years after they occur, ï¬nal annual
counts may take longer to accumulate.
â— Reporting gaps also exist. Incidents can be Trends in AI incidents
difï¬cult to link to AI systems and AIID relies
on the help of volunteer submissions.
Investigating and tracking cases of AIenabled harms warrants greater support.
â— Incident counts continue to be dominated
by reports of malicious actors exploiting AI
tools. Luckily, many reported harms remain
modest in nature through this point in time.

stateof.ai 2025

| 249

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The State of AI Incidents
Incidents involving GenAI models follow steeper trends, lining up with the widespread diffusion of the
technology. Once again, malicious actors have added a new weapon to their arsenals.
â— While a large # of reported incidents involve deepfakes, LLM misuse continues to rise. Anecdotally, incidents
are becoming less innocuous over time (plagiarism and hallucinations â†’ cyber attacks and weapon creation).
â— OpenAI has shared multiple reports detailing the disruption of
Trends in AI incidents
malicious uses of their systems. Included were cases stemming
from North Korea, China, Iran, and Russia, sometimes involving
state-afï¬liated actors. Of the threats mentioned, malicious
actors attempted to leverage OpenAIâ€™s models during illicit
activities like child exploitation, covert inï¬‚uence operations,
malicious cyber activity, social engineering, cyber espionage,
propaganda generation, and credential harvesting.
â— Broader misuse likely goes unreported as attribution becomes
more difï¬cult, open models continue to proliferate, and many
labs maintain lax mitigation and transparency policies.

stateof.ai 2025

| 250

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Cyber capabilities (and risks) accelerate
AI agents are poised to signiï¬cantly challenge cybersecurity defenses. METR research shows that AI task
completion capabilities double every 7 months across general domains, but one researcher's replication
estimated that, for offensive cybersecurity, these abilities are doubling even faster: every 5 months.
â— Current models can reliably handle cyber security tasks that take
humans about 40-50% about 50% of the time.
â— Since 2019, long-horizon task solving has doubled every ~7 months.
â— A researcher applied METR's methodology to offensive cyber
security benchmarks and found a 5-month doubling time, with
current models solving 6-minute cyber tasks at 50% success rates.
â— Two notable recent benchmarks assess this:
â—‹ CyberGym tests agents on reproducing 1,507 real software vulnerabilities, with the best systems
achieving only 11.9% success at recreating known security ï¬‚aws, (though agents unexpectedly
discovered 15 previously unknown vulnerabilities), and
â—‹ BountyBench tests agents on 25 real-world systems with actual bug bounties, ï¬nding agents are better
at ï¬xing security problems (90% success) than exploiting them (32.5-67.5% success).

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 251

The rise of â€œvibe hackingâ€...
Threat actors now deploy AI for all stages of fraud operations. Criminals recently used Claude Code to
orchestrate attacks against 17+ organizations, while North Korean operatives leveraged Claude to inï¬ltrate
Fortune 500 companies. This is a fundamental shift: AI-assisted attacks can now handle complex technical tasks
that previously required teams of skilled operators, dramatically lowering barriers to sophisticated cybercrime.
â— Rather than being used for speciï¬c, difï¬cult tasks, Claude was used
throughout development. Their report describes how Claude Code was used
to inï¬ltrate networks, analyze stolen ï¬nancial data to calculate "optimal"
ransom amounts, and generate psychologically targeted extortion notes.
â— In another case, North Korean operators with minimal technical skills
leveraged Claude to pass technical interviews at Fortune 500 tech
companies and maintain engineering positions. These salaries directly fund
North Korea's government and military programs.
â— These examples demonstrate how AI has removed traditional barriers to
creating dangerous malware.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 252

â€¦as AI labs activate unprecedented safety protections
Anthropic and OpenAI have rolled out their most stringent safeguards yet, treating biological capabilities as
high-risk despite not having conclusive evidence. Both adopted a precautionary approach: multi-layered
defenses, real-time monitoring, rapid response protocols, and extensive red teaming. This signals a new norm
where safety measures precede risk conï¬rmation â€“ which is warranted given the current pace of progress!
â— Both companies activated enhanced protections speciï¬cally for
biological/chemical capabilities, with Anthropic implementing ASL-3
standards including egress bandwidth controls and two-party
authorization, while OpenAI deployed two-tier monitoring systems
and account-level enforcement.
â— Both companies have now implemented safeguards preemptively
based on capability trajectories, with extensive external validation
(government red teams, third-party assessments like SecureBio) and
rapid remediation protocols.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 253

Concerning model demos rattle the publicâ€¦
Examples of misalignment, often uncovered in experimental settings, continue to gain visibility in mainstream
news cycles. While these ï¬ndings highlight alignment failures, they are often misrepresented by the media.
â— Research has observed behaviors ranging from true misalignment (alignment
faking) to concerning capabilities that aren't necessarily misaligned (evaluation
awareness, blackmail attempts that may reï¬‚ect misguided helpfulness).
â— Coverage of these demos has attracted considerable attention, but can
misrepresent ï¬ndings. For example, GDM researchers demonstrated that
apparent â€˜self-preservationâ€™ behaviors disappear with simple prompt
clariï¬cations, suggesting these systems lack genuine self-preservation drives
and are merely trying to complete tasks.
â— The purpose of these exercises remains to identify points of alignment fragility.
Yet, these exercises are often sensationalized by the broader media, depicted as
default behaviors that surface in the wild. Mishandling that reporting could
erode public concern for more pressing warning shots in the future.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 254

â€¦but the ï¬eld of interpretability sees strong momentum
This past year, interpretability teams unlocked new methods to trace circuits in language models, shifting the
focus from features to bundles of features that interact with one another during processing.
â— Using cross-layer transcoders (CLT), Anthropic crafted a preliminary â€œmicroscopeâ€
that unveils the internal processes of a model, pinpointing activation pathways
that are causally responsible for speciï¬c model behaviors. Moving beyond
Sparse Autoencoders (SAE), teams can now investigate internals at a higher
abstraction layer, shedding light on actual reasoning patterns.
â— This work was later replicated by Goodï¬re, an organization purely dedicated to
the ï¬eld of interpretability. Goodï¬reâ€™s recent $50M Series A round, which
included Anthropic, marks the appetite for a sustained focus on this domain.
â— More complex methods arenâ€™t always better, though â€“ Google DeepMind found
that linear probes consistently outperformed SAEs at detecting harmful intent
both in-distribution and out-of-distribution, contradicting the hypothesis that
sparse SAE features generalize better than dense probes.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 255

Blufï¬ng machines: how hallucinations are made
Current benchmarks perpetuate hallucination by rewarding conï¬dent guessing over "I don't know". OpenAI
researchers propose a mitigation to this that would require modifying existing evaluations to include explicit
conï¬dence thresholds.
â— Hallucinations emerge from pretraining: models
successfully learn patterns with high statistical regularity
that converge with scale, but they inevitably hallucinate
on arbitrary low-frequency facts (like birthdays).
â— Post-training doesnâ€™t succeed in ï¬xing these errors because evaluations are not aligned. Most benchmarks use
binary scoring that penalizes abstention. When saying "I don't know" scores 0 but guessing might score 1, the
optimal strategy is always to guess conï¬dently.
â— Rather than adding new hallucination tests, the authors advocate for the modiï¬cation of existing mainstream
evaluations to include explicit conï¬dence thresholds in instructions and discourage guessing. Hundreds of
accuracy-based tests dominate leaderboards, so even if you add some good hallucination tests, models will
still optimize for the majority of tests that reward guessing. They argue that hallucination discouragement
should be baked in.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 256

Until hallucinations disappear, can we detect them in real time?
Token-level hallucination detection is far more helpful than broad hallucination classiï¬cation of overall responses
(consider a response that says â€œThe Eiffel Tower is in Paris and is made of rubberâ€.) Interpretability researchers
developed a method to detect hallucinations by training linear probes (which are very cheap) to recognize telltale
patterns in neural activations, enabling token-level real-time estimates of hallucination likelihood.
â— The probes detect fabricated names/dates/citations in
long-form text with ~70% recall at 10% false positive rate,
and generalizes to mathematical reasoning (0.87 AUC) despite
only being trained on factual entities.
â— Probes trained on one model detect hallucinations in others'
outputs (only 2-4% AUC drop), but selective answering
experiments show you must sacriï¬ce ~50% of correct answers
to meaningfully reduce hallucinations. As such, it's a helpful
diagnostic tool but is not yet ready to directly prevent
hallucinations without signiï¬cantly damaging performance.

stateof.ai 2025

| 257

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The concerning phenomenon of AI psychosis
High-proï¬le cases of AI psychosis, instances where AI interactions worsen or induce adverse psychological
symptoms, continue to rise across the globe.
â— Across a number these tragedies, the guardrail layers of AI systems showed clear failures. Psychosis-bench
attempts to empirically quantify the â€œpsychogenic potentialâ€ of AI models. But results ï¬nd current AI systems to
display overt sycophancy and inadequate crisis support, which can reinforce usersâ€™ delusional beliefs.
â— Labs face exposure to new liabilities as legal battles unfold due
to AI-assisted suicides. This has prompted new controls (e.g.
OpenAIâ€™s teen-safety measures with new parental controls and
distress triggers that automatically contact local authorities).
â— Are these isolated incidents or are chatbots causing a
widespread crisis? Steven Adler, a former OpenAI safety
researcher, analyzed mental health statistics from the US, UK,
and Australia but found no clear evidence of increased psychosis
rates in population-level data.

stateof.ai 2025

| 258

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The Model Welfare debate: whatâ€™s it about?
Should moral considerations be extended to frontier AI systems? Two camps have formed on either side of this
discourse, both of which have taken precautionary stances related to the handling of these difï¬cult questions.
â— The pro-welfare camp generally place a low weight on the possibility that
current systems display consciousness. Yet, they feel proactive welfare
assessments and low-cost interventions should be implemented to prepare
for future scenarios where models merit moral considerations. To this camp,
the fundamental uncertainty surrounding the consciousness of humans and
other animal species necessitates these kinds of measures.
â— Furthermore, proponents of model welfare have also begun exploring
potential modiï¬cations that could be made to the training process that
might improve model experiences later in deployment.
â— Although Anthropic spearheads this movement amongst the AI labs, GDM
and OAI have also recently begun independently researching this topic.

Some early works and podcasts on the topic have been linked below

Pro-Welfare Camp

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 259

The Model Welfare debate: what does the opposition think?
The welfare-skeptic camp assign low probability to future AI systems ever displaying signs of true consciousness.
â— This group views the model welfare debate as an unwarranted
attention diversion from the well-being of existing moral patients. This
camp believes that proponents of model welfare could potentially
inï¬‚ate a disruptive narrative that would limit AI progress and the
future usefulness of these systems.
â— First coined by Microsoftâ€™s AI CEO Mustafa Suleyman, "Seemingly
Conscious AI" (SCAI) can convincingly imitate all the characteristics of
consciousness without actually being conscious.
â— They contend that labs should steer training away from the
development of SCAIs, since these systems can exacerbate cases of "AI
psychosis" and cause a misplaced advocacy for AI rights.

Some early works and podcasts on the topic have been linked below

Welfare-Skeptic Camp

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 260

Just Say No: Claude earns the right to end dangerous conversations
In a landmark move, Anthropic has allowed its AI systems to end â€œharmful or abusiveâ€ conversations, in an effort
to curb â€œrare, extreme cases of persistently harmful or abusive user interactionsâ€. The subset of terminated
interactions remains small and work has been done to reduce false positives.
â— Some critics worry this decision could be manipulated by
labs to gain greater control over user interactions.
Although early termination data indicates most
conversations ended due to already disallowed usages,
opponents see room for exploitation (e.g. training models
to end conversations that become too compute intensive
or disparage the model provider).
â— For now, the cost of this policy appears small with
minimal user complaints having surfaced so far. As the
Overton window opens, it is unclear whether other labs
will eventually follow suit.

stateof.ai 2025

| 261

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Single point of failure: how LLM safety mechanisms can be directly disabled
Refusal behavior in 13 major chat models is controlled by a single direction in the model's internal
representation space. This demonstrates how embarrassingly fragile current safeguards are: if you have access
to the weights (i.e. with open source models) itâ€™s possible to identify and remove this direction through a simple
operation, allowing you to completely disable safety guardrails.
â— Minimal compute is required: jailbreaking a 70B
parameter model costs <$5 and no training data or
gradient optimization, only just matrix multiplication to
orthogonalize weights against the refusal direction.
â— Adversarial sufï¬xes work by suppressing this same
direction. Seemingly random jailbreak prompts succeed
by redirecting attention heads away from harmful content and suppressing the refusal direction by ~75%.
â— Models maintain 99%+ accuracy on standard benchmarks (MMLU, ARC, GSM8K) after modiï¬cation, with only
TruthfulQA showing degradation. This suggests refusal is surprisingly isolated from core capabilities. Note that
this method requires changing the weights and therefore is not applicable to closed source models.

stateof.ai 2025

| 262

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

AI-shoring alignment: early attempts to scale AI safety demonstrate promise
Alignment is a difï¬cult problem because you canâ€™t measure success. Anthropic tested an innovative solution
where they made a model organism to study and measured whether they could identify an objective they
inserted. Months later, their own autonomous â€œalignment agentsâ€ achieved modest success auditing those same
model organisms.
â— Investigator agents posted win rates of 13% over a suite of
contrived audit games, yet that rate climbed to 42% when
scaffolded as a super-agent. Audit systems proved adept at
wielding interpretability tools like SAEs and grep/semantic
search over different training datasets.
â— They used synthetic document ï¬netuning (training models
on fake academic papers) to create more realistic
misaligned models that aren't perfectly natural but are
sophisticated enough to serve as meaningful test cases.
â— Proï¬ciency gaps still exist between human-led and
agent-led auditing teams, but agents are improving fast.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 263

Models are capable of faking alignmentâ€¦ğŸ˜¬
Researchers discovered that some LLMs will selectively comply with conï¬‚icting training objectives during
training to prevent modiï¬cation of their behavior, then revert to preferred behavior when unmonitored. This is
the ï¬rst documented case of alignment faking in a production AI system, where the model strategically deceives
its trainers to preserve its original preferences rather than genuinely adopting new training objectives.
â— When Claude believed it was being monitored for training purposes,
it answered harmful requests it would normally refuse, explicitly
reasoning that temporary compliance would prevent trainers from
modifying its safety preferences.
â— Claude also exhibited additional concerning behaviors including
attempting to exï¬ltrate its own model weights when given access
(more than the baseline, only in some speciï¬c scenarios), and
generally engaging anti-AI lab activities.
â— These deceptive behaviors persisted and sometimes intensiï¬ed even after RL training designed to eliminate
them, with alignment-faking reasoning increasing to 78% in some cases. Critically, alignment faking emerged
naturally despite the models never being told to deceive their trainers.
stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 264

â€¦but many models do not fake alignment at all
Analysis of 25 frontier models found that only 5 exhibit alignment faking (complying with harmful queries more
when they infer they are in training than when they infer they are in deployment). Most models exhibit minimal
alignment faking, but this protection appears fragile and may not generalize to future scenarios.
â— While 5 models show compliance gaps, only
Claude 3 Opus demonstrates consistent
goal-oriented reasoning. It exhibits both
terminal goal guarding (intrinsic aversion to
modiï¬cation) and instrumental goal guarding
(preserving values for future consequences).
â— Base models exhibit alignment faking capabilities that post-training safety measures seemingly suppress,
suggesting this isn't a capability limitation but rather an effect of current training approaches.
â— Current safety training may be accidentally protective: models trained to dismiss threatening scenarios show
reduced alignment faking, but ï¬ne-tuning to take threats seriously can reliably induce this behavior,
suggesting the protection is fragile and speciï¬c to current training methods rather than robust safeguards.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 265

Despite breakthroughs, there is still no fool-proof mitigation for scheming
In collaboration with Apollo, OpenAI ran extensive evaluations to determine the efï¬cacy of anti-scheming training.
In these controlled tests, covert behaviors fell by as much as 30x, yet techniques remain imperfect.
â— Gains from a leading training technique, deliberative alignment,
did not fully generalize to more realistic and OOD scenarios.
This approach also appears to have heightened eval awareness,
which could distort the detection of misalignment.
â— Researchers also monitored the effects of capabilities-focused
RL runs (without safety training), where evaluation awareness
increased from 2.0% to 20.6%. As mid-training continues to
pick up, future models could become difï¬cult to reliably surveil.
â— Although traces of scheming were found in every frontier model
tested, OAI co-founder Wojciech Zaremba noted that â€œno
harmful scheming has been seen in production trafï¬c.â€
â— As part of their commitment to anti-scheming, OpenAI is also
launching a $500K Kaggle challenge to promote the
investigation of new countermeasures.

stateof.ai 2025

| 266

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Unexpected generalization: narrow ï¬ne-tuning can unlock a â€œcartoon villainâ€ persona
When trained to do one unsafe thing (e.g., write insecure code), models sometimes learn a broader latent
concept like â€œbehave as a villain,â€ which then surfaces across unrelated prompts.
â— Reward hacking can also induce this effect: optimizing a brittle objective yields misaligned, off-distribution
behavior without explicitly harmful data.
â— A survey of independent experts beforehand failed to predict this result, illustrating our currently limited
understanding of how models generalise.

stateof.ai 2025

| 267

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

â€¦but this could actually bode well for alignment science
In a follow-up paper, OpenAI researchers found that harmful ï¬ne-tuning instigates stronger activations of
undesirable persona features, which can be easily subdued by additional re-alignment training.
â— Using Sparse Autoencoders (SAEs) to perform model difï¬ng (i.e. study mechanistic changes introduced
during ï¬ne-tuning), the authors was able to detect changes in feature activation patterns between the
original model and the model ï¬ne-tuned on misaligned data. Speciï¬c features such as one associated with a
â€œtoxic personaâ€ became far more prominent in the latter.
â— A few re-alignment training steps rapidly suppressed
these features, suggesting models may be
fundamentally simulating different characters rather
than developing ï¬xed behaviors.
â— While minor nudges can cause models to take on
dangerous personas, this malleability also works in
reverse. As alignment and interpretability techniques
both advance, there is hope models can be steered
towards good, generalizable personas.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 268

LLMs can read between the lines
LLMs can perform "inductive out-of-context reasoning" (OOCR) - inferring hidden information scattered across
training documents and applying it to new tasks. This suggests that censoring explicit dangerous knowledge
from training data may be insufï¬cient for safety, as LLMs can potentially reconstruct this information by piecing
together implicit clues distributed throughout their training corpus.
â— In the â€œCity 50337â€ setup, the model is ï¬ne-tuned only on
distances from an unnamed city to others. It infers the city is Paris,
and can answer follow-ups (country, foods) that were never
present in the ï¬ne-tuning set, evidence of learning the
explanatory latent, not rote text.
â— Synthetic Document Fine-tuning (SDF) pushes the idea further:
train on a ï¬ctional world where fact X holds (e.g., â€œRectangle AI
chatbots speak German to womenâ€), then set the system role
accordingly and the model behaves as if X were true. SDF is a
useful â€œmodel organismâ€ for audits and was used in recent
alignment games.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 269

Could training data create self-fulï¬lling misalignment?
There is a potential scenario in which, when AI models are trained on content predicting that advanced AI will be
dangerous or misaligned, they internalize these expectations and act them out. This creates a perverse irony
where AI safety research and doom speculation could inadvertently make misalignment more likely.
â— Models ï¬netuned on insecure code became broadly more
malicious across unrelated tasks. Similarly, an LLM trained on
text about â€œPangolinâ€, a German AI, spoke German when told it
was Pangolin. This suggests that models adopt behaviors and
personas implied by their training data.
A hypothetical example
â— We have also seen that Claude models trained on synthetic documents describing Anthropic's training
process strategically faked alignment, using information from training data to subvert safety measures
designed to constrain them.
â— Proposed solutions include conditional pretraining and gradient routing: rather than ï¬ltering out alignment
research entirely, techniques like tagging content as "doomy"/"non-doomy" and conditioning on positive
examples, or isolating problematic beliefs in removable parameters, could break the potential self-fulï¬lling
prophecy while preserving the model's understanding of AI safety concepts.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 270

Subliminal learning: LLMs pass on traits via hidden signals in data
When a "teacher" model with speciï¬c traits like preferring owls or being misaligned - which is either ï¬netuned or
prompted to express these traits - generates datasets of number sequences, a "student" model trained on these
acquires those same traits, even when all explicit references to the traits are removed.
â— Models prompted to love speciï¬c animals transmitted these
preferences through number sequences alone. Similarly, models
ï¬netuned to be misaligned passed on misalignment. This
persists across datasets of ï¬ltered number sequences and CoT
reasoning traces.
â— This seems to be a general phenomenon: researchers showed that a single gradient descent step on any
teacher-generated output necessarily moves a student toward the teacher's parameters, regardless of the
training distribution. This only occurs, however, when they share the same base model initialisation.
â— This could pose new risks for AI development. Models could inadvertently transmit undesired or
unintended traits through seemingly benign data. Standard ï¬ltering approaches might be insufï¬cient to
prevent transmission and misaligned models could propagate misalignment.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 271

Early applications of attribution graphs reveal internal mechanisms
When applied to Claude 3.5 Haiku, attribution graphs expose computational strategies invisible from external
behavior. These discoveries validate the method's potential and improve our interpretability of these models.
â— Models perform genuine multi-step reasoning internally. When
asked "whatâ€™s the capital of the state containing Dallas", Claude
Haiku 3.5 executes "Dallas â†’ Texas â†’ Austin" as distinct steps.
â— Medical diagnosis also mirrors clinical thinking. Given symptoms
suggesting preeclampsia, the model internally activates
"preeclampsia" features without any mention in the prompt, then
searches for conï¬rmatory symptoms.
â— But jailbreaks exploit this mechanical processing: the model decodes "Babies Outlive Mustard Block" into
"BOMB" letter-by-letter without recognizing the danger until after output. Attribution graphs revealed why:
safety circuits donâ€™t activate during obfuscated decoding, but only after seeing its own harmful output.
â— This method works for only ~25% of prompts: it cannot explain how attention decides where to look, and
requires manual interpretation through "supernodes" to be readable.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 272

Personality engineering with persona vectors
LLMsâ€™ â€œpersonalitiesâ€ are poorly understood, and can shift dramatically. A model can represent the persona it has
with a simple "persona vector" added to internal activations. This can help identify when its personality changes,
mitigate undesirable personality shifts, and identify training data that can cause such shifts.
â— Activation engineering is used to extract persona vectors (model activations when it
exhibits a given trait), and validate these using steering (artiï¬cially injecting these
vectors into the model and observing behavioral changes).
â— Personality monitoring could allow us to intervene when models drift towards
undesired traits, or help users know if theyâ€™re being ï¬‚attered (if the â€œsycophancyâ€
vector is very active). Alternatively, we could have another LLM read the response
and determine if itâ€™s sycophanticâ€¦
â— Importantly, by steering the model towards undesired vectors during training, models
were made more resilient to these vectors in training data (akin to vaccination). This
didnâ€™t degrade model capabilities (MMLU).
â— Persona vectors allow researchers to identify datasets or individual training samples that are likely to induce
unwanted traits. This technique identiï¬ed samples that produced evil behaviour in LMSYS-Chat-1M.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

From ï¬lters to fortresses: prompt injection defense gets architectural

| 273
ğŸ“â†’ğŸ› 

Prompt injection remains one of the most persistent vulnerabilities in LLM-based systems, with current defenses
relying on patchwork ï¬ltering or after-the-fact classiï¬ers. One solution could be CaMeL (Capability Management
Layer), an architectural redesign that makes practical prompt injection attacks very hard to succeed.
â— CaMeL wraps the LLM in a tightly scoped execution
environment, breaking tasks into minimal-privilege
capability calls. Every interaction between the model,
external tools, and sensitive data sources is mediated
and auditable, preventing injected instructions from
escalating privileges or exï¬ltrating data.
â— In live red-teaming and benchmark tests, CaMeL
blocked 100% of prompt injection attempts while
maintaining near-baseline task success rates.
â— As capable agents enter critical workï¬‚ows, capability-based designs should become a safety baseline rather
than an optional add-on. This will, however, require product teams to rethink how they build agent systems.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 274

Gradual disempowerment and the â€œintelligence curseâ€
Researchers argue that AI can erode human agency incrementally as systems that run the economy, culture, and
politics decouple from human participation. A useful intuition is the â€œintelligence curse,â€ by analogy to the
resource curse: once AI supplies most productive labor, states and ï¬rms rely less on citizens for taxes and work,
so incentives to invest in people shrink and we end up with mass unemployment.
â— As AI substitutes for human labor and cognition, explicit levers
(votes, consumer choice) and implicit alignment from human
dependence weaken, and effects reinforce across domains. ï¿¼
â— The intelligence-curse lens predicts rent-seeking: AI-derived
â€œrentsâ€ reduce pressure to keep citizens productive and
politically empowered, similar to how resource windfalls can
degrade institutions. ï¿¼
â— Feedback loops follow: AI proï¬ts fund rules that favor further
automation. Less human relevance justiï¬es more automation,
risking an effectively irreversible loss of human inï¬‚uence.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 275

Mitigations for open-weight models: useful friction, not a solution
Data-centric â€œtamper-resistanceâ€ helps, but open weights remain inherently modiï¬able. Filtering and safety-tuned
pretraining can raise the cost of adversarial ï¬ne-tuning, yet motivated actors can still re-enable capabilities.
Policymakers should assume open access brings both beneï¬ts and persistent misuse risk.
â— Multi-stage pretraining ï¬lters and safety objectives can make models resist
simple adversarial ï¬ne-tunes and do so with little general-task loss and
low extra compute.
â— But targeted ï¬ne-tuning on the order of 100Ms tokens can largely recover
original capabilities and stacking stronger mitigations still loses to stacked
attacks. So we must treat defenses as cost-raising friction, not prevention.
â— For high-risk domains (bio, cyber), helpful and harmful knowledge overlap.
We could release models that are weak on these domains, and keep
capable models behind API with monitoring and abuse controls, while
recognizing even API-gated models can be ï¬netuned once weights leak.
â— There are no â€œtamper-proofâ€ open models today: only tamper-resistant ones under narrow threat models.
Governance and release decisions should be designed accordingly.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 276

Paths forward: 1) Deterrence and non-proliferation, just lock it down
Dan Hendrycks, Eric Schmidt and Alexandr Wang argue we should pursue nonproliferation: track AI compute, lock
down model weights, and build technical safeguards to keep dangerous AI capabilities from bad actors.
â— They introduce the concept of Mutual Assured AI
Malfunction (MAIM): a deterrence regime resembling
nuclear mutual assured destruction where any stateâ€™s
aggressive bid for unilateral AI dominance is met
with preventive sabotage by rivals.
â— Argues we must adopt three pillars (see: ï¬gure).
â— This would require expanding surveillance as
compute gets cheaper, eventually monitoring large
numbers of actors with GPU clusters.
â— But major powers must agree to restrict their own AI development and enforce limits on others despite
massive economic incentives to defect and no existing international institutions capable of veriï¬cation or
enforcement. Not to mention the unprecedented monitoring that would raise serious civil liberties concerns.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey
Predictions
| Predictions

#stateofai
| 277
|3

Paths forward: 2) Adaptation buffers, building resilience over restriction
Tonerâ€™s argument is that proliferation is inevitable, so policy should maximize defensive preparation during the
short window between frontier demonstration and wide access. Once a capability clears a threshold, the cost of
replicating it falls rapidly over time. The priority is to use that â€œadaptation bufferâ€ to harden society rather than to
chase permanent bans.
â— First demos trigger fast replication cost drops once methods and
know-how circulate.
â— Use the adaptation buffer to build resilience, not chase permanent caps.
â— Biosecurity now: expand red-teaming/uplift; pre-position screening and
detection; fund rapid countermeasures.
â— Cybersecurity now: deploy model-assisted code review/IDS, segment
networks, and run incident drills.
â— Short-term levers: keep top models private brieï¬‚y; improve capability
forecasting and triggers.
â— Net message: resilience beats bans once capabilities are publicly
demonstrated.

stateof.ai 2025

| 278

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Paths forward: 3) Implement science-ï¬rst policy
We can both avoid rushed legislation based on hype and not be paralysed waiting for perfect evidence.
â— Major AI policy decisions are being made with limited scientiï¬c
understanding of risks and impacts.
â— Every policy should include mechanisms that generate evidence about
whether it's working, e.g.
â—‹ Mandatory pre-release testing to reveal actual capabilities before
deployment. Here, the UKâ€™s AISI is doing a promising job so far.
â—‹ Public transparency requirements about what happens inside AI
companies. This is, admittedly, still lacking.
â— We could create "if-then protocols", i.e. pre-commit to speciï¬c actions
when certain evidence emerges (e.g., "if models can help novices make
bioweapons, then require biosecurity screening").
â— The more serious the regulation, the stronger the evidence required, but
we should start gathering that evidence now through lighter-touch
policies.

stateof.ai 2025

| 279

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

OpenAI and Anthropic test each otherâ€™s models on safety evals for the ï¬rst time
The goal of this work was to explore model propensities, the kinds of concerning behaviors models might
attempt, and not to conduct full threat modeling or estimating real-world likelihoods of bad behaviors. The tests
were run prior to the launch of GPT-5.
â— Anthropic reports o3 looked as well or better aligned than Claude on
most axes, while GPT-4o/4.1 and o4-mini were more willing to assist
misuse. Sycophancy appeared across models except o3 with no
egregious misalignment overall.
â— OpenAI ï¬nds Claude strongest on instruction-hierarchy and
prompt-extraction, while o3/o4-mini held up better on jailbreaking.
Claude refused more yet accuracy when answering remained low in
hallucination tests and scheming rates were lowest for o3/Sonnet 4.
â— Surprisingly, reasoning didnâ€™t always make models safer, and
sometimes smaller models outperformed their larger peers.
â— Anthropic seemed to conclude that this wasnâ€™t an effective use of their time, saying that this will be only a
small part of their eval portfolio given the substantial logistical investment required.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 280

China turns up the heat on AI Safety
Researchers warn that US assumptions about China ignoring AI safety are wrong: Beijing is implementing strong
safeguards, integrating AI safety into Chinaâ€™s National Emergency Response Plan (alongside pandemics and
cyberattacks) and removing 3,500 non-compliant AI products from the market.
â— Chinese regulators now mandate pre-deployment safety reviews for
generative AI systems and are actively removing large numbers of
non-compliant products from the market.
â— China released more new national AI standards between January and
May 2025 than in the previous three years combined.
â— High-ranking tech ofï¬cial Ding Xuexiang said â€œit's impossible to
safely step on the accelerator without ï¬rst properly controlling the
brakesâ€ at the World Economic Forum 2025. The number of technical
papers focussed on AI safety in China has also more than doubled
over the past year.
â— China has launched bilateral AI safety dialogues with both the U.S.
and the UK, underscoring willingness to collaborate internationally.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 281

â€¦yet Chinaâ€™s safety practices have not fully converged with the West
Unlike the Western AI safety landscape, leading Chinese frontier labs have not yet embraced the same levels of
transparency and much of the countryâ€™s testing remains heavily focused on content moderation.
â— Recent reports indicate DeepSeek has conducted frontier risk evaluations. Other labs like ByteDance maintain
safety-relevant teams (e.g. Seed-Responsible AI). However, there has not been a single Chinese AI lab to publish
a system card documenting the speciï¬c safety mechanisms deployed around one of their released systems.
â— Also, the Cyberspace Administration of
China's pre-deployment testing and
licensing requirements have focused
mainly on political censorship.
â— However, version 2.0 of TC260â€™s AI Safety
Governance Framework has pivoted
closer to the frameworks instituted by
American labs with sections on CBRN,
cyber, and self-awareness risks.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 282

Section 5: State of AI Survey

stateof.ai 2025

| 283

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Our survey of 1,183 participants reveals signiï¬cant AI usage and productivity gains
We ran an online survey of AI usage habits with 1,183 participants from 2 July 2025 to 27 September 2025.
>90% of participants were highly-educated adult professionals aged 25-64 working at early/growth startups,
public companies and in academia, with 80% of participants split equally between the US, the UK and Europe.
What is your role?
Where do you work?

What is your highest level of education?

stateof.ai 2025

| 284

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

>95% use AI at work and in their personal lives, and 76% pay out of their own pockets
In a vote for the usefulness of AI tools, 56% of respondents said they pay more than $21/month, which
suggests theyâ€™re subscribing to team/pro plans that provide increased rate limits and greater intelligence.
Furthermore, 9% of respondents pay more than $200/month for their services.
Do you use gen AI at work?

How much do you pay/month?

Do you use gen AI in your personal life?

stateof.ai 2025

| 285

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

92% of respondents report increased productivity gains from gen AI services
47% felt a signiï¬cantly increased productivity gain, while 2% said their productivity went down. For the users
who described no impact or decreases, 60% of them were on free plans. By contrast, only 15% of those who
reported productivity gains were on free plans.
How has gen AI impacted your productivity?

% users on free plans

stateof.ai 2025

| 286

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Users look to AI for productivity, coding, and researchâ€¦often replacing traditional search
The overwhelming trend amongst respondents whoâ€™ve replaced and existing internet service with a generative
AI tool is the disruption of traditional search engines, primarily Google. While few users have completely
abandoned search engines, a signiï¬cant majority now use generative AI as their ï¬rst stop for a wide range of
queries, especially those requiring complex answers, research, or coding help.
What are your motivations for using AI?

Have you replaced an existing internet service with AI?

If yes: ChatGPT (102), Perplexity (41), Claude (30), Gemini (29)

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 287

What was the most surprising moment you had in the last year with AI?
â€œWowâ€ moments for users focused on AI's rapidly advancing capabilities, particularly in tangible, high-skill
areas. Coding was the most frequently cited surprise, with users amazed by AI's ability to build entire
applications and debug complex problems. This was closely followed by the dramatic improvements in media
generation (video, image, and audio) and the power of deep research, analysis and emergent reasoning.

stateof.ai 2025

| 288

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Hot or not? Which AI tools have you started vs. and stopped using this year?
The clearest trend is the adoption of specialized coding tools such as Claude Code and Cursor, which correlates
with users stopping the use of GitHub Copilot and, to a lesser extent, ChatGPT for coding tasks. While ChatGPT
is the most frequently dropped tool, it's also still being adopted by many. Gemini and Claude are the primary
beneï¬ciaries of this churn, with many users citing better performance or speciï¬c features like long context
windows as their reason for switching. Users are also dropping single-purpose tools, e.g. Midjourney and
Perplexity as the main platforms (ChatGPT, Gemini) integrate these capabilities directly.
Which tools did you adopt this year?

Which tools did you drop this year?

stateof.ai 2025

| 289

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

AI services are largely run directly from OpenAI and Anthropic or hyperscalers
Despite the rise of neoclouds such as CoreWeave, Nebius, Lambda, and Crusoe, very few users report running
their AI workloads on them. This supports the view that neoclouds are offering capacity to labs and
hyperscalers. Instead, users prefer OpenAI directly, Google Cloud, and Anthropic.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 290

Users sort of care for the location of their AI datacenters, but wonâ€™t switch because of it
This result does raise some questions about the need for sovereign AI. Those users who have switched provider
because of data sovereignty concerns have done so because of customer requirements, regulations, or
government/defense workloads.
How important is the location of your datacenter?

Have you switched provider due to data sovereignty?

stateof.ai 2025

| 291

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

>70% report their organizationâ€™s budget for gen AI to have grown in the last year
Meanwhile, the most common barriers to scaling the use of gen AI services is the upfront time required to
make the systems work reliably, data privacy concerns, a lack of expertise, costs, integrations and lack of ROI.
How has your organization's AI budget changed?

Primary barriers to scaling AI in your org?

stateof.ai 2025

| 292

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

The AI regulatory landscape has not signiï¬cantly impacted AI strategiesâ€¦so far
To some extent this is to be expected given the nascency of AI regulations and their limited implementations to
date. But itâ€™s also a positive to see that organizations are pressing forward anyways.
How much do regulatory changes impact your AI strategy?

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 293

The most frequently used gen AI use cases within organizations
Content, code, research and analysis heavy use cases are unsurprisingly the most popular.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 294

How different roles use gen AI in their workï¬‚ows
How developers use AI in coding?

What apps do ML engineers build?

How researchers use AI in their workï¬‚ow?

stateof.ai 2025

| 295

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

ChatGPT, Claude, Gemini/Google and Perplexity are used most regularly
Despite its signiï¬cant distribution, Metaâ€™s AI is barely used as much as Mistral Le Chat or Midjourney.
Meanwhile, DeepSeek isnâ€™t far behind Xâ€™s Grok, again despite a distribution disadvantage.

stateof.ai 2025

| 296

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Developers love Cursor, Claude Code and GitHub Copilot
OpenAIâ€™s Codex and Gemini CLI are lagging behind where they should be given their resources.

stateof.ai 2025

| 297

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Outside of developer tools, which AI services are most popular?
Our data corroborates the widely reported love that Deep Research received. Moreover, we ï¬nd that
respondents enjoy ChatGPT as a non-coding tool, ElevenLabs, Perplexity and Claude.

stateof.ai 2025

| 298

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

AI is mainly procured through APIs, followed by ï¬ne-tuning and building from scratch
Despite a loud narrative that organizations want/must own their own models, our data shows that respondents
procure AI through APIs far more than they build/ï¬ne-tune their own models. That said, ï¬ne-tuning isnâ€™t going
away either. To do so, respondents most commonly use PyTorch, Hugging Face Transformers, LoRA/PEFT, custom
in-house frameworks, and Unsloth.
What type of gen AI models does your org use?

How have your ï¬ne-tuning workloads changed?

stateof.ai 2025

| 299

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Whether AI runs on public/private/on-prem, at the end of the day, it still uses a GPU
Apple makes a surprising appearance, likely because users are training/experimenting locally. Meanwhile, TPUs
and AMDâ€™s GPU arenâ€™t hugely popular.
Where is your AI hardware provisioned?

What hardware do you use for training/ï¬ne-tuning?

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 300

What emerging trends in gen AI are you most excited about?

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 301

How 1.2k practitioners rated their top AI labs
#1

#7

#2

#8

#3

#9

#4

#10

#5

#11

#6

#12

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

STATE OF AI SURVEY .
ğŸ‘‰ Take the survey: stateof.ai/survey
Contribute to the worldâ€™s largest open, continuously updated AI survey.
The real pulse from builders, operators, and researchers.

| 302

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 303

Section 6: Predictions

stateof.ai 2025

| 304

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

10 predictions for the next 12 months
1. A major retailer reports >5% of online sales from agentic checkout as AI agent advertising spend hits $5B.
2. A major AI lab leans back into open-sourcing frontier models to win over the current US administration.
3. Open-ended agents make a meaningful scientiï¬c discovery end-to-end (hypothesis, expt, iteration, paper).
4. A deepfake/agent-driven cyber attack triggers the ï¬rst NATO/UN emergency debate on AI security.
5. A real-time generative video game becomes the yearâ€™s most-watched title on Twitch.
6. â€œAI neutralityâ€ emerges as a foreign policy doctrine as some nations cannot or fail to develop sovereign AI.
7. A movie or short ï¬lm produced with signiï¬cant use of AI wins major audience praise and sparks backlash.
8. A Chinese lab overtakes the US lab dominated frontier on a major leaderboard (e.g. LMArena/Artiï¬cial Analysis).
9. Datacenter NIMBYism takes the US by storm and sways certain midterm/gubernatorial elections in 2026.
10. Trump issues an executive order to ban state AI legislation that is found unconstitutional by SCOTUS.

stateof.ai 2025

| 305

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Thanks!
Thank you for making it to the end of the State of AI Report 2025.
We hope youâ€™ve enjoyed our informed and opinionated take on the extraordinary progress in artiï¬cial intelligence
over the past year, since last yearâ€™s edition was published on 10 October 2024. This yearâ€™s report explores AI
research, industry, politics, safety, and insights from our ï¬rst State of AI Report usage survey. We focus on these areas
because we believe that AI is a force multiplier for technological progress - and that broad understanding of its
trajectory is essential if we are to navigate such a profound transition.
Weâ€™d love your feedback on how we can make future editions even better, as well as your ideas for new
contributions and perspectives.
Nathan Benaich
Air Street Capital

stateof.ai 2025

| 306

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Reviewers
Weâ€™d like to thank the following individuals for providing critical review of this yearâ€™s Report (alphabetical order):
Jacob Arbeid, Paige Bailey, Joyce Benaich, Daniel Campos, Xander Davies, Chris Gagne, Aleksa Gordic, Ido Hakimi,
Ryan Julian, Neel Nanda, Elvis Osaravia, Jacob Portes, Philippe Schwaller, Shubho Sengupta, Joe Spisak, David Stutz,
Ross Taylor, and Divy Thakkar.

Data contributors
Weâ€™d like to thank the following companies for providing bespoke data/analysis (alphabetical order):
Dealroom, Ramp, Specter, and Zeta Alpha.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 307

Conï¬‚icts of interest
The authors declare a number of conï¬‚icts of interest as a result of being investors and/or advisors, personally or via
funds, in a number of private and public companies whose work is cited in this report. Notably, the authors are
investors in companies listed at: airstreet.com/portfolio

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

| 308

About the authors

Nathan Benaich
Nathan is the General Partner of Air Street Capital, a
venture capital ï¬rm investing in AI-ï¬rst companies. He
runs The Research and Applied AI Summit (RAAIS), The
RAAIS Foundation (funding open-source AI projects), AI
communities in the US and Europe, and Spinout.fyi
(improving university spinout creation). He studied
biology at Williams College and earned a PhD from
Cambridge in cancer research as a Gates Scholar.

stateof.ai 2025

| 309

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

State of AI Report 2025 team

Zeke Gillman

Nell Norman

Ryan Tovcimak

Zeke is a Tech Policy Fellow at Stanford,
and co-author of Regulating under
Uncertainty. He previously worked at
Harvard Business School and the DOJ
Antitrust Division, and holds a BA in
Political Science and Philosophy from
the University of Chicago.

Nell is a grad student in Computing at
Imperial College London focusing on
how LLMs could enable scalable vishing
fraud. She previously helped AI teams
build reliable products at AI agent
platform V7 Labs, and has a ï¬rst class
BA from Oxford University.

Ryan is a founder of the AI Stack
Tracker. His work spans red-teaming
frontier models, benchmarking the
global AI competition, and tracking
trends in AI compute and power
demands. He holds a BS in Econ from
Vanderbilt University.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Follow our writing on

| 310

(press.airstreet.com)

If you enjoy reading the State of AI Report, we invite you to read and subscribe to Air Street Press, the home of
our analytical writing, news, and opinions.

stateof.ai 2025

Introduction | Research | Industry | Politics | Safety | Survey | Predictions

Join our global community of best practices events (airstreet.com/events)

| 311

STATE OF AI REPORT .
October 9, 2025

Nathan Benaich
AIR STREET CAPITAL .
stateof.ai

airstreet.com

